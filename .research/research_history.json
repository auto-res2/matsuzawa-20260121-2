{
  "research_topic": "Data-efficient image classification methods using novel regularization and augmentation techniques",
  "queries": [
    "data-efficient classification",
    "novel regularization",
    "image augmentation"
  ],
  "research_study_list": [
    {
      "title": "Automated Data Augmentations for Graph Classification",
      "full_text": "Published as a conference paper at ICLR 2023 AUTOMATED DATA AUGMENTATIONS FOR GRAPH CLASSIFICATION Youzhi Luo1∗, Michael McThrow2, Wing Au2, Tao Komikado3, Kanji Uchino2, Koji Maruhashi3, Shuiwang Ji1 1Texas A&M University, TX, USA 2Fujitsu Research of America, INC., CA, USA 3Fujitsu Research, Fujitsu Limited, Kanagawa, Japan {yzluo,sji}@tamu.edu {mmcthrow,WAu,komikado.tao,kanji,maruhashi.koji}@fujitsu.com ABSTRACT Data augmentations are effective in improving the invariance of learning ma- chines. We argue that the core challenge of data augmentations lies in designing data transformations that preserve labels. This is relatively straightforward for im- ages, but much more challenging for graphs. In this work, we propose GraphAug, a novel automated data augmentation method aiming at computing label-invariant augmentations for graph classiﬁcation. Instead of using uniform transformations as in existing studies, GraphAug uses an automated augmentation model to avoid compromising critical label-related information of the graph, thereby producing label-invariant augmentations at most times. To ensure label-invariance, we de- velop a training method based on reinforcement learning to maximize an estimated label-invariance probability. Experiments show that GraphAug outperforms pre- vious graph augmentation methods on various graph classiﬁcation tasks. 1 I NTRODUCTION Many real-world objects, such as molecules and social networks, can be naturally represented as graphs. Developing effective classiﬁcation models for these graph-structured data has been highly desirable but challenging. Recently, advances in deep learning have signiﬁcantly accelerated the progress in this direction. Graph neural networks (GNNs) (Gilmer et al., 2017), a class of deep neural network models speciﬁcally designed for graphs, have been widely applied to many graph representation learning and classiﬁcation tasks, such as molecular property prediction (Wang et al., 2022b; Liu et al., 2022; Wang et al., 2022a; 2023; Yan et al., 2022). However, just like deep models on images, GNN models can easily overﬁt and fail to achieve satis- factory performance on small datasets. To address this issue, data augmentations can be used to gen- erate more data samples. An important property of desirable data augmentations is label-invariance, which requires that label-related information should not be compromised during the augmentation process. This is relatively easy and straightforward to achieve for images (Taylor & Nitschke, 2018), since commonly used image augmentations, such as ﬂipping and rotation, can preserve almost all information of original images. However, ensuring label-invariance is much harder for graphs be- cause even minor modiﬁcation of a graph may change its semantics and thus labels. Currently, most commonly used graph augmentations (You et al., 2020) are based on random modiﬁcation of nodes and edges in the graph, but they do not explicitly consider the importance of label-invariance. In this work, we propose GraphAug, a novel graph augmentation method that can produce label- invariant augmentations with an automated learning model. GraphAug uses a learnable model to automate augmentation category selection and graph transformations. It optimizes the model to maximize an estimated label-invariance probability through reinforcement learning. Experimen- tal results show that GraphAug outperforms prior graph augmentation methods on multiple graph classiﬁcation tasks. The codes of GraphAug are available in DIG (Liu et al., 2021) library. ∗Work was done while the author was at Fujitsu Research of America, INC. 1 arXiv:2202.13248v4  [cs.LG]  28 Feb 2023Published as a conference paper at ICLR 2023 2 B ACKGROUND AND RELATED WORK 2.1 G RAPH CLASSIFICATION WITH NEURAL NETWORKS In this work, we study the problem of graph classiﬁcation. Let G = (V,E,X ) be an undirected graph, where V is the set of nodes and Eis the set of edges. The node feature matrix of the graphG is X ∈R|V|×d where the i-th row of X denotes the d-dimensional feature vector for the i-th node in G. For a graph classiﬁcation task with kcategories, the objective is to learn a classiﬁcation model f : G→y∈{1,...,k }that can predict the categorical label of G. Recently, GNNs (Kipf & Welling, 2017; Veliˇckovi´c et al., 2018; Xu et al., 2019; Gilmer et al., 2017; Gao & Ji, 2019) have shown great success in various graph classiﬁcation problems. Most GNNs use the message passing mechanism to learn graph node embeddings. Formally, the message passing for any node v∈V at the ℓ-th layer of a GNN model can be described as hℓ v = UPDATE ( hℓ−1 v ,AGG ({ mℓ jv : j ∈N(v) })) , (1) where N(v) denotes the set of all nodes connected to the nodevin the graph G, hℓ v is the embedding outputted from the ℓ-th layer for v, mℓ jv is the message propagated from the node jto the node vat the ℓ-th layer and is usually a function ofhℓ−1 v and hℓ−1 j . The aggregation function AGG(·) maps the messages from all neighboring nodes to a single vector, and the function UPDATE(·) updates hℓ−1 v to hℓ v using this aggregated message vector. Assuming that the GNN model has Llayers, the graph representation hG is computed by a global pooling function READOUT over all node embeddings as hG = READOUT ({ hL v : v∈V }) . (2) Afterwards, hG is fed into a multi-layer perceptron (MLP) model to compute the probability that G belongs to each of the categories {1,...,k }. Despite the success of GNNs, a major challenge in many graph classiﬁcation problems is data scarcity. For example, GNNs have been extensively used to predict molecular properties from graph structures of molecules. However, the manual labeling of molecules usually requires expensive wet lab experiments, so the amount of labeled molecule data is usually not large enough for expressive GNNs to achieve satisfactory prediction accuracy. In this work, we address this data scarcity chal- lenge with data augmentations. We focus on designing advanced graph augmentation strategies to generate more data samples by performing transformations on data samples in the dataset. 2.2 D ATA AUGMENTATIONS Data augmentations have been demonstrated to be effective in improving the performance for image and text classiﬁcation. For images, various image transformation or distortion techniques have been proposed to generate artiﬁcial image samples, such as ﬂipping, cropping, color shifting (Krizhevsky et al., 2012), scaling, rotation, and elastic distortion (Sato et al., 2015; Simard et al., 2003). And for texts, useful augmentation techniques include synonym replacement, positional swaps (Ratner et al., 2017a), and back translation (Sennrich et al., 2016). These data augmentation techniques have been widely used to reduce overﬁtting and improve robustness in training deep neural network models. In addition to hand-crafted augmentations, automating the selection of augmentations with learnable neural network model has been a recent emerging research area. Ratner et al. (2017b) selects and composes multiple image data augmentations using an LSTM (Hochreiter & Schmidhuber, 1997) model, and proposes to make the model avoid producing out-of-distribution samples through adver- sarial training. Cubuk et al. (2019) proposes AutoAugment, which adopts reinforcement learning based method to search optimal augmentations maximizing the classiﬁcation accuracy. To speed up training and reduce computational cost, a lot of methods have been proposed to improve AutoAug- ment through either faster searching mechanism (Ho et al., 2019; Lim et al., 2019), or advanced optimization methods (Hataya et al., 2020; Li et al., 2020; Zhang et al., 2020). 2.3 D ATA AUGMENTATIONS FOR GRAPHS While image augmentations have been extensively studied, doing augmentations for graphs is much more challenging. Images are Euclidean data formed by pixel values organized in matrices. Thus, 2Published as a conference paper at ICLR 2023 many well studied matrix transformations can naturally be used to design image augmentations, such as ﬂipping, scaling, cropping or rotation. They are either strict information lossless transformation, or able to preserve signiﬁcant information at most times, so label-invariance is relatively straight- forward to be satisﬁed. Differently, graphs are non-Euclidean data formed with nodes connected by edges in an irregular manner. Even minor structural modiﬁcation of a graph can destroy important information in it. Hence, it is very hard to design generic label-invariant transformations for graphs. Currently, designing data augmentations for graph classiﬁcation (Zhao et al., 2022; Ding et al., 2022; Yu et al., 2022) is a challenging problem. Some studies (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) propose interpolation-based mixup methods for graph augmentations, and Kong et al. (2022) propose to augment node features through adversarial learning. Nonetheless, most commonly used graph augmentation methods (Hamilton et al., 2017; Wang et al., 2020; You et al., 2020; Zhou et al., 2020a; Rong et al., 2020; Zhu et al., 2021a) are based on the random modiﬁcation of graph structures or features, such as randomly dropping nodes, perturbing edges, or masking node features. However, such random transformations are not necessarily label-invariant, because important label-related information may be randomly compromised (see Section 3.2 for detailed analysis and discussion). Hence, in practice, these augmentations do not always improve the performance of graph classiﬁcation models. 3 T HE PROPOSED GRAPH AUG METHOD While existing graph augmentation methods do not consider the importance of label-invariance, we dive deep into this challenging problem and propose to solve it by automated data augmentations. Note that though automated data augmentations have been applied to graph contrastive learning (You et al., 2021; Yin et al., 2022; Suresh et al., 2021; Hassani & Khasahmadi, 2022; Xie et al., 2022) and node classiﬁcation (Zhao et al., 2021; Sun et al., 2021), they have not been studied in supervised graph classiﬁcation. In this work, we propose GraphAug, a novel automated data augmentation framework for graph classiﬁcation. GraphAug automates augmentation category selection and graph transformations through a learnable augmentation model. To produce label-invariant augmentations, we optimize the model to maximize an estimated label-invariance probability with reinforcement learning. To our best knowledge, GraphAug is the ﬁrst work successfully applying automated data augmentations to generate new graph data samples for supervised graph classiﬁcation. 3.1 A UGMENTATION BY SEQUENTIAL TRANSFORMATIONS Similar to the automated image augmentation method in Ratner et al. (2017b), we consider graph augmentations as a sequential transformation process. Given a graph G0 sampled from the train- ing dataset, we map it to the augmented graph GT with a sequence of transformation functions a1,a2,...,a T generated by an automated data augmentation model g. Speciﬁcally, at the t-th step (1 ≤t ≤T), let the graph obtained from the last step be Gt−1, we ﬁrst use the augmentation model to generate at based on Gt−1, and map Gt−1 to Gt with at. In summary, this sequential augmentation process can be described as at = g(Gt−1), G t = at(Gt−1), 1 ≤t≤T. (3) In our method, a1,a2,...,a T are all selected from three categories of graph transformations: • Node feature masking (MaskNF), which sets some values in node feature vectors to zero; • Node dropping (DropNode), which drops certain portion of nodes from the input graph; • Edge perturbation (PerturbEdge), which produces the new graph by removing existing edges from the input graph and adding new edges to the input graph. 3.2 L ABEL -INVARIANT AUGMENTATIONS Most automated image augmentation methods focus on automating augmentation category selection. For instance, Ratner et al. (2017b) automate image augmentations by generating a discrete sequence from an LSTM (Hochreiter & Schmidhuber, 1997) model, and each token in the sequence represents a certain category of image transformation, such as random ﬂip and rotation. Following this setting, 3Published as a conference paper at ICLR 2023 our graph augmentation model g also selects the augmentation category at each step. Speciﬁcally, g will generate a discrete token ct representing the category of augmentation transformation at, denoting whether MaskNF, DropNode, or PerturbEdge will be used at the t-th step. We have experimented to only automate augmentation category selection and use the graph trans- formations that are uniformly operated on each graph element, such as each node, edge, or node feature. For example, the uniform DropNode will randomly drop each node in the graph with the same probability. These transformations are commonly used in other studies (You et al., 2020; Zhu et al., 2021a; Rong et al., 2020), and we call them as uniform transformations. However, we ﬁnd that this automated composition of multiple uniform transformations does not improve classiﬁca- tion performance (see Section 4.3 for details). We argue that it is because uniform transformations have equal chances to randomly modify each graph element, thus may accidentally damage signif- icant label-related information and change the label of the original data sample. For instance, in a molecular graph dataset, assuming that all molecular graphs containing a cycle are labeled as toxic because the cyclic structures are exactly the cause of toxicity. If we are using DropNode transfor- mation, dropping any node belonging to the cycle will damage this cyclic structure, and map a toxic molecule to a non-toxic one. By default, data augmentations only involve modifying data samples while labels are not changed, so data augmentations that are not label-invariant may ﬁnally produce many noisy data samples and greatly harm the training of the classiﬁcation model. We use the TRIANGLES dataset (Knyazev et al., 2019) as an example to study the effect of label- invariance. The task in this dataset is classifying graphs by the number of triangles (the cycles formed by only three nodes) contained in the graph. As shown in Figure 3 of Appendix A, the uni- form DropNode transformation is not label-invariant because it produces data samples with wrong labels through dropping nodes belonging to triangles, and the classiﬁcation accuracy is low when the classiﬁcation model is trained on these data samples. However, if we intentionally avoid dropping nodes in triangles, training the classiﬁcation model with this label-invariant data augmentation im- proves the classiﬁcation accuracy. The signiﬁcant performance gap between these two augmentation strategies clearly demonstrates the importance of label-invariance for graph augmentations. Based on the above analysis and experimental results, we can conclude that uniform transforma- tions should be avoided in designing label-invariant graph augmentations. Instead, we generate transformations for each element in the graph by the augmentation modelgin our method. Next, we introduce the detailed augmentation process in Section 3.3 and the training procedure in Section 3.4. 3.3 A UGMENTATION PROCESS Our augmentation model gis composed of three parts. They are a GNN based encoder for extracting features from graphs, a GRU (Cho et al., 2014) model for generating augmentation categories, and four MLP models for computing probabilities. We use graph isomorphism network (GIN) (Xu et al., 2019) model as the encoder. At the t-th augmentation step ( 1 ≤t ≤T), let the graph obtained from the last step be Gt−1 = (Vt−1,Et−1,Xt−1), we ﬁrst add a virtual node vvirtual into Vt−1 and add edges connecting the virtual node with all the nodes in Vt−1. In other words, a new graph G′ t−1 = (V′ t−1,E′ t−1,X′ t−1) is created from Gt−1 such that V′ t−1 = Vt−1 ∪{vvirtual}, E′ t−1 = Et−1 ∪{(vvirtual,v) :v∈Vt−1}, and X′ t−1 ∈R|V′ t−1|×d is the concatenation of Xt−1 and a trainable initial feature vector for the virtual node. We use the virtual node here to extract graph-level information because it can capture long range interactions in the graph more effectively than a pooling based readout layer (Gilmer et al., 2017). The GNN encoder performs multiple message passing operations on G′ t−1 to obtain r- dimensional embeddings {ev t−1 ∈Rr : v∈Vt−1}for nodes in Vt−1 and the virtual node embedding evirtual t−1 ∈Rr. Afterwards, the probabilities of selecting each augmentation category is computed from evirtual t−1 as qt = GRU(qt−1,evirtual t−1 ), p C t = MLPC(qt),where qt is the hidden state vector of the GRU model at thet-th step, and the MLP model MLPC outputs the probability vectorpC t ∈R3 denoting the probabilities of selecting MaskNF, DropNode, or PerturbEdge as the augmentation at the t-th step. The exact augmentation categoryctfor the t-th step is then randomly sampled from the categorical distribution with the probabilities in pC t . Finally, as described below, the computation of transformation probabilities for all graph elements and the process of producing the new graph Gt from Gt−1 vary depending on ct. 4Published as a conference paper at ICLR 2023 DropNode virtual node GNN  encoder virtual node  embedding node  embeddings GRU MaskNF PerturbEdge sample node  embeddings node features virtual node feature  masked node  features 1 0 … node  embeddings sample for each  node feature sample for  each node virtual node  embedding sample for  each edge edge embeddings Extract node embeddings Select  augmentation category Perform transformation + , + , Figure 1: An illustration of the process of producing Gt from Gt−1 with the augmentation model. • If ct is MaskNF, then for any node v ∈Vt−1, the probabilities pM t,v ∈Rd of masking each node feature of v is computed by the MLP model MLP M taking the node embedding ev t−1 as input. Afterwards, a binary vector oM t,v ∈{0,1}d is randomly sampled from the Bernoulli distribution parameterized with pM t,v. If the k-th element of oM t,v is one, i.e., oM t,v[k] = 1, the k-th node feature of vis set to zero. Such MaskNF transformation is performed for every node feature in Xt−1. • If ctis DropNode, then the probabilitypD t,v of dropping any nodev∈Vt−1 from Gt−1 is computed by the MLP model MLP D taking the node embedding ev t−1 as input. Afterwards, a binary value oD t,v ∈{0,1}is sampled from the Bernoulli distribution parameterized with pD t,v and vis dropped from Vt−1 if oD t,v = 1. Such DropNode transformation is performed for every node in Vt−1. • If ct is PerturbEdge, the transformations involve dropping some existing edges from Et−1 and adding some new edges into Et−1. We consider the set Et−1 as the droppable edge set, and we create an addable edge set Et−1, by randomly sampling at most |Et−1|addable edges from the set {(u,v) : u,v ∈Vt−1,(u,v) /∈Et−1}. For any (u,v) in Et−1, we compute the probability pP t,(u,v) of dropping it by the MLP model MLP P taking [eu t−1 + ev t−1,1] as input, where [·,·] denotes the concatenation operation. For any (u,v) in Et−1, we compute the probability pP t,(u,v) of adding an edge connecting uand vby MLPP taking [eu t−1 + ev t−1,0] as input. Afterwards, for every (u,v) ∈Et−1, we randomly sample a binary value oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v), and drop (u,v) from Et−1 if oP t,(u,v) = 1. Similarly, we randomly sample oP t,(u,v) for every (u,v) ∈Et−1 but we will add (u,v) into Et−1 if oP t,(u,v) = 1. An illustration of the process of producing Gt from Gt−1 with our augmentation model is given in Figure 1. We also provide the detailed augmentation algorithm in Algorithm 1 of Appendix B. 3.4 L ABEL -INVARIANCE OPTIMIZATION WITH REINFORCEMENT LEARNING As our objective is generating label-invariant augmentations at most times, the ideal augmentation model g should assign low transformation probabilities to graph elements corresponding to label- related information. For instance, when DropNode is used, if the dropping of some nodes will damage important graph substructures and cause label changing, the model g should assign very low dropping probabilities to these nodes. However, we cannot directly make the model learn to produce label-invariant augmentations through supervised training because we do not have ground truth labels denoting which graph elements are important and should not be modiﬁed. To tackle this issue, we implicitly optimize the model with a reinforcement learning based training method. We formulate the sequential graph augmentations as a Markov Decision Process (MDP). This is intuitive and reasonable, because the Markov property is naturally satisﬁed, i.e., the output graph at any transformation step is only dependent on the input graph, not on previously performed transfor- mation. Speciﬁcally, at the t-th augmentation step, we deﬁne Gt−1, the graph obtained from the last step, as the current state, and the process of augmenting Gt−1 to Gt is deﬁned as state transition. The action is deﬁned as the augmentation transformation at generated from the model g, which includes the augmentation category ct and the exact transformations performed on all elements of Gt−1. The probability p(at) of taking action at for different ct is is described as below. 5Published as a conference paper at ICLR 2023 • If ct is MaskNF, then the transformation probability is the product of masking or unmasking probabilities for features of all nodes in Vt−1, so p(at) is deﬁned as p(at) =p(ct) ∗ ∏ v∈Vt−1 d∏ k=1 ( pM t,v[k] )oM t,v[k] ( 1 −pM t,v[k] )1−oM t,v[k] . (4) • If ct is DropNode, then the transformation probability is the product of dropping or non-dropping probabilities for all nodes in Vt−1, so p(at) is deﬁned as p(at) =p(ct) ∗ ∏ v∈Vt−1 ( pD t,v )oD t,v ( 1 −pD t,v )1−oD t,v . (5) • If ct is PerturbEdge, then the transformation probability is the product of perturbing or non- perturbing probabilities for all edges in Et−1 and Et−1, so p(at) is deﬁned as p(at) =p(ct) ∗ ∏ (u,v)∈Et−1∪Et−1 ( pP t,(u,v) )oP t,(u,v) ( 1 −pP t,(u,v) )1−oP t,(u,v) . (6) We use the predicted label-invariance probabilities from a reward generation modelsas the feedback reward signal in the above reinforcement learning environment. We use graph matching network (Li et al., 2019) as the backbone of the reward generation model s (see Appendix C for detailed in- troduction). When the sequential augmentation process starting from the graph G0 ends, s takes (G0,GT) as inputs and outputs s(G0,GT), which denotes the probability that the label is invariant after mapping the graph G0 to the graph GT. We use the logarithm of the predicted label-invariance probability, i.e., RT = logs(G0,GT), as the return of the sequential augmentation process. Then the augmentation model gis optimized by the REINFORCE algorithm (Sutton et al., 2000), which updates the model by the policy gradient ˆgθ computed as ˆgθ = RT∇θ ∑T t=1 log p(at), where θ denotes the trainable parameters of g. Prior to training the augmentation model g, we ﬁrst train the reward generation model on manually sampled graph pairs from the training dataset. Speciﬁcally, a graph pair (G1,G2) is ﬁrst sampled from the dataset and passed into the reward generation model to predict the probability that G1 and G2 have the same label. Afterwards, the model is optimized by minimizing the binary cross entropy loss. During the training of the augmentation model g, the reward generation model is only used to generate rewards, so its parameters are ﬁxed. See Algorithm 2 and 3 in Appendix B for the detailed training algorithm of reward generation model and augmentation model. 3.5 D ISCUSSIONS AND RELATIONS WITH PRIOR METHODS Advantages of our method. Our method explicitly estimates the transformation probability of each graph element by the automated augmentation model, thereby eliminating the negative effect of adopting a uniform transformation probability. Also, the reinforcement learning based training method can effectively help the model detect critical label-related information in the input graph, so the model can avoid damaging it and produce label-invariant augmentations with greater chances. We will show these advantages through extensive empirical studies in Section 4.1 and 4.2. Besides, the use of sequential augmentation, i.e., multiple steps of augmentation, can naturally help produce more diverse augmentations and samples, and the downstream classiﬁcation model can beneﬁt from diverse training samples. We will demonstrate it through ablation studies in Section 4.3. Relations with prior automated graph augmentations. Several automated graph augmentation methods (You et al., 2021; Yin et al., 2022; Suresh et al., 2021; Hassani & Khasahmadi, 2022) have been proposed to generate multiple graph views for contrastive learning based pre-training. However, their augmentation models are optimized by contrastive learning objectives, which are not related to graph labels. Hence, their augmentation methods may still damage label-related infor- mation, and we experimentally show that they do not perform as well as GraphAug in supervised learning scenarios in Section 4.2. Though a recent study (Trivedi et al., 2022) claims that label- invariance is also important in contrastive learning, to our best knowledge, no automated graph augmentations have been proposed to preserve label-invariance in contrastive learning. Besides, we notice that a very recent study (Yue et al., 2022) also proposes a label-invariant automated augmenta- tion method named GLA for semi-supervised graph classiﬁcation. However, GLA is fundamentally 6Published as a conference paper at ICLR 2023 Table 1: The testing accuracy on the COLORS and TRIANGLES datasets with the GIN model. We report the average accuracy and standard deviation over ten runs on ﬁxed train/validation/test splits. Dataset No augmentation Uniform MaskNF Uniform DropNode Uniform PerturbEdge Uniform Mixture GraphAug COLORS 0.578±0.012 0.507±0.014 0.547±0.012 0.618±0.014 0.560±0.016 0.633±0.009 TRIANGLES0.506±0.006 0.509±0.020 0.473±0.006 0.303±0.010 0.467±0.007 0.513±0.006 Figure 2: The changing curves of average rewards and label-invariance ratios on the validation set of the COLORS and TRIANGLES datasets as the augmentation model training proceeds. The results are averaged over ten runs, and the shadow shows the standard deviation. different from GraphAug. For a graph data sample, GLA ﬁrst obtains its graph-level representation by a GNN encoder. Then the augmentations are performed by perturbing the representation vector and label-invariant representations are selected by an auxiliary classiﬁcation model. However, our GraphAug directly augments the graph data samples, and label-invariance is ensured by our pro- posed training method based on reinforcement learning. Hence, GraphAug can generate new data samples to enrich the existing training dataset while GLA cannot achieve it. Due to the space limitation, we will discuss computational cost, augmentation step number, pre- training reward generation models, limitations, and relation with more prior methods in Appendix D. 4 E XPERIMENTS In this section, we evaluate the proposed GraphAug method on two synthetic graph datasets and seven benchmark datasets. We show that in various graph classiﬁcation tasks, GraphAug can consis- tently outperform previous graph augmentation methods. In addition, we conduct extensive ablation studies to evaluate the contributions of some components in GraphAug. 4.1 E XPERIMENTS ON SYNTHETIC GRAPH DATASETS Data. We ﬁrst show that our method can indeed produce label-invariant augmentations and outper- form uniform transformations through experiments on two synthetic graph datasets COLORS and TRIANGLES, which are synthesized by running the open sourced data synthesis code1 of Knyazev et al. (2019). The task of COLORS dataset is classifying graphs by the number of green nodes, and the color of a node is speciﬁed by its node feature. The task of TRIANGLES dataset is classifying graphs by the number of triangles (three-node cycles). We use ﬁxed train/validation/test splits for experiments on both datasets. See more information about these two datasets in Appendix E.1. Setup. We ﬁrst train the reward generation model until it converges, then train the automated aug- mentation model. To check whether our augmentation model can learn to produce label-invariant augmentations, at different training iterations, we calculate the average rewards and the label- invariance ratio achieved after augmenting graphs in the validation set. Note that since we ex- plicitly know how to obtain the labels of graphs from data generation codes, we can calculate label- invariance ratio, i.e., the ratio of augmented graphs that preserve their labels. To compare GraphAug with other augmentation methods, we train a GIN (Xu et al., 2019) based classiﬁcation model with different augmentations for ten times, and report the averaged testing classiﬁcation accuracy. We compare our GraphAug method with not using any data augmentations, and four graph augmenta- tion baseline methods. Speciﬁcally, the augmentation methods using uniform MaskNF, DropNode, and PerturbEdge transformations, and a mixture of these three uniform transformations (Uniform Mixture), i.e., randomly picking one to augment graphs at each time, are used as baselines. To en- 1https://github.com/bknyaz/graph_attention_pool 7Published as a conference paper at ICLR 2023 Table 2: The performance on seven benchmark datasets with the GIN model. We report the average ROC-AUC and standard deviation over ten runs for the ogbg-molhiv dataset, and the average ac- curacy and standard deviations over three 10-fold cross-validation runs for the other datasets. Note that for JOAOv2, AD-GCL, and AutoGCL, we evaluate the augmentation methods of them under the supervised learning setting, so the numbers here are different from those in their papers. Method PROTEINS IMDB-BINARY COLLAB MUTAG NCI109 NCI1 ogbg-molhiv No augmentation0.704±0.004 0.731±0.004 0.806±0.003 0.827±0.013 0.794±0.003 0.804±0.003 0.756±0.014 Uniform MaskNF0.702±0.008 0.720±0.006 0.815±0.002 0.788±0.012 0.777±0.006 0.794±0.002 0.741±0.010Uniform DropNode0.707±0.004 0.728±0.006 0.815±0.004 0.787±0.003 0.777±0.002 0.787±0.003 0.717±0.011Uniform PerturbEdge0.668±0.006 0.728±0.007 0.816±0.003 0.764±0.008 0.555±0.014 0.545±0.006 0.755±0.013Uniform Mixture0.707±0.004 0.730±0.009 0.815±0.003 0.779±0.014 0.776±0.006 0.783±0.003 0.746±0.010 DropEdge 0.707±0.002 0.733±0.012 0.812±0.003 0.779±0.005 0.762±0.007 0.780±0.002 0.762±0.010M-Mixup 0.706±0.003 0.736±0.004 0.811±0.005 0.798±0.015 0.788±0.005 0.803±0.003 0.753±0.013G-Mixup 0.715±0.006 0.748±0.004 0.811±0.009 0.805±0.020 0.654±0.043 0.686±0.037 0.771±0.005FLAG 0.709±0.007 0.747±0.008 0.803±0.006 0.835±0.015 0.804±0.002 0.804±0.002 0.765±0.011 JOAOv2 0.700±0.003 0.707±0.008 0.688±0.003 0.775±0.016 0.675±0.003 0.670±0.006 0.744±0.014AD-GCL 0.699±0.008 0.712±0.008 0.670±0.008 0.837±0.010 0.634±0.003 0.641±0.004 0.762±0.013AutoGCL 0.684±0.008 0.707±0.007 0.745±0.002 0.783±0.022 0.705±0.003 0.737±0.002 0.704±0.016 GraphAug 0.722±0.004 0.762±0.004 0.829±0.002 0.853±0.008 0.811±0.002 0.816±0.001 0.774±0.010 sure fair comparison, we use the same hyper-parameter setting in training classiﬁcation models for all methods. See hyper-parameters and more experimental details in Appendix E.1. Results. The changing curves of average rewards and label-invariance ratios are visualized in Fig- ure 2. These curves show that as the training proceeds, our model can gradually learn to obtain higher rewards and produce augmentations leading to higher label-invariance ratio. In other words, they demonstrate that our augmentation model can indeed learn to produce label-invariant augmentations after training. The testing accuracy of all methods on two datasets are presented in Table 1. From the results, we can clearly ﬁnd using some uniform transformations that do not satisfy label-invariance, such as uniform MaskNF on the COLORS dataset, achieve much worse performance than not us- ing augmentations. However, using augmentations produced by the trained GraphAug models can consistently achieve the best performance, which demonstrates the signiﬁcance of label-invariant augmentations to improving the performance of graph classiﬁcation models. We further study the training stability and generalization ability of GraphAug models, conduct an exploration experiment about training GraphAug models with adversarial learning, compare with some manually designed label-invariant augmentations, and compare label-invariance ratios with baseline methods on the COLORS and TRIANGLES datasets. See Appendix F.1, F.2, F.3, and F.6 for details. 4.2 E XPERIMENTS ON GRAPH BENCHMARK DATASETS Data. We further demonstrate the advantages of our GraphAug method over previous graph aug- mentation methods on six widely used datasets from the TUDatasets benchmark (Morris et al., 2020), including MUTAG, NCI109, NCI1, PROTEINS, IMDB-BINARY , and COLLAB. We also conduct experiments on the ogbg-molhiv dataset, which is a large molecular graph dataset from the OGB benchmark (Hu et al., 2020). See more information about datasets in Appendix E.2. Setup. We evaluate the performance by testing accuracy for the six datasets of the TUDatasets benchmark, and use testing ROC-AUC for the ogbg-molhiv dataset. We use two classiﬁcation mod- els, including GIN (Xu et al., 2019) and GCN (Kipf & Welling, 2017). We use the 10-fold cross- validation scheme with train/validation/test splitting ratios of 80%/10%/10% on the datasets from the TU-Datasets benchmark, and report the averaged testing accuracy over three different runs. For the ogbg-molhiv dataset, we use the ofﬁcial train/validation/test splits and report the averaged testing ROC-AUC over ten runs. In addition to the baselines in Section 4.1, we also compare with previ- ous graph augmentation methods, including DropEdge (Rong et al., 2020), M-Mixup (Wang et al., 2021), G-Mixup (Han et al., 2022), and FLAG (Kong et al., 2022). Besides, we compare with three automated augmentations proposed for graph self-supervised learning, including JOAOv2 (You et al., 2021), AD-GCL (Suresh et al., 2021), and AutoGCL (Yin et al., 2022). Note that we take their trained augmentation modules as the data augmenter, and evaluate the performance of supervised classiﬁcation models trained on the samples produced by these data augmenters. For fair compar- ison, we use the same hyper-parameter setting in training classiﬁcation models for GraphAug and baseline methods. See hyper-parameters and more experimental details in Appendix E.2. 8Published as a conference paper at ICLR 2023 Table 3: Results of ablation studies about learnable and sequential augmentation. We report the av- erage accuracy and standard deviation over three 10-fold cross-validation runs with the GIN model. Method PROTEINS IMDB-BINARY NCI1 GraphAug w/o learnable graph transformation 0.696±0.006 0.724 ±0.003 0.760 ±0.003 GraphAug w/o learnable category selection 0.702±0.004 0.746 ±0.009 0.796 ±0.006 GraphAug w/o sequential augmentation 0.712±0.002 0.753 ±0.003 0.809 ±0.004 GraphAug 0.722±0.004 0.762 ±0.004 0.816 ±0.001 Results. The performance of different methods on all seven datasets with the GIN model is sum- marized in Table 2, and see Table 9 in Appendix F.4 for the results of the GCN model. According to the results, our GraphAug method can achieve the best performance among all graph augmenta- tion methods over seven datasets. Similar to the results in Table 1, for molecule datasets including MUTAG, NCI109, NCI1, and ogbg-molhiv, using some uniform transformations based augmenta- tion methods dramatically degrades the classiﬁcation accuracy. On the other hand, our GraphAug method consistently outperforms baseline methods, such as mixup methods and existing automated data augmentations in graph self-supervised learning. The success on graph benchmark datasets once again validates the effectiveness of our proposed GraphAug method. 4.3 A BLATION STUDIES In addition to demonstrating the effectiveness of GraphAug, we conduct a series of ablation experi- ments and use empirical results to answer (1) why we make augmentation automated and learnable, (2) why we use sequential, multi-step augmentation, (3) why we adopt a combination of three differ- ent transformations (MaskNF, DropNode, PerturbEdge) instead of using only one, and (4) why we use virtual nodes. We present the ablation studies (1) and (2) in this section and leave (3) and (4) in Appendix F.5. For all ablation studies, we train GIN based classiﬁcation models on the PROTEINS, IMDB-BINARY , and NCI1 datasets, and use the same evaluation pipeline as Section 4.2. Ablation on learnable graph transformation and category selection. We ﬁrst show that making the model learn to generate graph transformations for each graph element and select augmenta- tion category are both important. We compare with a variant of GraphAug that does not learn graph transformations but simply adopts uniform transformations, and another variant that randomly select the category of graph transformation, instead of explicitly predicting it. The classiﬁcation accuracy on three datasets of these two variants are presented in the ﬁrst two rows of Table 3. Results show that the performance of two variants is worse, and particularly removing learnable graph transfor- mation will signiﬁcantly degrade the performance. It is demonstrated that learning to generate graph transformations and select augmentation category are both key success factors of GraphAug. Ablation on sequential augmentation. We next show the advantage of sequential augmentation over one-step augmentation. We compare with the variant of GraphAug that performs only one step of augmentation, i.e., with the augmentation step number T=1, and present its performance in the third row of Table 3. It is clear that using one step of augmentation will result in worse performance over all datasets. We think this demonstrates that the downstream classiﬁcation model will beneﬁt from the diverse training samples generated from sequential and multi-step augmentation. 5 C ONCLUSIONS AND FUTURE WORK We propose GraphAug, the ﬁrst automated data augmentation framework for graph classiﬁcation. GraphAug considers graph augmentations as a sequential transformation process. To eliminate the negative effect of uniform transformations, GraphAug uses an automated augmentation model to generate transformations for each element in the graph. In addition, GraphAug adopts a reinforce- ment learning based training procedure, which helps the augmentation model learn to avoid damag- ing label-related information and produce label-invariant augmentations. Through extensive empiric studies, we demonstrate that GraphAug can achieve better performance than many existing graph augmentation methods on various graph classiﬁcation tasks. In the future, we would like to explore simplifying the current training procedure of GraphAug and applying GraphAug to other graph representation learning problems, such as the node classiﬁcation problem. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT We have provided the detailed algorithm pseudocodes in Appendix B and experimental setting de- tails in Appendix E for reproducing the results. The source codes of our method are included in DIG (Liu et al., 2021) library. REFERENCES Alessandro Bicciato and Andrea Torsello. GAMS: Graph augmentation with module swapping. In Proceedings of the 11th International Conference on Pattern Recognition Applications and Methods (ICPRAM 2022), pp. 249–255, 2022. Kyunghyun Cho, Bart van Merri ¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder– decoder for statistical machine translation. In Proceedings of the 2014 conference on em- pirical methods in natural language processing (EMNLP) , pp. 1724–1734, Doha, Qatar, Oc- tober 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL https://www.aclweb.org/anthology/D14-1179. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. AutoAugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 113–123, 2019. Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning: A survey. SIGKDD Explor. Newsl., 24(2):61–77, dec 2022. ISSN 1931-0145. doi: 10.1145/ 3575637.3575646. URL https://doi.org/10.1145/3575637.3575646. Hongyang Gao and Shuiwang Ji. Graph U-Nets. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pp. 2083–2092. PMLR, 09–15 Jun 2019. Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Doina Precup and Yee Whye Teh (eds.),Proceedings of the 34th International Conference on Machine Learning, volume 70 ofProceedings of Machine Learning Research, pp. 1263–1272, International Convention Centre, Sydney, Australia, 2017. Hongyu Guo and Yongyi Mao. Intrusion-free graph mixup.arXiv preprint arXiv:2110.09344, 2021. William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025–1035, 2017. Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-Mixup: Graph data augmentation for graph classiﬁcation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.),Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research , pp. 8230–8248. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/han22c.html. Kaveh Hassani and Amir Hosein Khasahmadi. Learning graph augmentations to learn graph repre- sentations. arXiv preprint arXiv:2201.09830, 2022. Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster AutoAugment: Learning augmentation strategies using backpropagation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 1–16, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58595-2. Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efﬁcient learning of augmentation policy schedules. In International Conference on Machine Learning, pp. 2731–2741. PMLR, 2019. Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. 10Published as a conference paper at ICLR 2023 Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems , volume 33, pp. 22118–22133. Curran As- sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf. Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceddings of the 3rd international conference on learning representations, 2015. Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net- works. In 5th International Conference on Learning Representations, 2017. Boris Knyazev, Graham W Taylor, and Mohamed Amer. Understanding attention and generalization in graph neural networks. Advances in Neural Information Processing Systems , 32:4202–4212, 2019. Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. FLAG: Adversarial data augmentation for graph neural networks. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classiﬁcation with deep con- volutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012. Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M. Robertson, and Yongxin Yang. Differentiable automatic data augmentation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 580–595, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58542-6. Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching net- works for learning the similarity of graph structured objects. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp. 3835–3845. PMLR, 09–15 Jun 2019. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast AutoAug- ment. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Asso- ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 6add07cf50424b14fdf649da87843d01-Paper.pdf. Hongyi Ling, Zhimeng Jiang, Youzhi Luo, Shuiwang Ji, and Na Zou. Learning fair graph represen- tations via automated data augmentations. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1_OGWcP1s9w. Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. Journal of Machine Learning Research, 22(240):1–9, 2021. URL http://jmlr.org/papers/v22/ 21-0343.html. Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3D molecular graphs. In International Conference on Learning Representa- tions, 2022. URL https://openreview.net/forum?id=givsRXsOt9r. Koji Maruhashi, Masaru Todoriki, Takuya Ohwa, Keisuke Goto, Yu Hasegawa, Hiroya Inakoshi, and Hirokazu Anai. Learning multi-way relations via tensor decomposition with neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. TUDataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020. URL www. graphlearning.io. 11Published as a conference paper at ICLR 2023 Joonhyung Park, Hajin Shim, and Eunho Yang. Graph Transplant: Node saliency-guided graph mixup with local structure preservation. Proceedings of the AAAI Conference on Artiﬁcial Intel- ligence, 2022. Alex Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Chris R´e. Data augmentation with Snorkel. https://www.snorkel.org/blog/tanda, 2017a. Accessed:2022-01-24. Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher R ´e. Learning to compose domain-speciﬁc transformations for data augmentation. In Proceedings of the 31st International Conference on Neural Information Processing Systems , NIPS’17, pp. 3239–3249, Red Hook, NY , USA, 2017b. Curran Associates Inc. ISBN 9781510860964. Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. DropEdge: Towards deep graph convolutional networks on node classiﬁcation. In International Conference on Learning Repre- sentations, 2020. URL https://openreview.net/forum?id=Hkx1qkrKPr. Ikuro Sato, Hiroki Nishimura, and Kensuke Yokoi. APAC: Augmented pattern classiﬁcation with neural networks. arXiv preprint arXiv:1505.03229, 2015. Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation mod- els with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 86–96, Berlin, Germany, Au- gust 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1009. URL https://aclanthology.org/P16-1009. P.Y . Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings., pp. 958–963, 2003. doi: 10.1109/ICDAR.2003.1227801. Junwei Sun, Bai Wang, and Bin Wu. Automated graph representation learning for node classiﬁca- tion. In 2021 International Joint Conference on Neural Networks (IJCNN) , pp. 1–7, 2021. doi: 10.1109/IJCNN52387.2021.9533811. Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to im- prove graph contrastive learning. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , 2021. URL https: //openreview.net/forum?id=ioyq7NsR1KJ. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural informa- tion processing systems, pp. 1057–1063, 2000. Luke Taylor and Geoff Nitschke. Improving deep learning with generic data augmentation. In 2018 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 1542–1547. IEEE, 2018. Puja Trivedi, Ekdeep Singh Lubana, Yujun Yan, Yaoqing Yang, and Danai Koutra. Augmentations in graph contrastive learning: Current methodological ﬂaws & towards better practices. In Proceed- ings of the ACM Web Conference 2022, WWW ’22, pp. 1538–1549, New York, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450390965. doi: 10.1145/3485447.3512200. URL https://doi.org/10.1145/3485447.3512200. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li `o, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=rJXMpikCZ. Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. ComENet: Towards complete and efﬁcient message passing for 3d molecular graphs. In Alice H. Oh, Alekh Agarwal, Danielle Bel- grave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022a. URL https://openreview.net/forum?id=mCzMqeWSFJ. Limei Wang, Haoran Liu, Yi Liu, Jerry Kurtin, and Shuiwang Ji. Hierarchical protein representations via complete 3d graph networks. InInternational Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=9X-hgLDLYkQ. 12Published as a conference paper at ICLR 2023 Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. GraphCrop: Subgraph cropping for graph classiﬁcation. arXiv preprint arXiv:2009.10564, 2020. Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classiﬁcation. In Proceedings of the Web Conference 2021 , WWW ’21, pp. 3663–3674, New York, NY , USA, 2021. Association for Computing Machinery. ISBN 9781450383127. doi: 10. 1145/3442381.3449796. URL https://doi.org/10.1145/3442381.3449796. Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, Yaochen Xie, Limei Wang, Lei Cai, Qi Qi, Zhuoning Yuan, Tianbao Yang, and Shuiwang Ji. Advanced graph and sequence neural net- works for molecular property prediction and drug discovery. Bioinformatics, 38(9):2579–2586, 02 2022b. ISSN 1367-4803. doi: 10.1093/bioinformatics/btac112. URL https://doi.org/ 10.1093/bioinformatics/btac112. Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: A uniﬁed review. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2022. doi: 10.1109/TPAMI.2022.3170559. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations , 2019. URL https: //openreview.net/forum?id=ryGs6iA5Km. Keqiang Yan, Yi Liu, Yuchao Lin, and Shuiwang Ji. Periodic graph transformers for crys- tal material property prediction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=pqCT3L-BU9T. Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. AutoGCL: Automated graph contrastive learning via learnable view generators. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 36(8):8892–8900, Jun. 2022. doi: 10.1609/aaai.v36i8.20871. URL https://ojs.aaai.org/index.php/AAAI/article/view/20871. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems , 33:5812–5823, 2020. Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning au- tomated. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con- ference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 12121–12132. PMLR, 18–24 Jul 2021. Shuo Yu, Huafei Huang, Minh N. Dao, and Feng Xia. Graph augmentation learning. In Companion Proceedings of the Web Conference 2022, WWW ’22, pp. 1063–1072, New York, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450391306. doi: 10.1145/3487553.3524718. URL https://doi.org/10.1145/3487553.3524718. Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. XGNN: Towards Model-Level Explanations of Graph Neural Networks , pp. 430–438. Association for Computing Machinery, New York, NY , USA, 2020. ISBN 9781450379984. URL https://doi.org/10.1145/3394486. 3403085. Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12241–12252. PMLR, 18–24 Jul 2021. Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. Label-invariant augmentation for semi-supervised graph classiﬁcation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=rg_yN3HpCp. Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial AutoAugment. In Interna- tional Conference on Learning Representations , 2020. URL https://openreview.net/ forum?id=ByxdUySKvS. 13Published as a conference paper at ICLR 2023 Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. Proceedings of the AAAI Conference on Artiﬁcial In- telligence, 35(12):11015–11023, May 2021. doi: 10.1609/aaai.v35i12.17315. URL https: //ojs.aaai.org/index.php/AAAI/article/view/17315. Tong Zhao, Gang Liu, Stephan G ¨unneman, and Meng Jiang. Graph data augmentation for graph machine learning: A survey. arXiv preprint arXiv:2202.08871, 2022. Jiajun Zhou, Jie Shen, and Qi Xuan. Data Augmentation for Graph Classiﬁcation, pp. 2341–2344. Association for Computing Machinery, New York, NY , USA, 2020a. ISBN 9781450368599. URL https://doi.org/10.1145/3340531.3412086. Jiajun Zhou, Jie Shen, Shanqing Yu, Guanrong Chen, and Qi Xuan. M-Evolve: Structural-mapping- based data augmentation for graph classiﬁcation. IEEE Transactions on Network Science and Engineering, 8(1):190–200, 2020b. Yanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An empirical study of graph contrastive learning. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021a. URL https://openreview.net/forum?id=UuUbIYnHKO. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, WWW ’21, pp. 2069–2080, New York, NY , USA, 2021b. Association for Computing Machinery. ISBN 9781450383127. doi: 10.1145/3442381.3449802. URL https://doi.org/10.1145/ 3442381.3449802. 14Published as a conference paper at ICLR 2023 A V ISUALIZATION OF DIFFERENT AUGMENTATION METHODS (a) An illustration of a data sam- ple from the TRIANGLES dataset. Red nodes represent the nodes be- longing to triangles. The label of this data sample is 4 since there are four triangles. Training without any augmentations on the TRIAN- GLES dataset achieves the average testing accuracy of 0.506 ±0.006. (b) The data sample generated by augmenting the data sample in (a) with the uniform DropNode transformation. Note that two nodes originally belonging to tri- angles are removed, and the label is changed to 1. Training with the uniform DropNode transformation achieves the average testing accu- racy of 0.473 ± 0.006. (c) The data sample generated by augmenting the data sample in (a) with the label-invariant DropN- ode transformation (the DropNode with GT method in Appendix F.3), which intentionally avoids drop- ping nodes in triangles. Training with this label-invariant augmen- tation achieves the average testing accuracy of 0.522 ± 0.007. Figure 3: Comparison of different augmentation methods on the TRIANGLES dataset. We use a GIN (Xu et al., 2019) based classiﬁcation model to evaluate different augmentation methods, and report the average accuracy and standard deviation over ten runs on a ﬁxed train/validation/test split. In (a), we show a graph data sample with 4 triangles. In (b) and (c), we the data samples generated by augmenting the data sample in (a) with two different augmentation methods. We can clearly ﬁnd that using the uniform DropNode transformation degrades the classiﬁcation performance but using the label-invariant augmentation improves the performance. 15Published as a conference paper at ICLR 2023 B A UGMENTATION AND TRAINING ALGORITHMS Algorithm 1: Augmentation Algorithm of GraphAug 1: Input: Graph G0 = (V0,E0,X0); total number of augmentation steps T; the augmentation model gcomposed of GNN-encoder, GRU, and four MLP models MLPC, MLPM, MLPD, MLPP 2: Initialize the hidden state q0 of the GRU model to zero vector 3: for t= 1to T do 4: Obtain G′ t−1 by adding a virtual node to Gt−1 5: evirtual t−1 ,{ev t−1 : v∈Vt−1}= GNN-encoder(G′ t−1) 6: qt = GRU(qt−1,evirtual t−1 ) 7: pC t = MLPC(qt) 8: Sample ct from the categorical distribution of pC t 9: if ct is MaskNF then 10: for v∈Vt−1 do 11: pM t,v = MLPM(ev t−1) 12: Sample oM t,v from the Bernoulli distribution parameterized with pM t,v 13: for k= 1to ddo 14: Set the k-th node feature of vto zero if oM t,v[k] = 1 15: else if ct is DropNode then 16: for v∈Vt−1 do 17: pD t,v = MLPD(ev t−1) 18: Sample oD t,v from the Bernoulli distribution parameterized with pD t,v 19: Drop the node vfrom Vt−1 if oD t,v = 1 20: else if ct is PerturbEdge then 21: Obtain the addable edge set Et−1 by randomly sampling at most |Et−1|addable edges from {(u,v) :u,v ∈Vt−1,(u,v) /∈Et−1} 22: for (u,v) ∈Et−1 do 23: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,1] ) 24: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 25: Drop (u,v) from Et−1 if oP t,(u,v) = 1 26: for (u,v) ∈Et−1 do 27: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,0] ) 28: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 29: Add (u,v) into Et−1 if oP t,(u,v) = 1 30: Set Gt as the outputted graph from the t-th augmentation step 31: Output GT 16Published as a conference paper at ICLR 2023 Algorithm 2: Training Algorithm of the reward generation model of GraphAug 1: Input: Graph dataset D; batch size B; learning rate α; the reward generation model swith the parameter ϕ 2: repeat 3: Sample a batch Gof Bdata samples from D 4: L= 0 5: for G∈G do 6: Randomly sample a graph G+ with the same label as Gfrom Gand a graph G−with different label as G 7: L= L−log s(G,G+) −log(1 −s(G,G−)) 8: Update the parameter ϕof sas ϕ= ϕ−α∇ϕL/B 9: until the training converges 10: Output the trained reward generation model s 17Published as a conference paper at ICLR 2023 Algorithm 3: Training Algorithm of the augmentation model of GraphAug 1: Input: Graph dataset D; batch size B; learning rate α; total number of augmentation steps T; the augmentation model gwith the parameter θcomposed of GNN-encoder, GRU, and four MLP models MLPC, MLPM, MLPD, MLPP; the trained reward generation model s 2: repeat 3: Sample a batch Gof Bdata samples from D 4: ˆgθ = 0 5: for G∈G do 6: Set G0 = (V0,E0,X0) as G 7: Initialize the hidden state q0 of the GRU model to zero vector 8: for t= 1to T do 9: Obtain G′ t−1 by adding a virtual node to Gt−1 10: evirtual t−1 ,{ev t−1 : v∈Vt−1}= GNN-encoder(G′ t−1) 11: qt = GRU(qt−1,evirtual t−1 ) 12: pC t = MLPC(qt) 13: Sample ct from the categorical distribution of pC t , set log p(at) = logpC t (ct) 14: if ct is MaskNF then 15: for v∈Vt−1 do 16: pM t,v = MLPM(ev t−1) 17: Sample oM t,v from the Bernoulli distribution parameterized with pM t,v 18: for k= 1to ddo 19: log p(at) = logp(at) +oM t,v[k] logpM t,v[k] + (1−oM t,v[k]) log(1−pM t,v[k]) 20: Set the k-th node feature of vto zero if oM t,v[k] = 1 21: else if ct is DropNode then 22: for v∈Vt−1 do 23: pD t,v = MLPD(ev t−1) 24: Sample oD t,v from the Bernoulli distribution parameterized with pD t,v 25: log p(at) = logp(at) +oD t,vlog pD t,v + (1−oD t,v) log(1−pD t,v) 26: Drop the node vfrom Vt−1 if oD t,v = 1 27: else if ct is PerturbEdge then 28: Obtain the addable edge set Et−1 by randomly sampling at most |Et−1|addable edges from {(u,v) :u,v ∈Vt−1,(u,v) /∈Et−1} 29: for (u,v) ∈Et−1 do 30: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,1] ) 31: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 32: log p(at) = logp(at) +oP t,(u,v) log pP t,(u,v) + (1−oP t,(u,v)) log(1−pP t,(u,v)) 33: Drop (u,v) from Et−1 if oP t,(u,v) = 1 34: for (u,v) ∈Et−1 do 35: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,0] ) 36: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 37: log p(at) = logp(at) +oP t,(u,v) log pP t,(u,v) + (1−oP t,(u,v)) log(1−pP t,(u,v)) 38: Add (u,v) into Et−1 if oP t,(u,v) = 1 39: Set Gt as the outputted graph from the t-th augmentation step 40: ˆgθ = ˆgθ + logs(G0,GT)∇θ ∑T t=1 log p(at) 41: Update the parameter θof gas θ= θ+ αˆgθ/B. 42: until the training converges 43: Output the trained augmentation model g 18Published as a conference paper at ICLR 2023 C D ETAILS OF REWARD GENERATION MODEL We use the graph matching network (Li et al., 2019) as the reward generation modelsto predict the probability s(G0,GT) that G0 and GT have the same label (here G0 is a graph sampled from the dataset, i.e., the starting graph of the sequential augmentation process, andGT is the graph produced from T steps of augmentation by the augmentation model). The graph matching network takes both G0 = (V0,E0,X0) and GT = (VT,ET,XT) as input, performs multiple message operations on them with a shared GNN model separately. The computational process of the message passing for any node vin G0 at the ℓ-th layer of the model is hℓ v = UPDATE ( hℓ−1 v ,AGG ({ mℓ jv : j ∈N(v) }) ,µGT v ) , (7) which is the same as the message passing of vanilla GNNs in Equation (1) other than involving propagating the messageµGT v from the graph GT to the node vin G0. The message µGT v is extracted by an attention based module as wiv = exp ( sim ( hℓ−1 v ,hℓ−1 i )) ∑ u∈VT exp ( sim ( hℓ−1v ,hℓ−1u )),µGT v = ∑ i∈VT wiv(hℓ−1 v −hℓ−1 i ),v ∈V0, (8) where sim(·,·) computes the similarity between two vectors by dot-product. The message passing for any node inGT is similarly computed as in Equation (7), and this also involves propagating mes- sage from G0 to nodes in GT with the attention module in Equation (8). Afterwards, the graph-level representations hG0 and hGT of G0 and GT are separately obtained from their node embeddings as in Equation (2). We pass |hG0 −hGT |, the element-wise absolute deviation of hG0 and hGT , to an MLP model to compute s(G0,GT). 19Published as a conference paper at ICLR 2023 D M ORE DISCUSSIONS Discussions about computational cost. Considering a graph with|V|nodes and |E|edges, the time complexity of performing our deﬁned DropNode or MaskNF transformation on it isO(|V|), and the time complexity is O(|E|) for the PerturbEdge transformation since both edge dropping or addition is operated on at most |E|edges. Hence, the time complexity of augmenting the graph forT steps is O(T|V|+ T|E|). This time cost is affordable for most real-world applications. We test the average time used to augment a graph on each benchmark dataset used in our experiments with our trained augmentation model, see Table 10 for time results. We can ﬁnd that for most dataset, our method only takes a very small amount of time ( < 0.05 s) to augment a graph in average. Besides, during the training of the augmentation model, the computation of rewards by the reward generation model involves attention module (see Equation (9)), which causes an extra computational cost ofO(|V|2). In practice, this does not have much effect on small graphs, but may lead to large computation and memory cost on large graphs. Discussions about augmentation step number. For the number of augmentation steps T, we do not let the model to learn or decide T itself but make T a ﬁxed hyper-parameter to avoid the model being stucked in the naive solution of not doing augmentation at all (i.e., learnT = 0). This strategy is also adopted by previous image augmentation method (e.g. AutoAugment (Cubuk et al., 2019)). A larger T encourages the model to produce more diverse augmentations but makes it harder to keep label-invariance. We experimentally ﬁnd that if T ≥8, it is hard to obtain sufﬁciently high reward for the model. Hence, we tune T in [1,8] for each dataset to achieve the best trade-off between producing diverse augmentations and keeping label-invariance. Discussions about pre-training reward generation model. In our method, before training the augmentation model, we ﬁrst pre-train the reward generation model and make it ﬁxed while training the augmentation model. Such a training pipeline has both advantages and disadvantages. The ad- vantages of using the ﬁxed/pre-trained reward generation model are two-fold. (1) First, pre-training the reward generation model enables it to accurately predict whether two input graphs have the same labels or not, so that the generated reward signals can provide accurate feedback for the augmen- tation model. (2) Second, using the ﬁxed reward generation model can stabilize the training of the augmentation model in practice. As we shown in Appendix F.2, if the reward generation model is not ﬁxed and jointly trained with the augmentation model, the training becomes unstable and mod- els consistently diverge. The disadvantage of pre-training the reward generation model is that this training pipeline is time-consuming, because we have to train two models every time to obtain the ﬁnally usable graph augmenter. Limitations of our method. There are some limitations in our method. (1) First, our method adopts a complicated two-step training pipeline which ﬁrst trains the reward generation model and then trains the augmentation model. We have tried simplifying it to one-step training through adversarial training as in Ratner et al. (2017b). However, we found it to be very unstable and the augmenta- tion model consistently diverges (see Appendix F.2 for an exploration experiment about adversarial training on the COLORS and TRIANGLES dataset). We leave the problem of simplifying the train- ing to the future. (2) Second, our augmentation method will take extra computational cost in both training the augmentation model and providing augmented samples for the downstream graph clas- siﬁcation training. The time and resource cost of training models can be large on the large datasets. For instance, on the ogbg-molhiv dataset, we ﬁnd it takes the total time of around 10 hours to train the reward generation model and augmentation model before we obtain a ﬁnally usable graph aug- menter. Given that the performance improvement is not signiﬁcant on the ogbg-molhiv dataset, such a large time cost is not a worthwhile investment. Our GraphAug mainly targets on improving the graph classiﬁcation performance by generating more training data samples for the tasks with small datasets, particularly for those that need huge cost to manually collect and label new data samples. But for the classiﬁcation task with sufﬁcient training data, the beneﬁts of using GraphAug are limited and not worth the large time and resource cost to train GraphAug models. Relations with automated image augmentations. GraphAug are somehow similar to some auto- mated image augmentations (Cubuk et al., 2019; Zhang et al., 2020) in that they both use sequential augmentation and reinforcement learning based training. However, they are actually fundamen- tally different. Label-invariance is not a problem in automated image augmentations because the used image transformations ensure label-invariance. On the other hand, as discussed in Section 3.2, 20Published as a conference paper at ICLR 2023 it is non-trivial to make graph transformations ensure label-invariance. In GraphAug, the learn- able graph transformation model and the reinforcement learning based training are used to produce label-invariant augmentations, which are actually the main contribution of GraphAug. Another fundamental difference between GraphAug and automated image augmentations lies in the reward design. Many automated image augmentation methods, such as AutoAugment (Cubuk et al., 2019), train a child network model on the training data and use the achieved classiﬁcation accuracy on the validation data as the reward. Instead, our GraphAug uses the label-invariance probability predicted by the reward generation model as the reward signal to train the augmentation model. We argue that such reward design has several advantages over using the classiﬁcation accuracy as the reward. (1) First, maximizing the label-invariance probability can directly encourage the augmentation model to produce label-invariant augmentations. However, the classiﬁcation accuracy is not directly related to label-invariance, so using it as the reward feedback does not necessarily make the augmentation model learn to ensure label-invariance. (2) Second, predicting the label-invariance probability only needs one simple model inference process that is computationally cheap, while obtaining the clas- siﬁcation accuracy is computationally expensive because it needs to train a model from scratch. (3) Most importantly, our reward generation scheme facilitates the learning of the augmentation model by providing the reward feedback for every individual graph . Even in the same dataset, the label-related structures or patterns in different graphs may vary a lot, hence, good augmentation strategies for different graphs can be different. However, the classiﬁcation accuracy evaluates the classiﬁcation performance when using the produced graph augmentations to train models on the overall dataset, which does not provide any feedback about whether the produced augmentation on every individual graph sample is good or not. Differently, the label-invariance probability is com- puted for every individual graph sample, thereby enabling the model to capture good augmentation strategies for every individual graph. Considering these advantages, we do not use the classiﬁcation accuracy but take the label-invariance probability predicted by the reward generation model as the reward. Overall, GraphAug cannot be considered as a simple extension of automated image augmentations to graphs. Relation with prior graph augmentation methods. In addition to GLA (Yue et al., 2022), we also notice Graphair (Ling et al., 2023), another recently proposed automated graph augmentation method. However, Graphair aims to produce fairness-aware graphs for fair graph representation learning, while our method is proposed for graph classiﬁcation. Additionally, graph mixup methods (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) synthesize a new graph or graph representation from two input graphs. Because the new data sample is assigned with the combination of labels of two input graphs, mixup operations are supposed to detect and mix the label-related information of two graphs (Guo & Mao, 2021). However, our method is simpler and more intuitive because it only needs to detect and preserve the label-related information of one in- put graph. In addition, another method FLAG (Kong et al., 2022) can only augment node features, while our method can produce augmentations in node features, nodes and edges. Besides, similar to the motivation of our GraphAug, some other studies have also found that preserving important structures or node features is signiﬁcant in designing effective graph augmentations. A pioneering method in this direction is GCA (Zhu et al., 2021b), which proposes to identify important edges and node features in the graph by node centralities. GCA augments the graph by random edge dropping and node feature masking, but assigns lower perturbation probabilities to the identiﬁed important edges and node features. Also, other studies (Wang et al., 2020; Bicciato & Torsello, 2022; Zhou et al., 2020b) assume that some motif or subgraph structures in the graph is signiﬁcant, and propose to augment graphs by manually designed transformations to avoid removing them. Overall, these augmentations are based on some rules or assumptions about how to preserve important structures of the input graph. Differently, our GraphAug method does not aim to deﬁne a ﬁxed graph aug- mentation strategy for every graph. Instead, it seeks to make the augmentation model ﬁnd good augmentation strategies automatically with reinforcement learning based training. Relations with graph explainability. Our method is related to graph explainability in that the predicted transformation probabilities from our augmentation model g is similar to explainability scores of some graph explainability methods (Maruhashi et al., 2018; Yuan et al., 2020; 2021). Hence, we hope that our augmentation method can bring inspiration to researchers in the graph explainability area. 21Published as a conference paper at ICLR 2023 Table 4: Statistics of graph benchmark datasets. Datasets # graphs Average # nodes Average # edges # classes PROTEINS 1113 39.06 72.82 2 IMDB-BINARY 1000 19.77 96.53 2 COLLAB 5000 74.49 2457.78 3 MUTAG 188 17.93 19.79 2 NCI109 4127 29.68 32.13 2 NCI1 4110 29.87 32.30 2 ogbg-molhiv 41,127 25.5 27.5 2 Table 5: Some hyper-parameters for the reward generation model and its training. Datasets # layers batch size # training epochs PROTEINS 6 32 420 IMDB-BINARY 6 32 320 COLLAB 5 8 120 MUTAG 5 32 230 NCI109 5 32 200 NCI1 5 32 200 ogbg-molhiv 5 32 200 E M ORE DETAILS ABOUT EXPERIMENTAL SETTING E.1 E XPERIMENTS ON SYNTHETIC GRAPH DATASETS Data information. We synthesize the COLORS and TRIANGLES dataset by running the open sourced data synthesis code of Knyazev et al. (2019). For the COLORS dataset, we synthesize 8000 graphs for training, 1000 graphs for validation, and 1000 graphs for testing. For the TRIANGLES dataset, we synthesize 30000 graphs for training, 5000 graphs for validation, and 5000 graphs for testing. The labels of all data samples in both datasets belong to {1,..., 10}. Details of the model and training. The Adam optimizer (Kingma & Ba, 2015) is used for the training of all models. For both datasets, we use a reward generation model with 5 layers and the hidden size of 256, and the graph level embedding is obtained by sum pooling. It is trained for 1 epoch on the COLORS dataset and 200 epochs on the TRIANGLES dataset. The batch size is 32 and the learning rate is 0.0001. For the augmentation model, we use a GIN model with 3 layers and the hidden size of 64 for GNN encoder, an MLP model with 2 layers, the hidden size of 64, and ReLU as the non-linear activation function for MLP C, and an MLP model with 2 layers, the hidden size of 128, and ReLU as the non-linear activation function for MLPM, MLPD, and MLPP. The augmentation model is trained for 5 epochs with the batch size of 32 and the learning rate of 0.0001 on both datasets. To stabilize the training of the augmentation model, we manually control the augmentation model to only modify 5% of graph elements at each augmentation step during the training. On the COLORS dataset, we use a classiﬁcation model where the number of layers is 3, the hidden size is 128, and the readout layer is max pooling. On the TRIANGLES dataset, we use a classiﬁcation model where the number of layers is 3, the hidden size is 64, and the readout layer is sum pooling. On both datasets, we set the training batch size as 32 and the learning rate as 0.001 when training classiﬁcation models, and all classiﬁcation models are trained for 100 epochs. E.2 E XPERIMENTS ON GRAPH BENCHMARK DATASETS Data information. We use six datasets from the TUDatasets benchmark (Morris et al., 2020), in- cluding three molecule datasets MUTAG, NCI109, NCI1, one bioinformatics dataset PROTEINS, and two social network datasets IMDB-BINARY and COLLAB. We also use the ogbg-molhiv 22Published as a conference paper at ICLR 2023 Table 6: Some hyper-parameters for the augmentation model and its training. Datasets # augmentation steps T batch size # training epochs PROTEINS 2 32 30 IMDB-BINARY 8 32 30 COLLAB 8 32 10 MUTAG 4 16 200 NCI109 2 32 20 NCI1 2 32 20 ogbg-molhiv 2 128 10 Table 7: Some hyper-parameters for the classiﬁcation model and its training. Datasets # layers hidden size batch size PROTEINS 3 128 32 IMDB-BINARY 4 128 32 COLLAB 4 64 32 MUTAG 4 128 16 NCI109 4 128 32 NCI1 3 128 32 ogbg-molhiv 5 300 32 dataset from the OGB benchmark (Hu et al., 2020). See Table 4 for the detailed statistics of all benchmark datasets used in our experiments. Details of model and training. The Adam optimizer (Kingma & Ba, 2015) is used for training of all models. For all six datasets, we set the hidden size as 256 and the readout layer as sum pooling for the reward generation model, and the reward generation model is trained using 0.0001 as the learning rate. See other hyper-parameters about the reward generation model and its training in Table 5. The hyper-parameters of the augmentation model is the same as those in experiments of synthetic graph datasets and the learning rate is 0.0001 during its training, but we tune the batch size, the training epochs and the number of augmentation steps T on each dataset. See Table 6 for the optimal values of them on each dataset. The strategy of modifying only 5% of graph elements is also used during the training of augmentation models. Besides, for classiﬁcation models, we set the readout layer as mean pooling, and tune the number of layers, the hidden size, and the training batch size. See Table 7 for these hyper-parameters. All classiﬁcation models are trained for 100 epochs with the learning rate of 0.001. 23Published as a conference paper at ICLR 2023 Figure 4: The changing curves of training and validation loss on the COLORS and TRIANGLES datasets when training the reward generation model of GraphAug with Algorithm 2. The results are averaged over ten runs, and the shadow shows the standard deviation. Figure 5: The changing curves of training and validation rewards on the COLORS and TRIANGLES datasets when training the augmentation model of GraphAug with Algorithm 3. The results are averaged over ten runs, and the shadow shows the standard deviation. Figure 6: The changing curves of training rewards of augmentation model and training loss of reward generation model when training two models together with adversarial learning on the COLORS dataset. The results are averaged over ten runs, and the shadow shows the standard deviation. F M ORE EXPERIMENTAL RESULTS F.1 S TUDY OF TRAINING STABILITY AND GENERALIZATION Taking the COLORS and TRIANGLES datasets as examples, we show the learning curves of reward generation models and augmentation models in Figure 4 and Figure 5, respectively. The learning curves on the training set show that the training is generally very stable for both reward generation models and augmentation models since no sharp oscillation happens. Comparing the learning curves on the training and validation set, we can ﬁnd that on the COLORS dataset, the curves converge to around the same loss and rewards on the training and validation set when the training converges. Hence, reward generation model and the augmentation model both have very good generalization abilities. Differently, on more complicated TRIANGLES dataset, slight overﬁtting exists for both models but the overall generalization ability is still acceptable. Actually, to eliminatee the negative effect of overﬁtting, we always take the reward generation model with lowest validation loss and the augmentation model with highest validation reward in our experiments. In a word, our studies about training stability and generalization show that both the reward generation model and augmentation model can be trained stably and have acceptable generalization ability. 24Published as a conference paper at ICLR 2023 Table 8: The testing accuracy on the COLORS and TRIANGLES datasets with the GIN model. We report the average accuracy and standard deviation over ten runs on ﬁxed train/validation/test splits. Dataset No augmentation MaskNF with GT DropNode with GT PerturbEdge with GT GraphAug COLORS 0.578±0.012 0.627±0.013 0.627 ±0.017 n/a 0.633±0.009 TRIANGLES 0.506±0.006 n/a 0.522 ±0.007 0.524 ±0.006 0.513±0.006 F.2 A DVERSARIAL TRAINING EXPERIMENT One possible one-stage training alternative of GraphAug is the adversarial training strategy in Ratner et al. (2017b). Speciﬁcally, the augmentation model is trained jointly with the reward generation model. We construct the positive graph pair (G,G+) sampled from the dataset in which G and G+ have the same label, and use the augmentation model to augment the graph Gto G−and form the negative graph pair (G,G−). The reward generation model is then trained to minimize the loss function L= −log s(G,G+) −log (1−s(G,G−)), but the augmentation model is trained to maximize the reward log s(G,G−) received from the reward generation model. In this adversarial training method, the reward generation model can actually be considered as the discriminator model. We conduct an exploration experiment of it on the COLORS and TRIANGLES datasets, but we ﬁnd that this strategy cannot work well on both datasets. On the TRIANGLES dataset, gradient explosion consistently happens during the training but we have not yet ﬁgure out how to ﬁx it. On the COLORS dataset, we show the learning curves of two models in Figure 6. Note that different from the learning curves of GraphAug in Figure 2, the augmentation model diverges and fails to learn to obtain more rewards as the training proceeds. In other words, the augmentation model struggles to learn to generate new graphs that can deceive the reward generation model. Given these existing problems in adversarial learning, we adopts the two-stage training pipeline in GraphAug and leaves the problem of simplifying the training to the future. F.3 C OMPARISON WITH MANUALLY DESIGNED LABEL -INVARIANT AUGMENTATIONS An interesting question is how does our GraphAug compare with the manually designed label- invariant augmentation methods (assuming we can design them from some domain knowledge)? We try answering this question by empirical studies on COLORS and TRIANGLES datasets. Since we explicitly know how the labels of graphs are obtained from their data generation codes, we can design some label-invariant augmentation strategies. We compare GraphAug with three designed label-invariant augmentation methods, which are based on MaskNF, DropNode, and PerturbEdge transformations intentionally avoiding damaging label-related information. Speciﬁcally, for the COLORS dataset, we compare with MaskNF that uniformly masks the node features other than the color feature, and DropNode that uniformly drops the nodes other than green nodes. In other words, they are exactly using the ground truth labels indicating which graph elements are label-related in- formation, so we call them as MaskNF with GT and DropNode with GT. Note that no PerturbEdge with GT is deﬁned on the COLORS dataset because the modiﬁcation of edges naturally ensures label-invariance. Similarly, for the TRIANGLES dataset, we compare with DropNode with GT and PerturbEdge with GT which intentionally avoid damaging any nodes or edges in triangles. The performance of no augmentation baseline, three manually designed augmentation methods, and our GraphAug method is summarized in Table 8. It is not surprising that all augmentation meth- ods can outperform no augmentation baseline since they all can produce label-invariant training samples. Interestingly, GraphAug is a competitive method compared with these manually designed label-invariant methods. GraphAug outperforms manually designed augmentations on the COLORS dataset but fails to do it on the TRIANGLES dataset. We ﬁnd that is because GraphAug model se- lects MaskNF with higher chances than DropNode and PerturbEdge, but graph classiﬁcation models beneﬁts more from diverse topology structures produced by DropNode and PerturbEdge transforma- tions. Note that although our GraphAug may not show signiﬁcant advantages over manually designed label-invariant augmentations on these two synthetic datasets, in most scenarios, de- signing such label-invariant augmentations is impossible because we do not know which graph 25Published as a conference paper at ICLR 2023 Table 9: The performance on seven benchmark datasets with the GCN model. We report the aver- age ROC-AUC and standard deviation over ten runs for the ogbg-molhiv dataset, and the average accuracy and standard deviations over three 10-fold cross-validation runs for the other datasets. Method PROTEINS IMDB-BINARY COLLAB MUTAG NCI109 NCI1 ogbg-molhiv No augmentation0.711±0.003 0.734±0.010 0.797±0.002 0.803±0.016 0.742±0.004 0.731±0.002 0.761±0.010 Uniform MaskNF0.716±0.001 0.723±0.006 0.802±0.002 0.765±0.017 0.734±0.005 0.729±0.004 0.745±0.011 Uniform DropNode0.714±0.005 0.733±0.001 0.798±0.002 0.759±0.007 0.727±0.003 0.722±0.003 0.723±0.012 Uniform PerturbEdge0.694±0.003 0.732±0.010 0.795±0.003 0.744±0.004 0.634±0.006 0.638±0.011 0.746±0.013 Uniform Mixture0.714±0.003 0.734±0.009 0.797±0.004 0.754±0.015 0.731±0.002 0.722±0.002 0.743±0.011 DropEdge 0.710±0.006 0.735±0.013 0.797±0.004 0.762±0.003 0.724±0.004 0.723±0.003 0.757±0.012 M-Mixup 0.714±0.004 0.728±0.007 0.794±0.003 0.783±0.007 0.739±0.005 0.741±0.002 0.753±0.014 G-Mixup 0.724±0.006 0.749±0.010 0.800±0.027 0.799±0.004 0.509±0.005 0.506±0.005 0.763±0.008 FLAG 0.723±0.003 0.743±0.008 0.797±0.002 0.819±0.004 0.746±0.003 0.734±0.004 0.768±0.010 JOAOv2 0.722±0.003 0.687±0.010 0.681±0.004 0.736±0.007 0.691±0.007 0.672±0.004 0.722±0.009 AD-GCL 0.691±0.011 0.697±0.011 0.612±0.004 0.665±0.001 0.634±0.003 0.641±0.004 0.752±0.013 AutoGCL 0.668±0.008 0.719±0.002 0.745±0.002 0.769±0.022 0.707±0.002 0.714±0.005 0.701±0.014 GraphAug 0.736±0.007 0.764±0.008 0.808±0.001 0.832±0.005 0.760±0.003 0.748±0.002 0.774±0.010 Table 10: Average augmentation time per graph with the trained augmentation model. Method PROTEINS IMDB-BINARY COLLAB MUTAG NCI109 NCI1 ogbg-molhiv JOAOv2 0.0323s 0.0854s 0.2846s 0.0397s 0.0208s 0.0223s 0.0299s AD-GCL 0.0127s 0.0418s 0.1478s 0.0169s 0.0092s 0.0083s 0.0115s AutoGCL 0.0218s 0.0643s 0.2398s 0.0256s 0.0162s 0.0168s 0.0221s GraphAug 0.0073s 0.0339s 0.1097s 0.0136s 0.0075s 0.0078s 0.0106s elements are label-related. However, our GraphAug can still work in these scenarios because it can automatically learn to produce label-invariant augmentations. F.4 M ORE EXPERIMENTAL RESULTS ON GRAPH BENCHMARK DATASETS The performance of different augmentation methods on all seven datasets with the GCN model is presented in Table 9. Besides, to quantify and compare the computational cost of our method and some automated graph augmentation baseline methods on each dataset, we test the average time they use to augment each graph and summarize the average augmentation time results in Table 10. For most dataset, our method only takes a very small amount of time (<0.05s) to augment a graph in average, which is an acceptable time cost for most real-world applications. In addition, from Table 10, we can clearly ﬁnd that among all automated graph augmentation methods, our GraphAug takes the least average runtime to augment graphs. For the other baseline methods in Table 2, because they do not need the computation with neural networks in augmentations, their runtime is unsurprisingly lower ( < 0.001s per graph). However, the classiﬁcation performance of them is consistently worse than our GraphAug. Overall, our GraphAug achieves the best classiﬁcation performance, and its time cost is the lowest among all automated graph augmentations. F.5 M ORE ABLATION STUDIES Ablation on combining three different transformations. In our method, we use a combination of three different graph transformations, including MaskNF, DropNode, and PerturbEdge. Our GraphAug model are designed to automatically select one of them at each augmentation step. Here we explore how the performance will change if only one category of graph transformation is used. Speciﬁcally, we compare with three variants of GraphAug that only uses learnable MaskNF, DropN- ode, and PerturbEdge, whose performance are listed in the ﬁrst three rows of Table 11. We can ﬁnd that sometimes using a certain category of learnable augmentation gives very good results, e.g., learnable DropNode on the NCI1 dataset. However, not all categories can achieve it, and actually the optimal category varies among datasets because graph structure distributions or modalities are very 26Published as a conference paper at ICLR 2023 Table 11: Results of ablation studies about combining three different transformations. We report the average accuracy and standard deviation over three 10-fold cross-validation runs with the GIN model. Method PROTEINS IMDB-BINARY NCI1 GraphAug with only learnable MaskNF transformation0.712±0.001 0.751 ±0.002 0.809 ±0.002 GraphAug with only learnable DropNode transformation0.716±0.003 0.752 ±0.005 0.814 ±0.002 GraphAug with only learnable PerturbEdge transformation0.702±0.009 0.754 ±0.005 0.780 ±0.001 GraphAug 0.722±0.004 0.762 ±0.004 0.816 ±0.001 Table 12: Results of ablation studies about using virtual nodes. We report the average accuracy and standard deviation over three 10-fold cross-validation runs with the GIN model. Method PROTEINS IMDB-BINARY NCI1 GraphAug with sum pooling 0.711±0.005 0.750 ±0.008 0.788 ±0.004 GraphAug with mean pooling 0.711±0.004 0.752 ±0.004 0.801 ±0.005 GraphAug with max pooling 0.713±0.002 0.737 ±0.005 0.795 ±0.005 GraphAug with virtual nodes 0.722±0.004 0.762 ±0.004 0.816 ±0.001 Table 13: The label-invariance ratios on the test sets of COLORS and TRIANGLES datasets. Dataset Uniform MaskNF Uniform DropNode Uniform PerturbEdge Uniform Mixture GraphAug COLORS 0.3547 0.3560 1.0000 0.5645 0.9994 TRIANGLES 1.0000 0.6674 0.1957 0.6181 1.0000 different in different datasets. Nonetheless, GraphAug can consistently achieve good performance without manually searching the optimal category on different datasets. Hence, combining different transformations makes it easier for the GraphAug model to adapt to different graph datasets than using only one category of transformation. Ablation on using virtual nodes. In our method, virtual nodes are used to capture graph-level representation and predict augmentation categories due to two advantages. (1) First, in the message passing process of GNNs, virtual nodes can help propagate messages among far-away nodes in the graph. (2) Second, virtual nodes can learn to more effectively capture graph-level representations through aggregating more information from important nodes or structures in the graph (similar to the attention mechanism). In fact, many prior studies (Gilmer et al., 2017; Hu et al., 2020) have demonstrated the advantages of using virtual nodes in graph representation learning. To justify the advantages of using virtual nodes in GraphAug, we compare the performance of taking different ways to predict the augmentation category in an ablation experiment. Speciﬁcally, we evaluate the performance of GraphAug model variants in which virtual nodes are not used, but the augmentation category is predicted from the graph-level representations obtained by sum pooling, mean pooling, or max pooling. The results of them are summarized in the ﬁrst three rows of Table 12. From the results, we can ﬁnd that using virtual nodes achieves the best performance, hence it is the best option. F.6 E VALUATION OF LABEL -INVARIANCE PROPERTY We evaluate the label-invariance ratios of our GraphAug method and the baseline methods used in Table 1 on the test sets of two synthetic datasets. The results are summarized in Table 13. Since the label is deﬁned as the number of nodes with green colors (indicated by node features) in the COLORS dataset, Uniform DropNode and Uniform PerturbEdge will destroy label-related informa- tion and achieve a very low label-invariance ratio. Similarly, the label is deﬁned as the number of 3-cycles in the TRIANGLES dataset, Uniform DropNode and Uniform PerturbEdge also achieve a 27Published as a conference paper at ICLR 2023 very low label-invariance ratio. However, our GraphAug can achieve a very high label-invariance ratio of close to 1.0 on both datasets. Besides, combining with the classiﬁcation performance in Table 1, we can ﬁnd that only the augmentations with high label-invariance ratios can outperform no augmentation baseline. This phenomenon demonstrates that label-invariance is signiﬁcant to achieve effective graph augmentations. 28",
      "references": [
        "GAMS: Graph augmentation with module swapping",
        "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
        "AutoAugment: Learning augmentation strategies from data",
        "Data augmentation for deep graph learning: A survey",
        "Graph U-Nets",
        "Neural message passing for quantum chemistry",
        "Intrusion‐free graph mixup",
        "Inductive representation learning on large graphs",
        "G-Mixup: Graph data augmentation for graph classiﬁcation",
        "Learning graph augmentations to learn graph representations",
        "Faster AutoAugment: Learning augmentation strategies using backpropagation",
        "Population based augmentation: Efﬁcient learning of augmentation policy schedules",
        "Long short-term memory",
        "Open graph benchmark: Datasets for machine learning on graphs",
        "Adam: A method for stochastic optimization",
        "Semi-supervised classiﬁcation with graph convolutional networks",
        "Understanding attention and generalization in graph neural networks",
        "FLAG: Adversarial data augmentation for graph neural networks",
        "ImageNet classiﬁcation with deep convolutional neural networks",
        "Differentiable automatic data augmentation",
        "Graph matching networks for learning the similarity of graph structured objects",
        "Fast AutoAugment",
        "Learning fair graph representations via automated data augmentations",
        "DIG: A turnkey library for diving into graph deep learning research",
        "Spherical message passing for 3D molecular graphs",
        "Learning multi-way relations via tensor decomposition with neural networks",
        "TUDataset: A collection of benchmark datasets for learning with graphs",
        "Graph Transplant: Node saliency-guided graph mixup with local structure preservation",
        "Data augmentation with Snorkel",
        "Learning to compose domain-specific transformations for data augmentation",
        "DropEdge: Towards deep graph convolutional networks on node classiﬁcation",
        "APAC: Augmented pattern classiﬁcation with neural networks",
        "Improving neural machine translation models with monolingual data",
        "Best practices for convolutional neural networks applied to visual document analysis",
        "Automated graph representation learning for node classiﬁcation",
        "Adversarial graph augmentation to im- prove graph contrastive learning",
        "Policy gradient methods for reinforcement learning with function approximation",
        "Improving deep learning with generic data augmentation",
        "Augmentations in graph contrastive learning: Current methodological ﬂaws & towards better practices",
        "Graph attention networks",
        "ComENet: Towards complete and efﬁcient message passing for 3d molecular graphs",
        "Hierarchical protein representations via complete 3d graph networks",
        "GraphCrop: Subgraph cropping for graph classiﬁcation",
        "Mixup for node and graph classiﬁcation",
        "Advanced graph and sequence neural networks for molecular property prediction and drug discovery",
        "Self-supervised learning of graph neural networks: A uniﬁed review",
        "How powerful are graph neural networks?",
        "Periodic graph transformers for crys- tal material property prediction",
        "AutoGCL: Automated graph contrastive learning via learnable view generators",
        "Graph contrastive learning with augmentations",
        "Graph contrastive learning au- tomated",
        "Graph augmentation learning",
        "XGNN: Towards Model-Level Explanations of Graph Neural Networks",
        "On explainability of graph neural networks via subgraph explorations",
        "Label-invariant augmentation for semi-supervised graph classiﬁcation",
        "Adversarial AutoAugment",
        "Data augmentation for graph neural networks",
        "Graph data augmentation for graph machine learning: A survey",
        "Data Augmentation for Graph Classiﬁcation",
        "M-Evolve: Structural-mapping- based data augmentation for graph classiﬁcation",
        "An empirical study of graph contrastive learning",
        "Graph contrastive learning with adaptive augmentation"
      ],
      "meta_data": {
        "arxiv_id": "2202.13248v4",
        "authors": [
          "Youzhi Luo",
          "Michael McThrow",
          "Wing Yee Au",
          "Tao Komikado",
          "Kanji Uchino",
          "Koji Maruhashi",
          "Shuiwang Ji"
        ],
        "published_date": "2022-02-26T23:00:34Z",
        "github_url": "https://github.com/bknyaz/graph_attention_pool"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces GraphAug, the first automated data augmentation framework designed for graph classification that emphasizes label-invariance. It addresses the challenge of preserving label-related information during graph transformations by automatically learning diverse augmentation strategies.",
        "methodology": "GraphAug employs a sequential transformation process wherein an automated augmentation model, composed of a GNN encoder, a GRU for category prediction, and multiple MLPs for transformation probabilities, generates non-uniform augmentations. The framework uses reinforcement learning to optimize augmentation parameters by maximizing an estimated label-invariance probability computed by a pre-trained reward generation model.",
        "experimental_setup": "Experiments were conducted on both synthetic datasets (COLORS, TRIANGLES) and several benchmark datasets from the TU Datasets and OGB (e.g., PROTEINS, IMDB-BINARY, COLLAB, MUTAG, NCI109, NCI1, ogbg-molhiv). The evaluation involved comparing GraphAug with various baseline methods including uniform transformations, drop-edge, and mixup strategies, using standard graph classification models (GIN, GCN) and performing multiple runs with cross-validation or fixed splits to measure accuracy and ROC-AUC.",
        "limitations": "The method requires a two-stage training pipeline that pre-trains a reward generation model before training the augmentation model, which adds complexity and computational overhead. The approach is also less beneficial for datasets with abundant training samples and may face challenges in stability if attempted with one-step or adversarial training schemes.",
        "future_research_directions": "Future work could focus on simplifying the training procedure into a more unified framework, extending the approach to other graph representation learning tasks such as node classification, and exploring strategies to reduce computational cost for large-scale graphs.",
        "experimental_code": "# File: train_test.py\nimport time\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom chebygin import *\nfrom utils import *\nfrom graphdata import *\nimport torch.multiprocessing as mp\nimport multiprocessing\ntry:\n    import ax\n    from ax.service.managed_loop import optimize\nexcept Exception as e:\n    print('AX is not available: %s' % str(e))\n\ndef set_pool(pool_thresh, args_pool):\n    pool = copy.deepcopy(args_pool)\n    for i, s in enumerate(pool):\n        try:\n            thresh = float(s)\n            pool[i] = str(pool_thresh)\n        except:\n            continue\n    return pool\n\ndef train_evaluate(datareader, args, collate_fn, loss_fn, feature_stats, parameterization, folds=10, threads=5):\n    print('parameterization', parameterization)\n    pool_thresh, kl_weight = (parameterization['pool'], parameterization['kl_weight'])\n    pool = args.pool\n    if args.tune_init:\n        scale, init = (parameterization['scale'], parameterization['init'])\n    else:\n        scale, init = (args.scale, args.init)\n    n_hidden_attn, layer = (parameterization['n_hidden_attn'], 1)\n    if layer == 0:\n        pool = copy.deepcopy(args.pool)\n        del pool[3]\n    pool = set_pool(pool_thresh, pool)\n    manager = multiprocessing.Manager()\n    val_acc = manager.dict()\n    assert threads <= folds, (threads, folds)\n    n_it = int(np.ceil(float(folds) / threads))\n    for i in range(n_it):\n        processes = []\n        if threads <= 1:\n            single_job(i * threads, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n        else:\n            for fold in range(threads):\n                p = mp.Process(target=single_job, args=(i * threads + fold, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale, init, n_hidden_attn))\n                p.start()\n                processes.append(p)\n            for p in processes:\n                p.join()\n    print(val_acc)\n    val_acc = list(val_acc.values())\n    print('average and std over {} folds: {} +- {}'.format(folds, np.mean(val_acc), np.std(val_acc)))\n    metric = np.mean(val_acc) - np.std(val_acc)\n    print('metric: avg acc - std: {}'.format(metric))\n    return metric\n\ndef ax_optimize(datareader, args, collate_fn, loss_fn, feature_stats, folds=10, threads=5, n_trials=30):\n    parameters = [{'name': 'pool', 'type': 'range', 'bounds': [0.0001, 0.02], 'log_scale': False}, {'name': 'kl_weight', 'type': 'range', 'bounds': [0.1, 10.0], 'log_scale': False}, {'name': 'n_hidden_attn', 'type': 'choice', 'values': [0, 32]}]\n    if args.tune_init:\n        parameters.extend([{'name': 'scale', 'type': 'range', 'bounds': [0.1, 2.0], 'log_scale': False}, {'name': 'init', 'type': 'choice', 'values': ['normal', 'uniform']}])\n    best_parameters, values, experiment, model = optimize(parameters=parameters, evaluation_function=lambda parameterization: train_evaluate(datareader, args, collate_fn, loss_fn, feature_stats, parameterization, folds=folds, threads=threads), total_trials=n_trials, objective_name='accuracy')\n    print('best_parameters', best_parameters)\n    print('values', values)\n    return best_parameters\n\ndef train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats=None, log=True):\n    model.train()\n    optimizer.zero_grad()\n    n_samples, correct, train_loss = (0, 0, 0)\n    alpha_pred, alpha_GT = ({}, {})\n    start = time.time()\n    for batch_idx, data in enumerate(train_loader):\n        data = data_to_device(data, args.device)\n        if feature_stats is not None:\n            data[0] = (data[0] - feature_stats[0]) / feature_stats[1]\n        if batch_idx == 0 and epoch <= 1:\n            sanity_check(model.eval(), data)\n            model.train()\n        optimizer.zero_grad()\n        mask = [data[2].view(len(data[2]), -1)]\n        output, other_outputs = model(data)\n        other_losses = other_outputs['reg'] if 'reg' in other_outputs else []\n        alpha = other_outputs['alpha'] if 'alpha' in other_outputs else []\n        mask.extend(other_outputs['mask'] if 'mask' in other_outputs else [])\n        targets = data[3]\n        loss = loss_fn(output, targets)\n        for l in other_losses:\n            loss += l\n        loss_item = loss.item()\n        train_loss += loss_item\n        n_samples += len(targets)\n        loss.backward()\n        optimizer.step()\n        time_iter = time.time() - start\n        correct += count_correct(output.detach(), targets.detach())\n        update_attn(data, alpha, alpha_pred, alpha_GT, mask)\n        acc = 100.0 * correct / n_samples\n        train_loss_avg = train_loss / (batch_idx + 1)\n        if log and (batch_idx > 0 and batch_idx % args.log_interval == 0 or batch_idx == len(train_loader) - 1):\n            print('Train set (epoch {}): [{}/{} ({:.0f}%)]\\tLoss: {:.4f} (avg: {:.4f}), other losses: {}\\tAcc metric: {}/{} ({:.2f}%)\\t AttnAUC: {}\\t avg sec/iter: {:.4f}'.format(epoch, n_samples, len(train_loader.dataset), 100.0 * n_samples / len(train_loader.dataset), loss_item, train_loss_avg, ['%.4f' % l.item() for l in other_losses], correct, n_samples, acc, ['%.2f' % a for a in attn_AUC(alpha_GT, alpha_pred)], time_iter / (batch_idx + 1)))\n    assert n_samples == len(train_loader.dataset), (n_samples, len(train_loader.dataset))\n    return (train_loss, acc)\n\ndef test(model, test_loader, epoch, loss_fn, split, args, feature_stats=None, noises=None, img_noise_level=None, eval_attn=False, alpha_WS_name=''):\n    model.eval()\n    n_samples, correct, test_loss = (0, 0, 0)\n    pred, targets, N_nodes = ([], [], [])\n    start = time.time()\n    alpha_pred, alpha_GT = ({}, {})\n    if eval_attn:\n        alpha_pred[0] = []\n        print('testing with evaluation of attention: takes longer time')\n    if args.debug:\n        debug_data = {}\n    with torch.no_grad():\n        for batch_idx, data in enumerate(test_loader):\n            data = data_to_device(data, args.device)\n            if feature_stats is not None:\n                assert feature_stats[0].shape[2] == feature_stats[1].shape[2] == data[0].shape[2], (feature_stats[0].shape, feature_stats[1].shape, data[0].shape)\n                data[0] = (data[0] - feature_stats[0]) / feature_stats[1]\n            if batch_idx == 0 and epoch <= 1:\n                sanity_check(model, data)\n            if noises is not None:\n                noise = noises[n_samples:n_samples + len(data[0])].to(args.device) * img_noise_level\n                if len(noise.shape) == 2:\n                    noise = noise.unsqueeze(2)\n                data[0][:, :, :3] = data[0][:, :, :3] + noise\n            mask = [data[2].view(len(data[2]), -1)]\n            N_nodes.append(data[4]['N_nodes'].detach())\n            targets.append(data[3].detach())\n            output, other_outputs = model(data)\n            other_losses = other_outputs['reg'] if 'reg' in other_outputs else []\n            alpha = other_outputs['alpha'] if 'alpha' in other_outputs else []\n            mask.extend(other_outputs['mask'] if 'mask' in other_outputs else [])\n            if args.debug:\n                for key in other_outputs:\n                    if key.find('debug') >= 0:\n                        if key not in debug_data:\n                            debug_data[key] = []\n                        debug_data[key].append([d.data.cpu().numpy() for d in other_outputs[key]])\n            if args.torch.find('1.') == 0:\n                loss = loss_fn(output, data[3], reduction='sum')\n            else:\n                loss = loss_fn(output, data[3], reduce=False).sum()\n            for l in other_losses:\n                loss += l\n            test_loss += loss.item()\n            pred.append(output.detach())\n            update_attn(data, alpha, alpha_pred, alpha_GT, mask)\n            if eval_attn:\n                assert len(alpha) == 0, 'invalid mode, eval_attn should be false for this type of pooling'\n                alpha_pred[0].extend(attn_heatmaps(model, args.device, data, output.data, test_loader.batch_size, constant_mask=args.dataset == 'mnist'))\n            n_samples += len(data[0])\n            if eval_attn and (n_samples % 100 == 0 or n_samples == len(test_loader.dataset)):\n                print('{}/{} samples processed'.format(n_samples, len(test_loader.dataset)))\n    assert n_samples == len(test_loader.dataset), (n_samples, len(test_loader.dataset))\n    pred = torch.cat(pred)\n    targets = torch.cat(targets)\n    N_nodes = torch.cat(N_nodes)\n    if args.dataset.find('colors') >= 0:\n        correct = count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=0, N_nodes_max=25)\n        if pred.shape[0] > 2500:\n            correct += count_correct(pred[2500:5000], targets[2500:5000], N_nodes=N_nodes[2500:5000], N_nodes_min=26, N_nodes_max=200)\n            correct += count_correct(pred[5000:], targets[5000:], N_nodes=N_nodes[5000:], N_nodes_min=26, N_nodes_max=200)\n    elif args.dataset == 'triangles':\n        correct = count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=0, N_nodes_max=25)\n        if pred.shape[0] > 5000:\n            correct += count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=26, N_nodes_max=100)\n    else:\n        correct = count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=0, N_nodes_max=100000.0)\n    time_iter = time.time() - start\n    test_loss_avg = test_loss / n_samples\n    acc = 100.0 * correct / n_samples\n    print('{} set (epoch {}): Avg loss: {:.4f}, Acc metric: {}/{} ({:.2f}%)\\t AttnAUC: {}\\t avg sec/iter: {:.4f}\\n'.format(split.capitalize(), epoch, test_loss_avg, correct, n_samples, acc, ['%.2f' % a for a in attn_AUC(alpha_GT, alpha_pred)], time_iter / (batch_idx + 1)))\n    if args.debug:\n        for key in debug_data:\n            for layer in range(len(debug_data[key][0])):\n                print('{} (layer={}): {:.5f}'.format(key, layer, np.mean([d[layer] for d in debug_data[key]])))\n    if eval_attn:\n        alpha_pred = alpha_pred[0]\n        if args.results in [None, 'None', ''] or alpha_WS_name == '':\n            print('skip saving alpha values, invalid results dir (%s) or alpha_WS_name (%s)' % (args.results, alpha_WS_name))\n        else:\n            file_path = pjoin(args.results, '%s_alpha_WS_%s_seed%d_%s.pkl' % (args.dataset, split, args.seed, alpha_WS_name))\n            if os.path.isfile(file_path):\n                print('WARNING: file %s exists and will be overwritten' % file_path)\n            with open(file_path, 'wb') as f:\n                pickle.dump(alpha_pred, f, protocol=2)\n    return (test_loss, acc, alpha_pred, pred)\n\ndef update_attn(data, alpha, alpha_pred, alpha_GT, mask):\n    key = 'node_attn_eval'\n    for layer in range(len(mask)):\n        mask[layer] = mask[layer].data.cpu().numpy() > 0\n    if key in data[4]:\n        if not isinstance(data[4][key], list):\n            data[4][key] = [data[4][key]]\n        for layer in range(len(data[4][key])):\n            if layer not in alpha_GT:\n                alpha_GT[layer] = []\n            alpha_GT[layer].extend(masked_alpha(data[4][key][layer].data.cpu().numpy(), mask[layer]))\n    for layer in range(len(alpha)):\n        if layer not in alpha_pred:\n            alpha_pred[layer] = []\n        alpha_pred[layer].extend(masked_alpha(alpha[layer].data.cpu().numpy(), mask[layer]))\n\ndef masked_alpha(alpha, mask):\n    alpha_lst = []\n    for i in range(len(alpha)):\n        alpha_lst.append(alpha[i][mask[i]])\n    return alpha_lst\n\ndef attn_heatmaps(model, device, data, output_org, batch_size=1, constant_mask=False):\n    labels = torch.argmax(output_org, dim=1)\n    B, N_nodes_max, C = data[0].shape\n    alpha_WS = []\n    if N_nodes_max > 1000:\n        print('WARNING: graph is too large (%d nodes) and not supported by this function (evaluation will be incorrect for graphs in this batch).' % N_nodes_max)\n        for b in range(B):\n            n = data[2][b].sum().item()\n            alpha_WS.append(np.zeros((1, n)) + 1.0 / n)\n        return alpha_WS\n    if constant_mask:\n        mask = torch.ones(N_nodes_max, N_nodes_max - 1).to(device)\n    node_ids = torch.arange(start=0, end=N_nodes_max, device=device).view(1, -1).repeat(N_nodes_max, 1)\n    node_ids[np.diag_indices(N_nodes_max, 2)] = -1\n    node_ids = node_ids[node_ids >= 0].view(N_nodes_max, N_nodes_max - 1).long()\n    with torch.no_grad():\n        for b in range(B):\n            x = torch.gather(data[0][b].unsqueeze(0).expand(N_nodes_max, -1, -1), dim=1, index=node_ids.unsqueeze(2).expand(-1, -1, C))\n            if not constant_mask:\n                mask = torch.gather(data[2][b].unsqueeze(0).expand(N_nodes_max, -1), dim=1, index=node_ids)\n            A = torch.gather(data[1][b].unsqueeze(0).expand(N_nodes_max, -1, -1), dim=1, index=node_ids.unsqueeze(2).expand(-1, -1, N_nodes_max))\n            A = torch.gather(A, dim=2, index=node_ids.unsqueeze(1).expand(-1, N_nodes_max - 1, -1))\n            output = torch.zeros(N_nodes_max).to(device)\n            n_chunks = int(np.ceil(N_nodes_max / float(batch_size)))\n            for i in range(n_chunks):\n                idx = np.arange(i * batch_size, (i + 1) * batch_size) if i < n_chunks - 1 else np.arange(i * batch_size, N_nodes_max)\n                output[idx] = model([x[idx], A[idx], mask[idx], None, {}])[0][:, labels[b]].data\n            alpha = torch.abs(output - output_org[b, labels[b]]).view(1, N_nodes_max)\n            if not constant_mask:\n                alpha = alpha[data[2][b].view(1, N_nodes_max)]\n            alpha_WS.append(normalize(alpha).data.cpu().numpy())\n    return alpha_WS\n\ndef save_checkpoint(model, scheduler, optimizer, args, epoch):\n    if args.results in [None, 'None']:\n        print('skip saving checkpoint, invalid results dir: %s' % args.results)\n        return\n    file_path = '%s/checkpoint_%s_%s_epoch%d_seed%07d.pth.tar' % (args.results, args.dataset, args.experiment_ID, epoch, args.seed)\n    try:\n        print('saving the model to %s' % file_path)\n        state = {'epoch': epoch, 'args': args, 'state_dict': model.state_dict(), 'scheduler': scheduler.state_dict(), 'optimizer': optimizer.state_dict()}\n        if os.path.isfile(file_path):\n            print('WARNING: file %s exists and will be overwritten' % file_path)\n        torch.save(state, file_path)\n    except Exception as e:\n        print('error saving the model', e)\n\ndef load_checkpoint(model, optimizer, scheduler, file_path):\n    print('loading the model from %s' % file_path)\n    state = torch.load(file_path)\n    model.load_state_dict(state['state_dict'])\n    optimizer.load_state_dict(state['optimizer'])\n    scheduler.load_state_dict(state['scheduler'])\n    print('loading from epoch %d done' % state['epoch'])\n    return state['epoch'] + 1\n\ndef create_model_optimizer(in_features, out_features, pool, kl_weight, args, scale=None, init=None, n_hidden_attn=None):\n    set_seed(args.seed, seed_data=None)\n    model = ChebyGIN(in_features=in_features, out_features=out_features, filters=args.filters, K=args.filter_scale, n_hidden=args.n_hidden, aggregation=args.aggregation, dropout=args.dropout, readout=args.readout, pool=pool, pool_arch=args.pool_arch if n_hidden_attn in [None, 0] else args.pool_arch[:2] + ['%d' % n_hidden_attn], large_graph=args.dataset.lower() == 'mnist', kl_weight=float(kl_weight), init=args.init if init is None else init, scale=args.scale if scale is None else scale, debug=args.debug)\n    print(model)\n    print('model capacity: %d' % np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model.parameters()]))\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay, betas=(0.5, 0.999))\n    scheduler = lr_scheduler.MultiStepLR(optimizer, args.lr_decay_step, gamma=0.1)\n    epoch = 1\n    if args.resume not in [None, 'None']:\n        epoch = load_checkpoint(model, optimizer, scheduler, args.resume)\n        if epoch < args.epochs + 1:\n            print('resuming training for epoch %d' % epoch)\n    model.to(args.device)\n    return (epoch, model, optimizer, scheduler)\n\ndef single_job(fold, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale=None, init=None, n_hidden_attn=None):\n    set_seed(args.seed, seed_data=None)\n    wsup = args.pool[1] == 'sup'\n    train_loader = DataLoader(GraphData(datareader, fold, 'train'), batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n    val_loader = DataLoader(GraphData(datareader, fold, 'val'), batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n    start_epoch, model, optimizer, scheduler = create_model_optimizer(train_loader.dataset.num_features, train_loader.dataset.num_classes, None if wsup else pool, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n    for epoch in range(start_epoch, args.epochs + 1):\n        scheduler.step()\n        train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats, log=False)\n    if wsup:\n        train_loader_test = DataLoader(GraphData(datareader, fold, 'train'), batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n        train_loss, train_acc, attn_WS = test(model, train_loader_test, epoch, loss_fn, 'train', args, feature_stats, eval_attn=True)[:3]\n        train_loader = DataLoader(GraphData(datareader, fold, 'train', attn_labels=attn_WS), batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n        val_loader = DataLoader(GraphData(datareader, fold, 'val'), batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n        start_epoch, model, optimizer, scheduler = create_model_optimizer(train_loader.dataset.num_features, train_loader.dataset.num_classes, pool, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n        for epoch in range(start_epoch, args.epochs + 1):\n            scheduler.step()\n            train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats, log=False)\n    acc = test(model, val_loader, epoch, loss_fn, 'val', args, feature_stats)[1]\n    val_acc[fold] = acc\n\ndef cross_validation(datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, n_hidden_attn=None, folds=10, threads=5):\n    print('%d-fold cross-validation' % folds)\n    manager = multiprocessing.Manager()\n    val_acc = manager.dict()\n    assert threads <= folds, (threads, folds)\n    n_it = int(np.ceil(float(folds) / threads))\n    for i in range(n_it):\n        processes = []\n        if threads <= 1:\n            single_job(i * threads, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale=args.scale, init=args.init, n_hidden_attn=n_hidden_attn)\n        else:\n            for fold in range(threads):\n                p = mp.Process(target=single_job, args=(i * threads + fold, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, args.scale, args.init, n_hidden_attn))\n                p.start()\n                processes.append(p)\n            for p in processes:\n                p.join()\n    print(val_acc)\n    val_acc = list(val_acc.values())\n    print('average and std over {} folds: {} +- {}'.format(folds, np.mean(val_acc), np.std(val_acc)))\n    metric = np.mean(val_acc) - np.std(val_acc)\n    print('metric: avg acc - std: {}'.format(metric))\n    return metric\n\n# File: chebygin.py\nimport numpy as np\nimport torch\nimport torch.sparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom attention_pooling import *\nfrom utils import *\n\nclass ChebyGINLayer(nn.Module):\n    \"\"\"\n    General Graph Neural Network layer that depending on arguments can be:\n    1. Graph Convolution Layer (T. Kipf and M. Welling, ICLR 2017)\n    2. Chebyshev Graph Convolution Layer (M. Defferrard et al., NeurIPS 2017)\n    3. GIN Layer (K. Xu et al., ICLR 2019)\n    4. ChebyGIN Layer (B. Knyazev et al., ICLR 2019 Workshop on Representation Learning on Graphs and Manifolds)\n    The first three types (1-3) of layers are particular cases of the fourth (4) case.\n    \"\"\"\n\n    def __init__(self, in_features, out_features, K, n_hidden=0, aggregation='mean', activation=nn.ReLU(True), n_relations=1):\n        super(ChebyGINLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_relations = n_relations\n        assert K > 0, 'order is assumed to be > 0'\n        self.K = K\n        assert n_hidden >= 0, ('invalid n_hidden value', n_hidden)\n        self.n_hidden = n_hidden\n        assert aggregation in ['mean', 'sum'], ('invalid aggregation', aggregation)\n        self.aggregation = aggregation\n        self.activation = activation\n        n_in = self.in_features * self.K * n_relations\n        if self.n_hidden == 0:\n            fc = [nn.Linear(n_in, self.out_features)]\n        else:\n            fc = [nn.Linear(n_in, n_hidden), nn.ReLU(True), nn.Linear(n_hidden, self.out_features)]\n        if activation is not None:\n            fc.append(activation)\n        self.fc = nn.Sequential(*fc)\n        print('ChebyGINLayer', list(self.fc.children())[0].weight.shape, torch.norm(list(self.fc.children())[0].weight, dim=1)[:10])\n\n    def __repr__(self):\n        return 'ChebyGINLayer(in_features={}, out_features={}, K={}, n_hidden={}, aggregation={})\\nfc={}'.format(self.in_features, self.out_features, self.K, self.n_hidden, self.aggregation, str(self.fc))\n\n    def chebyshev_basis(self, L, X, K):\n        \"\"\"\n        Return T_k X where T_k are the Chebyshev polynomials of order up to K.\n        :param L: graph Laplacian, batch (B), nodes (N), nodes (N)\n        :param X: input of size batch (B), nodes (N), features (F)\n        :param K: Chebyshev polynomial order, i.e. filter size (number of hopes)\n        :return: Tensor of size (B,N,K,F) as a result of multiplying T_k(L) by X for each order\n        \"\"\"\n        if K > 1:\n            Xt = [X]\n            Xt.append(torch.bmm(L, X))\n            for k in range(2, K):\n                Xt.append(2 * torch.bmm(L, Xt[k - 1]) - Xt[k - 2])\n            Xt = torch.stack(Xt, 2)\n            return Xt\n        else:\n            assert K == 1, K\n            return torch.bmm(L, X).unsqueeze(2)\n\n    def laplacian_batch(self, A, add_identity=False):\n        \"\"\"\n        Computes normalized Laplacian transformed so that its eigenvalues are in range [-1, 1].\n        Note that sum of all eigenvalues = trace(L) = 0.\n        :param A: Tensor of size (B,N,N) containing batch (B) of adjacency matrices of shape N,N\n        :return: Normalized Laplacian of size (B,N,N)\n        \"\"\"\n        B, N = A.shape[:2]\n        if add_identity:\n            A = A + torch.eye(N, device=A.get_device() if A.is_cuda else 'cpu').unsqueeze(0)\n        D = torch.sum(A, 1)\n        D_hat = (D + 1e-05) ** (-0.5)\n        L = D_hat.view(B, N, 1) * A * D_hat.view(B, 1, N)\n        if not add_identity:\n            L = -L\n        return (D, L)\n\n    def forward(self, data):\n        x, A, mask = data[:3]\n        B, N, F = x.shape\n        assert N == A.shape[1] == A.shape[2], ('invalid shape', N, x.shape, A.shape)\n        if len(A.shape) == 3:\n            A = A.unsqueeze(3)\n        y_out = []\n        for rel in range(A.shape[3]):\n            D, L = self.laplacian_batch(A[:, :, :, rel], add_identity=self.K == 1)\n            y = self.chebyshev_basis(L, x, self.K)\n            if self.aggregation == 'sum':\n                if self.K == 1:\n                    y = y * D.view(B, N, 1, 1)\n                else:\n                    D_GIN = torch.ones(B, N, self.K, device=x.get_device() if x.is_cuda else 'cpu')\n                    D_GIN[:, :, 1:] = D.view(B, N, 1).expand(-1, -1, self.K - 1)\n                    y = y * D_GIN.view(B, N, self.K, 1)\n            y_out.append(y)\n        y = torch.cat(y_out, dim=2)\n        y = self.fc(y.view(B, N, -1))\n        if len(mask.shape) == 2:\n            mask = mask.unsqueeze(2)\n        y = y * mask.float()\n        output = [y, A, mask]\n        output.extend(data[3:] + [x])\n        return output\n\nclass GraphReadout(nn.Module):\n    \"\"\"\n    Global pooling layer applied after the last graph layer.\n    \"\"\"\n\n    def __init__(self, pool_type):\n        super(GraphReadout, self).__init__()\n        self.pool_type = pool_type\n        dim = 1\n        if pool_type == 'max':\n            self.readout_layer = lambda x, mask: torch.max(x, dim=dim)[0]\n        elif pool_type in ['avg', 'mean']:\n            self.readout_layer = lambda x, mask: torch.sum(x, dim=dim) / torch.sum(mask, dim=dim).float()\n        elif pool_type in ['sum']:\n            self.readout_layer = lambda x, mask: torch.sum(x, dim=dim)\n        else:\n            raise NotImplementedError(pool_type)\n\n    def __repr__(self):\n        return 'GraphReadout({})'.format(self.pool_type)\n\n    def forward(self, data):\n        x, A, mask = data[:3]\n        B, N = x.shape[:2]\n        x = self.readout_layer(x, mask.view(B, N, 1))\n        output = [x]\n        output.extend(data[1:])\n        return output\n\nclass ChebyGIN(nn.Module):\n    \"\"\"\n    Graph Neural Network class.\n    \"\"\"\n\n    def __init__(self, in_features, out_features, filters, K=1, n_hidden=0, aggregation='mean', dropout=0, readout='max', pool=None, pool_arch='fc_prev'.split('_'), large_graph=False, kl_weight=None, graph_layer_fn=None, init='normal', scale=None, debug=False):\n        super(ChebyGIN, self).__init__()\n        self.out_features = out_features\n        assert len(filters) > 0, 'filters must be an iterable object with at least one element'\n        assert K > 0, 'filter scale must be a positive integer'\n        self.pool = pool\n        self.pool_arch = pool_arch\n        self.debug = debug\n        n_prev = None\n        attn_gnn = None\n        if graph_layer_fn is None:\n            graph_layer_fn = lambda n_in, n_out, K_, n_hidden_, activation: ChebyGINLayer(in_features=n_in, out_features=n_out, K=K_, n_hidden=n_hidden_, aggregation=aggregation, activation=activation)\n            if self.pool_arch is not None and self.pool_arch[0] == 'gnn':\n                attn_gnn = lambda n_in: ChebyGIN(in_features=n_in, out_features=0, filters=[32, 32, 1], K=np.min((K, 2)), n_hidden=0, graph_layer_fn=graph_layer_fn)\n        graph_layers = []\n        for layer, f in enumerate(filters + [None]):\n            n_in = in_features if layer == 0 else filters[layer - 1]\n            if self.pool is not None and len(self.pool) > len(filters) + layer and (self.pool[layer + 3] != 'skip'):\n                graph_layers.append(AttentionPooling(in_features=n_in, in_features_prev=n_prev, pool_type=self.pool[:3] + [self.pool[layer + 3]], pool_arch=self.pool_arch, large_graph=large_graph, kl_weight=kl_weight, attn_gnn=attn_gnn, init=init, scale=scale, debug=debug))\n            if f is not None:\n                graph_layers.append(graph_layer_fn(n_in, f, K, n_hidden, None if self.out_features == 0 and layer == len(filters) - 1 else nn.ReLU(True)))\n                n_prev = n_in\n        if self.out_features > 0:\n            graph_layers.append(GraphReadout(readout))\n        self.graph_layers = nn.Sequential(*graph_layers)\n        if self.out_features > 0:\n            self.fc = nn.Sequential(*([nn.Dropout(p=dropout)] if dropout > 0 else []) + [nn.Linear(filters[-1], out_features)])\n\n    def forward(self, data):\n        data = self.graph_layers(data)\n        if self.out_features > 0:\n            y = self.fc(data[0])\n        else:\n            y = data[0]\n        return (y, data[4])\n\n# File: graphdata.py\nimport numpy as np\nimport os\nfrom os.path import join as pjoin\nimport pickle\nimport copy\nimport torch\nimport torch.utils\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nfrom scipy.spatial.distance import cdist\nfrom utils import *\n\ndef compute_adjacency_matrix_images(coord, sigma=0.1):\n    coord = coord.reshape(-1, 2)\n    dist = cdist(coord, coord)\n    A = np.exp(-dist / (sigma * np.pi) ** 2)\n    A[np.diag_indices_from(A)] = 0\n    return A\n\ndef precompute_graph_images(img_size):\n    col, row = np.meshgrid(np.arange(img_size), np.arange(img_size))\n    coord = np.stack((col, row), axis=2) / img_size\n    A = torch.from_numpy(compute_adjacency_matrix_images(coord)).float().unsqueeze(0)\n    coord = torch.from_numpy(coord).float().unsqueeze(0).view(1, -1, 2)\n    mask = torch.ones(1, img_size * img_size, dtype=torch.uint8)\n    return (A, coord, mask)\n\ndef collate_batch_images(batch, A, mask, use_mean_px=True, coord=None, gt_attn_threshold=0, replicate_features=True):\n    B = len(batch)\n    C, H, W = batch[0][0].shape\n    N_nodes = H * W\n    params_dict = {'N_nodes': torch.zeros(B, dtype=torch.long) + N_nodes, 'node_attn_eval': None}\n    has_WS_attn = len(batch[0]) > 2\n    if has_WS_attn:\n        WS_attn = torch.from_numpy(np.stack([batch[b][2].reshape(N_nodes) for b in range(B)]).astype(np.float32)).view(B, N_nodes)\n        WS_attn = normalize_batch(WS_attn)\n        params_dict.update({'node_attn': WS_attn})\n    if use_mean_px:\n        x = torch.stack([batch[b][0].view(C, N_nodes).t() for b in range(B)]).float()\n        if gt_attn_threshold == 0:\n            GT_attn = (x > 0).view(B, N_nodes).float()\n        else:\n            GT_attn = x.view(B, N_nodes).float().clone()\n            GT_attn[GT_attn < gt_attn_threshold] = 0\n        GT_attn = normalize_batch(GT_attn)\n        params_dict.update({'node_attn_eval': GT_attn})\n        if not has_WS_attn:\n            params_dict.update({'node_attn': GT_attn})\n    else:\n        raise NotImplementedError('this case is not well supported')\n    if coord is not None:\n        if use_mean_px:\n            x = torch.cat((x, coord.expand(B, -1, -1)), dim=2)\n        else:\n            x = coord.expand(B, -1, -1)\n    if x is None:\n        x = torch.ones(B, N_nodes, 1)\n    if replicate_features:\n        x = F.pad(x, (2, 0), 'replicate')\n    try:\n        labels = torch.Tensor([batch[b][1] for b in range(B)]).long()\n    except:\n        labels = torch.stack([batch[b][1] for b in range(B)]).long()\n    return [x, A.expand(B, -1, -1), mask.expand(B, -1), labels, params_dict]\n\ndef collate_batch(batch):\n    \"\"\"\n    Creates a batch of same size graphs by zero-padding node features and adjacency matrices up to\n    the maximum number of nodes in the current batch rather than in the entire dataset.\n    \"\"\"\n    B = len(batch)\n    N_nodes = [batch[b][2] for b in range(B)]\n    C = batch[0][0].shape[1]\n    N_nodes_max = int(np.max(N_nodes))\n    mask = torch.zeros(B, N_nodes_max, dtype=torch.bool)\n    A = torch.zeros(B, N_nodes_max, N_nodes_max)\n    x = torch.zeros(B, N_nodes_max, C)\n    has_GT_attn = len(batch[0]) > 4 and batch[0][4] is not None\n    if has_GT_attn:\n        GT_attn = torch.zeros(B, N_nodes_max)\n    has_WS_attn = len(batch[0]) > 5 and batch[0][5] is not None\n    if has_WS_attn:\n        WS_attn = torch.zeros(B, N_nodes_max)\n    for b in range(B):\n        x[b, :N_nodes[b]] = batch[b][0]\n        A[b, :N_nodes[b], :N_nodes[b]] = batch[b][1]\n        mask[b][:N_nodes[b]] = 1\n        if has_GT_attn:\n            GT_attn[b, :N_nodes[b]] = batch[b][4].squeeze()\n        if has_WS_attn:\n            WS_attn[b, :N_nodes[b]] = batch[b][5].squeeze()\n    N_nodes = torch.from_numpy(np.array(N_nodes)).long()\n    params_dict = {'N_nodes': N_nodes}\n    if has_WS_attn:\n        params_dict.update({'node_attn': WS_attn})\n    if has_GT_attn:\n        params_dict.update({'node_attn_eval': GT_attn})\n        if not has_WS_attn:\n            params_dict.update({'node_attn': GT_attn})\n    elif has_WS_attn:\n        params_dict.update({'node_attn_eval': WS_attn})\n    labels = torch.from_numpy(np.array([batch[b][3] for b in range(B)])).long()\n    return [x, A, mask, labels, params_dict]\n\nclass MNIST(torchvision.datasets.MNIST):\n    \"\"\"\n    Wrapper around MNIST to use predefined attention coefficients\n    \"\"\"\n\n    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, attn_coef=None):\n        super(MNIST, self).__init__(root, train, transform, target_transform, download)\n        self.alpha_WS = None\n        if attn_coef is not None and train:\n            print('loading weakly-supervised labels from %s' % attn_coef)\n            with open(attn_coef, 'rb') as f:\n                self.alpha_WS = pickle.load(f)\n            print(train, len(self.alpha_WS))\n\n    def __getitem__(self, index):\n        img, target = super(MNIST, self).__getitem__(index)\n        if self.alpha_WS is None:\n            return (img, target)\n        else:\n            return (img, target, self.alpha_WS[index])\n\nclass MNIST75sp(torch.utils.data.Dataset):\n\n    def __init__(self, data_dir, split, use_mean_px=True, use_coord=True, gt_attn_threshold=0, attn_coef=None):\n        self.data_dir = data_dir\n        self.split = split\n        self.is_test = split.lower() in ['test', 'val']\n        with open(pjoin(data_dir, 'mnist_75sp_%s.pkl' % split), 'rb') as f:\n            self.labels, self.sp_data = pickle.load(f)\n        self.use_mean_px = use_mean_px\n        self.use_coord = use_coord\n        self.n_samples = len(self.labels)\n        self.img_size = 28\n        self.gt_attn_threshold = gt_attn_threshold\n        self.alpha_WS = None\n        if attn_coef is not None and (not self.is_test):\n            with open(attn_coef, 'rb') as f:\n                self.alpha_WS = pickle.load(f)\n            print('using weakly-supervised labels from %s (%d samples)' % (attn_coef, len(self.alpha_WS)))\n\n    def train_val_split(self, samples_idx):\n        self.sp_data = [self.sp_data[i] for i in samples_idx]\n        self.labels = self.labels[samples_idx]\n        self.n_samples = len(self.labels)\n\n    def precompute_graph_data(self, replicate_features, threads=0):\n        print('precompute all data for the %s set...' % self.split.upper())\n        self.Adj_matrices, self.node_features, self.GT_attn, self.WS_attn = ([], [], [], [])\n        for index, sample in enumerate(self.sp_data):\n            mean_px, coord = sample[:2]\n            coord = coord / self.img_size\n            A = compute_adjacency_matrix_images(coord)\n            N_nodes = A.shape[0]\n            x = None\n            if self.use_mean_px:\n                x = mean_px.reshape(N_nodes, -1)\n            if self.use_coord:\n                coord = coord.reshape(N_nodes, 2)\n                if self.use_mean_px:\n                    x = np.concatenate((x, coord), axis=1)\n                else:\n                    x = coord\n            if x is None:\n                x = np.ones(N_nodes, 1)\n            if replicate_features:\n                x = np.pad(x, ((0, 0), (2, 0)), 'edge')\n            if self.gt_attn_threshold == 0:\n                gt_attn = (mean_px > 0).astype(np.float32)\n            else:\n                gt_attn = mean_px.copy()\n                gt_attn[gt_attn < self.gt_attn_threshold] = 0\n            self.GT_attn.append(normalize(gt_attn))\n            if self.alpha_WS is not None:\n                self.WS_attn.append(normalize(self.alpha_WS[index]))\n            self.node_features.append(x)\n            self.Adj_matrices.append(A)\n\n    def __len__(self):\n        return self.n_samples\n\n    def __getitem__(self, index):\n        data = [self.node_features[index], self.Adj_matrices[index], self.Adj_matrices[index].shape[0], self.labels[index], self.GT_attn[index]]\n        if self.alpha_WS is not None:\n            data.append(self.WS_attn[index])\n        data = list_to_torch(data)\n        return data\n\nclass SyntheticGraphs(torch.utils.data.Dataset):\n\n    def __init__(self, data_dir, dataset, split, degree_feature=True, attn_coef=None, threads=12):\n        self.is_test = split.lower() in ['test', 'val']\n        self.split = split\n        self.degree_feature = degree_feature\n        if dataset.find('colors') >= 0:\n            dim = int(dataset.split('-')[1])\n            data_file = 'random_graphs_colors_dim%d_%s.pkl' % (dim, split)\n            is_triangles = False\n            self.feature_dim = dim + 1\n        if dataset.find('triangles') >= 0:\n            data_file = 'random_graphs_triangles_%s.pkl' % split\n            is_triangles = True\n        else:\n            NotImplementedError(dataset)\n        with open(pjoin(data_dir, data_file), 'rb') as f:\n            data = pickle.load(f)\n        for key in data:\n            if not isinstance(data[key], list) and (not isinstance(data[key], np.ndarray)):\n                print(split, key, data[key])\n            else:\n                print(split, key, len(data[key]))\n        self.Node_degrees = [np.sum(A, 1).astype(np.int32) for A in data['Adj_matrices']]\n        if is_triangles:\n            self.feature_dim = data['Max_degree'] + 1\n            self.node_features = []\n            for i in range(len(data['Adj_matrices'])):\n                N = data['Adj_matrices'][i].shape[0]\n                if degree_feature:\n                    D_onehot = np.zeros((N, self.feature_dim))\n                    D_onehot[np.arange(N), self.Node_degrees[i]] = 1\n                else:\n                    D_onehot = np.zeros((N, 1))\n                self.node_features.append(D_onehot)\n            if not degree_feature:\n                self.feature_dim = 1\n        else:\n            self.node_features = []\n            for i in range(len(data['node_features'])):\n                features = data['node_features'][i]\n                if features.shape[1] < self.feature_dim:\n                    features = np.pad(features, ((0, 0), (0, 1)), 'constant')\n                self.node_features.append(features)\n        self.alpha_WS = None\n        if attn_coef is not None and (not self.is_test):\n            with open(attn_coef, 'rb') as f:\n                self.alpha_WS = pickle.load(f)\n            print('using weakly-supervised labels from %s (%d samples)' % (attn_coef, len(self.alpha_WS)))\n            self.WS_attn = []\n            for index in range(len(self.alpha_WS)):\n                self.WS_attn.append(normalize(self.alpha_WS[index]))\n        N_nodes = np.array([A.shape[0] for A in data['Adj_matrices']])\n        self.Adj_matrices = data['Adj_matrices']\n        self.GT_attn = data['GT_attn']\n        for i in range(len(self.GT_attn)):\n            self.GT_attn[i] = normalize(self.GT_attn[i])\n        self.labels = data['graph_labels'].astype(np.int32)\n        self.classes = np.unique(self.labels)\n        self.n_classes = len(self.classes)\n        R = np.corrcoef(self.labels, N_nodes)[0, 1]\n        degrees = []\n        for i in range(len(self.Node_degrees)):\n            degrees.extend(list(self.Node_degrees[i]))\n        degrees = np.array(degrees, np.int32)\n        print('N nodes avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(*stats(N_nodes)))\n        print('N edges avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(*stats(data['N_edges'])))\n        print('Node degree avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(*stats(degrees)))\n        print('Node features dim: \\t\\t%d' % self.feature_dim)\n        print('N classes: \\t\\t\\t%d' % self.n_classes)\n        print('Correlation of labels with graph size: \\t%.2f' % R)\n        print('Classes: \\t\\t\\t%s' % str(self.classes))\n        for lbl in self.classes:\n            idx = self.labels == lbl\n            print('Class {}: \\t\\t\\t{} samples, N_nodes: avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(lbl, np.sum(idx), *stats(N_nodes[idx])))\n\n    def __len__(self):\n        return len(self.Adj_matrices)\n\n    def __getitem__(self, index):\n        data = [self.node_features[index], self.Adj_matrices[index], self.Adj_matrices[index].shape[0], self.labels[index], self.GT_attn[index]]\n        if self.alpha_WS is not None:\n            data.append(self.WS_attn[index])\n        data = list_to_torch(data)\n        return data\n\nclass GraphData(torch.utils.data.Dataset):\n\n    def __init__(self, datareader, fold_id, split, degree_feature=True, attn_labels=None):\n        self.fold_id = fold_id\n        self.split = split\n        self.w_sup_signal_attn = None\n        print('The degree_feature argument is ignored for this dataset. \\n        It will automatically be set to True if nodes do not have any features. Otherwise it will be set to False')\n        if attn_labels is not None:\n            if isinstance(attn_labels, str) and os.path.isfile(attn_labels):\n                with open(attn_labels, 'rb') as f:\n                    self.w_sup_signal_attn = pickle.load(f)\n            else:\n                self.w_sup_signal_attn = attn_labels\n            for i in range(len(self.w_sup_signal_attn)):\n                alpha = self.w_sup_signal_attn[i]\n                alpha[alpha < 0.001] = 0\n                self.w_sup_signal_attn[i] = normalize(alpha)\n            print(('!!!using weakly supervised labels (%d samples)!!!' % len(self.w_sup_signal_attn)).upper())\n        self.set_fold(datareader.data, fold_id)\n\n    def set_fold(self, data, fold_id):\n        self.total = len(data['targets'])\n        self.N_nodes_max = data['N_nodes_max']\n        self.num_classes = data['num_classes']\n        self.num_features = data['num_features']\n        if self.split in ['train', 'val']:\n            self.idx = data['splits'][self.split][fold_id]\n        else:\n            assert self.split in ['train_val', 'test'], ('unexpected split', self.split)\n            self.idx = data['splits'][self.split]\n        self.labels = np.array(copy.deepcopy([data['targets'][i] for i in self.idx]))\n        self.adj_list = copy.deepcopy([data['adj_list'][i] for i in self.idx])\n        self.features_onehot = copy.deepcopy([data['features_onehot'][i] for i in self.idx])\n        self.N_edges = np.array([A.sum() // 2 for A in self.adj_list])\n        print('%s: %d/%d' % (self.split.upper(), len(self.labels), len(data['targets'])))\n        classes = np.unique(self.labels)\n        for lbl in classes:\n            print('Class %d: \\t\\t\\t%d samples' % (lbl, np.sum(self.labels == lbl)))\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        if isinstance(index, str):\n            if index == 'Adj_matrices':\n                return self.adj_list\n            elif index == 'GT_attn':\n                print('Ground truth attention is unavailable for this dataset: weakly-supervised labels will be returned')\n                return self.w_sup_signal_attn\n            elif index == 'graph_labels':\n                return self.labels\n            elif index == 'node_features':\n                return self.features_onehot\n            elif index == 'N_edges':\n                return self.N_edges\n            else:\n                raise KeyError(index)\n        else:\n            data = [self.features_onehot[index], self.adj_list[index], self.adj_list[index].shape[0], self.labels[index], None]\n            if self.w_sup_signal_attn is not None:\n                data.append(self.w_sup_signal_attn[index])\n            data = list_to_torch(data)\n            return data\n\nclass DataReader:\n    \"\"\"\n    Class to read the txt files containing all data of the dataset\n    Should work for any dataset from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n    \"\"\"\n\n    def __init__(self, data_dir, N_nodes=None, rnd_state=None, use_cont_node_attr=False, folds=10, fold_id=None):\n        self.data_dir = data_dir\n        self.rnd_state = np.random.RandomState() if rnd_state is None else rnd_state\n        self.use_cont_node_attr = use_cont_node_attr\n        self.N_nodes = N_nodes\n        if os.path.isfile('%s/data.pkl' % data_dir):\n            print('loading data from %s/data.pkl' % data_dir)\n            with open('%s/data.pkl' % data_dir, 'rb') as f:\n                data = pickle.load(f)\n        else:\n            files = os.listdir(self.data_dir)\n            data = {}\n            nodes, graphs = self.read_graph_nodes_relations(list(filter(lambda f: f.find('graph_indicator') >= 0, files))[0])\n            lst = list(filter(lambda f: f.find('node_labels') >= 0, files))\n            if len(lst) > 0:\n                data['features'] = self.read_node_features(lst[0], nodes, graphs, fn=lambda s: int(s.strip()))\n            else:\n                data['features'] = None\n            data['adj_list'] = self.read_graph_adj(list(filter(lambda f: f.find('_A') >= 0, files))[0], nodes, graphs)\n            data['targets'] = np.array(self.parse_txt_file(list(filter(lambda f: f.find('graph_labels') >= 0, files))[0], line_parse_fn=lambda s: int(float(s.strip()))))\n            if self.use_cont_node_attr:\n                data['attr'] = self.read_node_features(list(filter(lambda f: f.find('node_attributes') >= 0, files))[0], nodes, graphs, fn=lambda s: np.array(list(map(float, s.strip().split(',')))))\n            features, n_edges, degrees = ([], [], [])\n            for sample_id, adj in enumerate(data['adj_list']):\n                N = len(adj)\n                if data['features'] is not None:\n                    assert N == len(data['features'][sample_id]), (N, len(data['features'][sample_id]))\n                n = np.sum(adj)\n                n_edges.append(int(n / 2))\n                if not np.allclose(adj, adj.T):\n                    print(sample_id, 'not symmetric')\n                degrees.extend(list(np.sum(adj, 1)))\n                if data['features'] is not None:\n                    features.append(np.array(data['features'][sample_id]))\n            if data['features'] is not None:\n                features_all = np.concatenate(features)\n                features_min = features_all.min()\n                num_features = int(features_all.max() - features_min + 1)\n                features_onehot = []\n                for i, x in enumerate(features):\n                    feature_onehot = np.zeros((len(x), num_features))\n                    for node, value in enumerate(x):\n                        feature_onehot[node, value - features_min] = 1\n                    if self.use_cont_node_attr:\n                        feature_onehot = np.concatenate((feature_onehot, np.array(data['attr'][i])), axis=1)\n                    features_onehot.append(feature_onehot)\n                if self.use_cont_node_attr:\n                    num_features = features_onehot[0].shape[1]\n            else:\n                degree_max = int(np.max([np.sum(A, 1).max() for A in data['adj_list']]))\n                num_features = degree_max + 1\n                features_onehot = []\n                for A in data['adj_list']:\n                    n = A.shape[0]\n                    D = np.sum(A, 1).astype(np.int)\n                    D_onehot = np.zeros((n, num_features))\n                    D_onehot[np.arange(n), D] = 1\n                    features_onehot.append(D_onehot)\n            shapes = [len(adj) for adj in data['adj_list']]\n            labels = data['targets']\n            labels -= np.min(labels)\n            classes = np.unique(labels)\n            num_classes = len(classes)\n            if not np.all(np.diff(classes) == 1):\n                print('making labels sequential, otherwise pytorch might crash')\n                labels_new = np.zeros(labels.shape, dtype=labels.dtype) - 1\n                for lbl in range(num_classes):\n                    labels_new[labels == classes[lbl]] = lbl\n                labels = labels_new\n                classes = np.unique(labels)\n                assert len(np.unique(labels)) == num_classes, np.unique(labels)\n\n            def stats(x):\n                return (np.mean(x), np.std(x), np.min(x), np.max(x))\n            print('N nodes avg/std/min/max: \\t%.2f/%.2f/%d/%d' % stats(shapes))\n            print('N edges avg/std/min/max: \\t%.2f/%.2f/%d/%d' % stats(n_edges))\n            print('Node degree avg/std/min/max: \\t%.2f/%.2f/%d/%d' % stats(degrees))\n            print('Node features dim: \\t\\t%d' % num_features)\n            print('N classes: \\t\\t\\t%d' % num_classes)\n            print('Classes: \\t\\t\\t%s' % str(classes))\n            for lbl in classes:\n                print('Class %d: \\t\\t\\t%d samples' % (lbl, np.sum(labels == lbl)))\n            if data['features'] is not None:\n                for u in np.unique(features_all):\n                    print('feature {}, count {}/{}'.format(u, np.count_nonzero(features_all == u), len(features_all)))\n            N_graphs = len(labels)\n            assert N_graphs == len(data['adj_list']) == len(features_onehot), 'invalid data'\n            data['features_onehot'] = features_onehot\n            data['targets'] = labels\n            data['N_nodes_max'] = np.max(shapes)\n            data['num_features'] = num_features\n            data['num_classes'] = num_classes\n            with open('%s/data.pkl' % data_dir, 'wb') as f:\n                pickle.dump(data, f, protocol=2)\n        labels = data['targets']\n        N_graphs = len(labels)\n        shapes = np.array([len(adj) for adj in data['adj_list']])\n        train_ids, val_ids, train_val_ids, test_ids = self.split_ids_shape(np.arange(N_graphs), shapes, N_nodes, folds=folds)\n        splits = {'train': [], 'val': [], 'train_val': train_val_ids, 'test': test_ids}\n        for fold in range(folds):\n            splits['train'].append(train_ids[fold])\n            splits['val'].append(val_ids[fold])\n        data['splits'] = splits\n        self.data = data\n\n    def split_ids_shape(self, ids_all, shapes, N_nodes, folds=1, fold_id=0):\n        if N_nodes > 0:\n            small_graphs_ind = np.where(shapes <= N_nodes)[0]\n            print('{}/{} graphs with at least {} nodes'.format(len(small_graphs_ind), len(shapes), N_nodes))\n            idx = self.rnd_state.permutation(len(small_graphs_ind))\n            if len(idx) > 1000:\n                n = 1000\n            else:\n                n = 500\n            train_val_ids = small_graphs_ind[idx[:n]]\n            test_ids = small_graphs_ind[idx[n:]]\n            large_graphs_ind = np.where(shapes > N_nodes)[0]\n            test_ids = np.concatenate((test_ids, large_graphs_ind))\n        else:\n            idx = self.rnd_state.permutation(len(ids_all))\n            n = len(ids_all) // folds\n            test_ids = ids_all[idx[fold_id * n:(fold_id + 1) * n if fold_id < folds - 1 else -1]]\n            train_val_ids = []\n            for i in ids_all:\n                if i not in test_ids:\n                    train_val_ids.append(i)\n            train_val_ids = np.array(train_val_ids)\n        assert np.all(np.unique(np.concatenate((train_val_ids, test_ids))) == sorted(ids_all)), 'some graphs are missing in the test sets'\n        if folds > 0:\n            print('generating %d-fold cross-validation splits' % folds)\n            train_ids, val_ids = self.split_ids(train_val_ids, folds=folds)\n            for fold in range(folds):\n                ind = np.concatenate((train_ids[fold], val_ids[fold]))\n                print(fold, len(train_ids[fold]), len(val_ids[fold]))\n                assert len(train_ids[fold]) + len(val_ids[fold]) == len(np.unique(ind)) == len(ind) == len(train_val_ids), 'invalid splits'\n        else:\n            train_ids, val_ids = ([], [])\n        return (train_ids, val_ids, train_val_ids, test_ids)\n\n    def split_ids(self, ids, folds=10):\n        n = len(ids)\n        stride = int(np.ceil(n / float(folds)))\n        test_ids = [ids[i:i + stride] for i in range(0, n, stride)]\n        assert np.all(np.unique(np.concatenate(test_ids)) == sorted(ids)), 'some graphs are missing in the test sets'\n        assert len(test_ids) == folds, 'invalid test sets'\n        train_ids = []\n        for fold in range(folds):\n            train_ids.append(np.array([e for e in ids if e not in test_ids[fold]]))\n            assert len(train_ids[fold]) + len(test_ids[fold]) == len(np.unique(list(train_ids[fold]) + list(test_ids[fold]))) == n, 'invalid splits'\n        return (train_ids, test_ids)\n\n    def parse_txt_file(self, fpath, line_parse_fn=None):\n        with open(pjoin(self.data_dir, fpath), 'r') as f:\n            lines = f.readlines()\n        data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]\n        return data\n\n    def read_graph_adj(self, fpath, nodes, graphs):\n        edges = self.parse_txt_file(fpath, line_parse_fn=lambda s: s.split(','))\n        adj_dict = {}\n        for edge in edges:\n            node1 = int(edge[0].strip()) - 1\n            node2 = int(edge[1].strip()) - 1\n            graph_id = nodes[node1]\n            assert graph_id == nodes[node2], ('invalid data', graph_id, nodes[node2])\n            if graph_id not in adj_dict:\n                n = len(graphs[graph_id])\n                adj_dict[graph_id] = np.zeros((n, n))\n            ind1 = np.where(graphs[graph_id] == node1)[0]\n            ind2 = np.where(graphs[graph_id] == node2)[0]\n            assert len(ind1) == len(ind2) == 1, (ind1, ind2)\n            adj_dict[graph_id][ind1, ind2] = 1\n        adj_list = [adj_dict[graph_id] for graph_id in sorted(list(graphs.keys()))]\n        return adj_list\n\n    def read_graph_nodes_relations(self, fpath):\n        graph_ids = self.parse_txt_file(fpath, line_parse_fn=lambda s: int(s.rstrip()))\n        nodes, graphs = ({}, {})\n        for node_id, graph_id in enumerate(graph_ids):\n            if graph_id not in graphs:\n                graphs[graph_id] = []\n            graphs[graph_id].append(node_id)\n            nodes[node_id] = graph_id\n        graph_ids = np.unique(list(graphs.keys()))\n        for graph_id in graph_ids:\n            graphs[graph_id] = np.array(graphs[graph_id])\n        return (nodes, graphs)\n\n    def read_node_features(self, fpath, nodes, graphs, fn):\n        node_features_all = self.parse_txt_file(fpath, line_parse_fn=fn)\n        node_features = {}\n        for node_id, x in enumerate(node_features_all):\n            graph_id = nodes[node_id]\n            if graph_id not in node_features:\n                node_features[graph_id] = [None] * len(graphs[graph_id])\n            ind = np.where(graphs[graph_id] == node_id)[0]\n            assert len(ind) == 1, ind\n            assert node_features[graph_id][ind[0]] is None, node_features[graph_id][ind[0]]\n            node_features[graph_id][ind[0]] = x\n        node_features_lst = [node_features[graph_id] for graph_id in sorted(list(graphs.keys()))]\n        return node_features_lst\n\n# File: utils.py\nimport numpy as np\nimport os\nimport torch\nimport copy\nfrom graphdata import *\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom sklearn.metrics import roc_auc_score\nimport numbers\nimport random\n\ndef load_save_noise(f, noise_shape):\n    if os.path.isfile(f):\n        print('loading noise from %s' % f)\n        noises = torch.load(f)\n    else:\n        noises = torch.randn(noise_shape, dtype=torch.float)\n        torch.save(noises, f)\n    return noises\n\ndef list_to_torch(data):\n    for i in range(len(data)):\n        if data[i] is None:\n            continue\n        elif isinstance(data[i], np.ndarray):\n            if data[i].dtype == np.bool:\n                data[i] = data[i].astype(np.float32)\n            data[i] = torch.from_numpy(data[i]).float()\n        elif isinstance(data[i], list):\n            data[i] = list_to_torch(data[i])\n    return data\n\ndef data_to_device(data, device):\n    if isinstance(data, dict):\n        keys = list(data.keys())\n    else:\n        keys = range(len(data))\n    for i in keys:\n        if isinstance(data[i], list) or isinstance(data[i], dict):\n            data[i] = data_to_device(data[i], device)\n        elif isinstance(data[i], torch.Tensor):\n            try:\n                data[i] = data[i].to(device)\n            except:\n                print('error', i, data[i], type(data[i]))\n                raise\n    return data\n\ndef count_correct(output, target, N_nodes=None, N_nodes_min=0, N_nodes_max=25):\n    if output.shape[1] == 1:\n        pred = output.round().long()\n    else:\n        pred = output.max(1, keepdim=True)[1]\n    target = target.long().squeeze().cpu()\n    pred = pred.squeeze().cpu()\n    if N_nodes is not None:\n        idx = (N_nodes >= N_nodes_min) & (N_nodes <= N_nodes_max)\n        if idx.sum() > 0:\n            correct = pred[idx].eq(target[idx]).sum().item()\n            for lbl in torch.unique(target, sorted=True):\n                idx_lbl = target[idx] == lbl\n                eq = (pred[idx][idx_lbl] == target[idx][idx_lbl]).float()\n                print('lbl: {}, avg acc: {:2.2f}% ({}/{})'.format(lbl, 100 * eq.mean(), int(eq.sum()), int(idx_lbl.float().sum())))\n            eq = (pred[idx] == target[idx]).float()\n            print('{} <= N_nodes <= {} (min={}, max={}), avg acc: {:2.2f}% ({}/{})'.format(N_nodes_min, N_nodes_max, N_nodes[idx].min(), N_nodes[idx].max(), 100 * eq.mean(), int(eq.sum()), int(idx.sum())))\n        else:\n            correct = 0\n            print('no graphs with nodes >= {} and <= {}'.format(N_nodes_min, N_nodes_max))\n    else:\n        correct = pred.eq(target).sum().item()\n    return correct\n\ndef attn_AUC(alpha_GT, alpha):\n    auc = []\n    if len(alpha) > 0 and alpha_GT is not None and (len(alpha_GT) > 0):\n        for layer in alpha:\n            alpha_gt = np.concatenate([a.flatten() for a in alpha_GT[layer]]) > 0\n            if len(np.unique(alpha_gt)) <= 1:\n                print('Only one class ({}) present in y_true. ROC AUC score is not defined in that case.'.format(np.unique(alpha_gt)))\n                auc.append(np.nan)\n            else:\n                auc.append(100 * roc_auc_score(y_true=alpha_gt, y_score=np.concatenate([a.flatten() for a in alpha[layer]])))\n    return auc\n\ndef stats(arr):\n    return (np.mean(arr), np.std(arr), np.min(arr), np.max(arr))\n\ndef normalize(x, eps=1e-07):\n    return x / (x.sum() + eps)\n\ndef normalize_batch(x, dim=1, eps=1e-07):\n    return x / (x.sum(dim=dim, keepdim=True) + eps)\n\ndef normalize_zero_one(im, eps=1e-07):\n    m1 = im.min()\n    m2 = im.max()\n    return (im - m1) / (m2 - m1 + eps)\n\ndef mse_loss(target, output, reduction='mean', reduce=None):\n    loss = (target.float().squeeze() - output.float().squeeze()) ** 2\n    if reduce is None:\n        if reduction == 'mean':\n            return torch.mean(loss)\n        elif reduction == 'sum':\n            return torch.sum(loss)\n        elif reduction == 'none':\n            return loss\n        else:\n            NotImplementedError(reduction)\n    elif not reduce:\n        return loss\n    else:\n        NotImplementedError('use reduction if reduce=True')\n\ndef shuffle_nodes(batch):\n    x, A, mask, labels, params_dict = batch\n    for b in range(x.shape[0]):\n        idx = np.random.permutation(x.shape[1])\n        x[b] = x[b, idx]\n        A[b] = A[b, :, idx][idx, :]\n        mask[b] = mask[b, idx]\n        if 'node_attn' in params_dict:\n            params_dict['node_attn'][b] = params_dict['node_attn'][b, idx]\n    return [x, A, mask, labels, params_dict]\n\ndef copy_batch(data):\n    data_cp = []\n    for i in range(len(data)):\n        if isinstance(data[i], dict):\n            data_cp.append({key: data[i][key].clone() for key in data[i]})\n        else:\n            data_cp.append(data[i].clone())\n    return data_cp\n\ndef sanity_check(model, data):\n    with torch.no_grad():\n        output1 = model(copy_batch(data))[0]\n        output2 = model(shuffle_nodes(copy_batch(data)))[0]\n        if not torch.allclose(output1, output2, rtol=0.01, atol=0.001):\n            print('WARNING: model outputs different depending on the nodes order', (torch.norm(output1 - output2), torch.max(output1 - output2), torch.max(output1), torch.max(output2)))\n    print('model is checked for nodes shuffling')\n\ndef set_seed(seed, seed_data=None):\n    random.seed(seed)\n    rnd = np.random.RandomState(seed)\n    if seed_data is not None:\n        rnd_data = np.random.RandomState(seed_data)\n    else:\n        rnd_data = rnd\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    return (rnd, rnd_data)\n\ndef compute_feature_stats(model, train_loader, device, n_batches=100):\n    print('computing mean and std of input features')\n    model.eval()\n    x = []\n    with torch.no_grad():\n        for batch_idx, data in enumerate(train_loader):\n            x.append(data[0].data.cpu().numpy())\n            if batch_idx > n_batches:\n                break\n    x = np.concatenate(x, axis=1).reshape(-1, x[0].shape[-1])\n    print('features shape loaded', x.shape)\n    mn = x.mean(axis=0, keepdims=True)\n    sd = x.std(axis=0, keepdims=True)\n    print('mn', mn)\n    print('std', sd)\n    sd[sd < 0.01] = 1\n    print('corrected (non zeros) std', sd)\n    mn = torch.from_numpy(mn).float().to(device).unsqueeze(0)\n    sd = torch.from_numpy(sd).float().to(device).unsqueeze(0)\n    return (mn, sd)\n\ndef copy_data(data, idx):\n    data_new = {}\n    for key in data:\n        if key == 'Max_degree':\n            data_new[key] = data[key]\n            print(key, data_new[key])\n        else:\n            data_new[key] = copy.deepcopy([data[key][i] for i in idx])\n            if key in ['graph_labels', 'N_edges']:\n                data_new[key] = np.array(data_new[key], np.int32)\n            print(key, len(data_new[key]))\n    return data_new\n\ndef concat_data(data):\n    data_new = {}\n    for key in data[0]:\n        if key == 'Max_degree':\n            data_new[key] = np.max(np.array([d[key] for d in data]))\n            print(key, data_new[key])\n        else:\n            if key in ['graph_labels', 'N_edges']:\n                data_new[key] = np.concatenate([d[key] for d in data])\n            else:\n                lst = []\n                for d in data:\n                    lst.extend(d[key])\n                data_new[key] = lst\n            print(key, len(data_new[key]))\n    return data_new\n\n# File: main.py\nimport argparse\nimport random\nimport datetime\nfrom torchvision import transforms\nfrom graphdata import *\nfrom train_test import *\nimport warnings\nwarnings.filterwarnings('once')\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Run experiments with Graph Neural Networks')\n    parser.add_argument('-D', '--dataset', type=str, default='colors-3', choices=['colors-3', 'colors-4', 'colors-8', 'colors-16', 'colors-32', 'triangles', 'mnist', 'mnist-75sp', 'TU'], help='colors-n means the colors dataset with n-dimensional features; TU is any dataset from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets')\n    parser.add_argument('-d', '--data_dir', type=str, default='./data', help='path to the dataset')\n    parser.add_argument('--epochs', type=int, default=None, help='# of the epochs')\n    parser.add_argument('--batch_size', type=int, default=32, help='batch size for training data')\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning Rate')\n    parser.add_argument('--lr_decay_step', type=str, default=None, help='number of epochs after which to reduce learning rate')\n    parser.add_argument('--wdecay', type=float, default=0.0001, help='weight decay')\n    parser.add_argument('--dropout', type=float, default=0, help='dropout rate')\n    parser.add_argument('-f', '--filters', type=str, default='64,64,64', help='number of filters in each graph layer')\n    parser.add_argument('-K', '--filter_scale', type=int, default=1, help='filter scale (receptive field size), must be > 0; 1 for GCN or GIN')\n    parser.add_argument('--n_hidden', type=int, default=0, help='number of hidden units inside the graph layer')\n    parser.add_argument('--aggregation', type=str, default='mean', choices=['mean', 'sum'], help='neighbors aggregation inside the graph layer')\n    parser.add_argument('--readout', type=str, default=None, choices=['mean', 'sum', 'max'], help='type of global pooling over all nodes')\n    parser.add_argument('--kl_weight', type=float, default=100, help='weight of the KL term in the loss')\n    parser.add_argument('--pool', type=str, default=None, help='type of pooling between layers, None for global pooling only')\n    parser.add_argument('--pool_arch', type=str, default=None, help='pooling layers architecture defining whether to use fully-connected layers or GNN and to which layer to attach (e.g.: fc_prev, gnn_prev, fc_curr, gnn_curr, fc_prev_32)')\n    parser.add_argument('--init', type=str, default='normal', choices=['normal', 'uniform'], help='distribution used for initialization for the attention model')\n    parser.add_argument('--scale', type=str, default='1', help='initialized weights scale for the attention model, set to None to use PyTorch default init')\n    parser.add_argument('--degree_feature', action='store_true', default=False, help='use degree features (only for the Triangles dataset)')\n    parser.add_argument('--n_nodes', type=int, default=25, help='maximum number of nodes in the training set for collab, proteins and dd (35 for collab, 25 for proteins, 200 or 300 for dd)')\n    parser.add_argument('--cv_folds', type=int, default=5, help='number of folds for cross-validating hyperparameters for collab, proteins and dd (5 or 10 shows similar results, 5 is faster)')\n    parser.add_argument('--cv_threads', type=int, default=5, help='number of parallel threads for cross-validation')\n    parser.add_argument('--tune_init', action='store_true', default=False, help='do not tune initialization hyperparameters')\n    parser.add_argument('--ax', action='store_true', default=False, help='use AX for hyperparameter optimization (recommended)')\n    parser.add_argument('--ax_trials', type=int, default=30, help='number of AX trials (hyperparameters optimization steps)')\n    parser.add_argument('--cv', action='store_true', default=False, help='run in the cross-validation mode')\n    parser.add_argument('--seed_data', type=int, default=111, help='random seed for data splits')\n    parser.add_argument('--img_features', type=str, default='mean,coord', help='image features to use as node features')\n    parser.add_argument('--img_noise_levels', type=str, default=None, help='Gaussian noise standard deviations for grayscale and color image features')\n    parser.add_argument('--validation', action='store_true', default=False, help='run in the validation mode')\n    parser.add_argument('--debug', action='store_true', default=False, help='evaluate on the test set after each epoch (only for visualization purposes)')\n    parser.add_argument('--eval_attn_train', action='store_true', default=False, help='evaluate attention and save coefficients on the training set for models without learnable attention')\n    parser.add_argument('--eval_attn_test', action='store_true', default=False, help='evaluate attention and save coefficients on the test set for models without learnable attention')\n    parser.add_argument('--test_batch_size', type=int, default=100, help='batch size for test data')\n    parser.add_argument('--alpha_ws', type=str, default=None, help='attention labels that will be used for (weak)supervision')\n    parser.add_argument('--log_interval', type=int, default=400, help='print interval')\n    parser.add_argument('--results', type=str, default='./results', help='directory to save model checkpoints and other results, set to None to prevent saving anything')\n    parser.add_argument('--resume', type=str, default=None, help='checkpoint to load the model and optimzer states from and continue training')\n    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'], help='cuda/cpu')\n    parser.add_argument('--seed', type=int, default=111, help='random seed for model parameters')\n    parser.add_argument('--threads', type=int, default=0, help='number of threads for data loader')\n    args = parser.parse_args()\n    if args.readout in [None, 'None']:\n        args.readout = 'max'\n    set_default_lr_decay_step = args.lr_decay_step in [None, 'None']\n    if args.epochs in [None, 'None']:\n        if args.dataset.find('mnist') >= 0:\n            args.epochs = 30\n            if set_default_lr_decay_step:\n                args.lr_decay_step = '20,25'\n        elif args.dataset == 'triangles':\n            args.epochs = 100\n            if set_default_lr_decay_step:\n                args.lr_decay_step = '85,95'\n        elif args.dataset == 'TU':\n            args.epochs = 50\n            if set_default_lr_decay_step:\n                args.lr_decay_step = '25,35,45'\n        elif args.dataset.find('color') >= 0:\n            if args.readout in [None, 'None']:\n                args.readout = 'sum'\n            if args.pool in [None, 'None']:\n                args.epochs = 100\n                if set_default_lr_decay_step:\n                    args.lr_decay_step = '90'\n            else:\n                args.epochs = 300\n                if set_default_lr_decay_step:\n                    args.lr_decay_step = '280'\n        else:\n            raise NotImplementedError(args.dataset)\n    args.lr_decay_step = list(map(int, args.lr_decay_step.split(',')))\n    args.filters = list(map(int, args.filters.split(',')))\n    args.img_features = args.img_features.split(',')\n    args.img_noise_levels = None if args.img_noise_levels in [None, 'None'] else list(map(float, args.img_noise_levels.split(',')))\n    args.pool = None if args.pool in [None, 'None'] else args.pool.split('_')\n    args.pool_arch = None if args.pool_arch in [None, 'None'] else args.pool_arch.split('_')\n    try:\n        args.scale = float(args.scale)\n    except:\n        args.scale = None\n    args.torch = torch.__version__\n    for arg in vars(args):\n        print(arg, getattr(args, arg))\n    return args\n\ndef load_synthetic(args):\n    train_dataset = SyntheticGraphs(args.data_dir, args.dataset, 'train', degree_feature=args.degree_feature, attn_coef=args.alpha_ws)\n    test_dataset = SyntheticGraphs(args.data_dir, args.dataset, 'val' if args.validation else 'test', degree_feature=args.degree_feature)\n    loss_fn = mse_loss\n    collate_fn = collate_batch\n    in_features = train_dataset.feature_dim\n    out_features = 1\n    return (train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features)\n\ndef load_mnist(args):\n    use_mean_px = 'mean' in args.img_features\n    use_coord = 'coord' in args.img_features\n    assert use_mean_px, ('this mode is not well supported', use_mean_px)\n    gt_attn_threshold = 0 if args.pool is not None and args.pool[1] in ['gt'] and (args.filter_scale > 1) else 0.5\n    if args.dataset == 'mnist':\n        train_dataset = MNIST(args.data_dir, train=True, download=True, transform=transforms.ToTensor(), attn_coef=args.alpha_ws)\n    else:\n        train_dataset = MNIST75sp(args.data_dir, split='train', use_mean_px=use_mean_px, use_coord=use_coord, gt_attn_threshold=gt_attn_threshold, attn_coef=args.alpha_ws)\n    noises, color_noises = (None, None)\n    if args.validation:\n        n_val = 5000\n        if args.dataset == 'mnist':\n            train_dataset.train_data = train_dataset.train_data[:-n_val]\n            train_dataset.train_labels = train_dataset.train_labels[:-n_val]\n            test_dataset = MNIST(args.data_dir, train=True, download=True, transform=transforms.ToTensor())\n            test_dataset.train_data = train_dataset.train_data[-n_val:]\n            test_dataset.train_labels = train_dataset.train_labels[-n_val:]\n        else:\n            train_dataset.train_val_split(np.arange(0, train_dataset.n_samples - n_val))\n            test_dataset = MNIST75sp(args.data_dir, split='train', use_mean_px=use_mean_px, use_coord=use_coord, gt_attn_threshold=gt_attn_threshold)\n            test_dataset.train_val_split(np.arange(train_dataset.n_samples - n_val, train_dataset.n_samples))\n    else:\n        noise_file = pjoin(args.data_dir, '%s_noise.pt' % args.dataset.replace('-', '_'))\n        color_noise_file = pjoin(args.data_dir, '%s_color_noise.pt' % args.dataset.replace('-', '_'))\n        if args.dataset == 'mnist':\n            test_dataset = MNIST(args.data_dir, train=False, download=True, transform=transforms.ToTensor())\n            noise_shape = (len(test_dataset.test_labels), 28 * 28)\n        else:\n            test_dataset = MNIST75sp(args.data_dir, split='test', use_mean_px=use_mean_px, use_coord=use_coord, gt_attn_threshold=gt_attn_threshold)\n            noise_shape = (len(test_dataset.labels), 75)\n        noises = load_save_noise(noise_file, noise_shape)\n        color_noises = load_save_noise(color_noise_file, (noise_shape[0], noise_shape[1], 3))\n    if args.dataset == 'mnist':\n        A, coord, mask = precompute_graph_images(train_dataset.train_data.shape[1])\n        collate_fn = lambda batch: collate_batch_images(batch, A, mask, use_mean_px=use_mean_px, coord=coord if use_coord else None, gt_attn_threshold=gt_attn_threshold, replicate_features=args.img_noise_levels is not None)\n    else:\n        train_dataset.precompute_graph_data(replicate_features=args.img_noise_levels is not None, threads=12)\n        test_dataset.precompute_graph_data(replicate_features=args.img_noise_levels is not None, threads=12)\n        collate_fn = collate_batch\n    loss_fn = F.cross_entropy\n    in_features = 0 if args.img_noise_levels is None else 2\n    for features in args.img_features:\n        if features == 'mean':\n            in_features += 1\n        elif features == 'coord':\n            in_features += 2\n        else:\n            raise NotImplementedError(features)\n    in_features = np.max((in_features, 1))\n    out_features = 10\n    return (train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, noises, color_noises)\n\ndef load_TU(args, cv_folds=5):\n    loss_fn = F.cross_entropy\n    collate_fn = collate_batch\n    scale, init = (args.scale, args.init)\n    n_hidden_attn = float(args.pool_arch[2]) if args.pool_arch is not None and len(args.pool_arch) > 2 else 0\n    if args.pool is None:\n        datareader = DataReader(data_dir=args.data_dir, N_nodes=args.n_nodes, rnd_state=rnd_data, folds=0)\n        train_dataset = GraphData(datareader, None, 'train_val')\n        test_dataset = GraphData(datareader, None, 'test')\n        in_features = train_dataset.num_features\n        out_features = train_dataset.num_classes\n        pool = args.pool\n        kl_weight = args.kl_weight\n    elif args.pool[1] == 'gt':\n        raise ValueError('ground truth attention for TU datasets is not available')\n    elif args.pool[1] in ['sup', 'unsup']:\n        datareader = DataReader(data_dir=args.data_dir, N_nodes=args.n_nodes, rnd_state=rnd_data, folds=cv_folds)\n        if args.ax:\n            best_parameters = ax_optimize(datareader, args, collate_fn, loss_fn, None, folds=cv_folds, threads=args.cv_threads, n_trials=args.ax_trials)\n            pool = args.pool\n            kl_weight = best_parameters['kl_weight']\n            if args.tune_init:\n                scale, init = (best_parameters['scale'], best_parameters['init'])\n            n_hidden_attn, layer = (best_parameters['n_hidden_attn'], 1)\n            if layer == 0:\n                pool = copy.deepcopy(args.pool)\n                del pool[3]\n            pool = set_pool(best_parameters['pool'], pool)\n        else:\n            if not args.cv:\n                pool_thresh_values = np.array([float(args.pool[-1])])\n                n_hiddens = [n_hidden_attn]\n                layers = [1]\n            elif args.debug:\n                pool_thresh_values = np.array([0.0001, 0.1])\n                n_hiddens = [n_hidden_attn]\n                layers = [1]\n            else:\n                if args.data_dir.lower().find('proteins') >= 0:\n                    pool_thresh_values = np.array([0.002, 0.005, 0.01, 0.03, 0.05])\n                elif args.data_dir.lower().find('dd') >= 0:\n                    pool_thresh_values = np.array([0.0001, 0.001, 0.002, 0.005, 0.01, 0.03, 0.05, 0.1])\n                elif args.data_dir.lower().find('collab') >= 0:\n                    pool_thresh_values = np.array([0.001, 0.002, 0.005, 0.01, 0.03, 0.05, 0.1])\n                else:\n                    raise NotImplementedError('this dataset is not supported currently')\n                n_hiddens = np.array([0, 32])\n                layers = np.array([0, 1])\n            if args.pool[1] == 'sup' and (not args.debug) and args.cv:\n                kl_weight_values = np.array([0.25, 1, 2, 10])\n            else:\n                kl_weight_values = np.array([args.kl_weight])\n            if len(pool_thresh_values) > 1 or len(kl_weight_values) > 1 or len(n_hiddens) > 1 or (len(layers) > 1):\n                val_acc = np.zeros((len(layers), len(n_hiddens), len(pool_thresh_values), len(kl_weight_values)))\n                for i_, layer in enumerate(layers):\n                    if layer == 0:\n                        pool = copy.deepcopy(args.pool)\n                        del pool[3]\n                    else:\n                        pool = args.pool\n                    for j_, n_hidden_attn in enumerate(n_hiddens):\n                        for k_, pool_thresh in enumerate(pool_thresh_values):\n                            for m_, kl_weight in enumerate(kl_weight_values):\n                                val_acc[i_, j_, k_, m_] = cross_validation(datareader, args, collate_fn, loss_fn, set_pool(pool_thresh, pool), kl_weight, None, n_hidden_attn=n_hidden_attn, folds=cv_folds, threads=args.cv_threads)\n                ind1, ind2, ind3, ind4 = np.where(val_acc == np.max(val_acc))\n                print(val_acc)\n                print(ind1, ind2, ind3, ind4, layers[ind1], n_hiddens[ind2], pool_thresh_values[ind3], kl_weight_values[ind4], val_acc[ind1[0], ind2[0], ind3[0], ind4[0]])\n                layer = layers[ind1[0]]\n                if layer == 0:\n                    pool = copy.deepcopy(args.pool)\n                    del pool[3]\n                else:\n                    pool = args.pool\n                n_hidden_attn = n_hiddens[ind2[0]]\n                pool = set_pool(pool_thresh_values[ind3[0]], pool)\n                kl_weight = kl_weight_values[ind4[0]]\n            else:\n                pool = args.pool\n                kl_weight = args.kl_weight\n        train_dataset = GraphData(datareader, None, 'train_val')\n        test_dataset = GraphData(datareader, None, 'test')\n        in_features = train_dataset.num_features\n        out_features = train_dataset.num_classes\n        if args.pool[1] == 'sup':\n            train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n            train_loader_test = DataLoader(train_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n            start_epoch, model, optimizer, scheduler = create_model_optimizer(in_features, out_features, None, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n            for epoch in range(start_epoch, args.epochs + 1):\n                scheduler.step()\n                train_loss, acc = train(model, train_loader, optimizer, epoch, args, loss_fn, None)\n            train_loss, train_acc, attn_WS = test(model, train_loader_test, epoch, loss_fn, 'train', args, None, eval_attn=True)[:3]\n            train_dataset = GraphData(datareader, None, 'train_val', attn_labels=attn_WS)\n    else:\n        raise NotImplementedError(args.pool)\n    return (train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, pool, kl_weight, scale, init, n_hidden_attn)\nif __name__ == '__main__':\n    dt = datetime.datetime.now()\n    print('start time:', dt)\n    args = parse_args()\n    args.experiment_ID = '%06d' % dt.microsecond\n    print('experiment_ID: ', args.experiment_ID)\n    if args.cv_threads > 1 and args.dataset == 'TU':\n        torch.multiprocessing.set_start_method('spawn')\n    print('gpus: ', torch.cuda.device_count())\n    if args.results not in [None, 'None'] and (not os.path.isdir(args.results)):\n        os.mkdir(args.results)\n    rnd, rnd_data = set_seed(args.seed, args.seed_data)\n    pool = args.pool\n    kl_weight = args.kl_weight\n    scale = args.scale\n    init = args.init\n    n_hidden_attn = float(args.pool_arch[2]) if args.pool_arch is not None and len(args.pool_arch) > 2 else 0\n    if args.dataset.find('colors') >= 0 or args.dataset == 'triangles':\n        train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features = load_synthetic(args)\n    elif args.dataset in ['mnist', 'mnist-75sp']:\n        train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, noises, color_noises = load_mnist(args)\n    else:\n        train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, pool, kl_weight, scale, init, n_hidden_attn = load_TU(args, cv_folds=args.cv_folds)\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n    train_loader_test = DataLoader(train_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n    print('test_dataset', test_dataset.split)\n    test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n    start_epoch, model, optimizer, scheduler = create_model_optimizer(in_features, out_features, pool, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n    feature_stats = None\n    if args.dataset in ['mnist', 'mnist-75sp']:\n        feature_stats = compute_feature_stats(model, train_loader, args.device, n_batches=1000)\n\n    def test_fn(loader, epoch, split, eval_attn):\n        test_loss, acc, _, _ = test(model, loader, epoch, loss_fn, split, args, feature_stats, noises=None, img_noise_level=None, eval_attn=eval_attn, alpha_WS_name='orig')\n        if args.dataset in ['mnist', 'mnist-75sp'] and split == 'test' and (args.img_noise_levels is not None):\n            test(model, loader, epoch, loss_fn, split, args, feature_stats, noises=noises, img_noise_level=args.img_noise_levels[0], eval_attn=eval_attn, alpha_WS_name='noisy')\n            test(model, loader, epoch, loss_fn, split, args, feature_stats, noises=color_noises, img_noise_level=args.img_noise_levels[1], eval_attn=eval_attn, alpha_WS_name='noisy-c')\n        return (test_loss, acc)\n    if start_epoch > args.epochs:\n        print('evaluating the model')\n        test_fn(test_loader, start_epoch - 1, 'val' if args.validation else 'test', args.eval_attn_test)\n    else:\n        for epoch in range(start_epoch, args.epochs + 1):\n            eval_epoch = epoch <= 1 or epoch == args.epochs\n            scheduler.step()\n            train_loss, acc = train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats)\n            if eval_epoch:\n                save_checkpoint(model, scheduler, optimizer, args, epoch)\n                test_fn(train_loader_test, epoch, 'train', epoch == args.epochs and args.eval_attn_train)\n            if args.validation:\n                test_fn(test_loader, epoch, 'val', epoch == args.epochs and args.eval_attn_test)\n            elif eval_epoch or args.debug:\n                test_fn(test_loader, epoch, 'test', epoch == args.epochs and args.eval_attn_test)\n    print('done in {}'.format(datetime.datetime.now() - dt))\n\n# File: generate_data.py\nimport os\nimport numpy as np\nimport pickle\nimport argparse\nimport networkx as nx\nimport datetime\nimport random\nimport multiprocessing as mp\nfrom utils import *\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Generate synthetic graph datasets')\n    parser.add_argument('-D', '--dataset', type=str, default='colors', choices=['colors', 'triangles'])\n    parser.add_argument('-o', '--out_dir', type=str, default='./data', help='path where to save superpixels')\n    parser.add_argument('--N_train', type=int, default=500, help='number of training graphs (500 for colors and 30000 for triangles)')\n    parser.add_argument('--N_val', type=int, default=2500, help='number of graphs in the validation set (2500 for colors and 5000 for triangles)')\n    parser.add_argument('--N_test', type=int, default=2500, help='number of graphs in each test subset (2500 for colors and 5000 for triangles)')\n    parser.add_argument('--label_min', type=int, default=0, help='smallest label value for a graph (i.e. smallest number of green nodes); 1 for triangles')\n    parser.add_argument('--label_max', type=int, default=10, help='largest label value for a graph (i.e. largest number of green nodes)')\n    parser.add_argument('--N_min', type=int, default=4, help='minimum number of nodes')\n    parser.add_argument('--N_max', type=int, default=200, help='maximum number of nodes (default: 200 for colors and 100 for triangles')\n    parser.add_argument('--N_max_train', type=int, default=25, help='maximum number of nodes in the training set')\n    parser.add_argument('--dim', type=int, default=3, help='node feature dimensionality')\n    parser.add_argument('--green_ch_index', type=int, default=1, help='index of non-zero value in a one-hot node feature vector, i.e. [0, 1, 0] in case green_channel_index=1 and dim=3')\n    parser.add_argument('--seed', type=int, default=111, help='seed for shuffling nodes')\n    parser.add_argument('--threads', type=int, default=0, help='only for triangles')\n    args = parser.parse_args()\n    for arg in vars(args):\n        print(arg, getattr(args, arg))\n    return args\n\ndef check_graph_duplicates(Adj_matrices, node_features=None):\n    n_graphs = len(Adj_matrices)\n    print('check for duplicates for %d graphs' % n_graphs)\n    n_duplicates = 0\n    for i in range(n_graphs):\n        if node_features is not None:\n            assert Adj_matrices[i].shape[0] == node_features[i].shape[0], ('invalid data', i, Adj_matrices[i].shape[0], node_features[i].shape[0])\n        for j in range(i + 1, n_graphs):\n            if Adj_matrices[i].shape[0] == Adj_matrices[j].shape[0]:\n                if np.allclose(Adj_matrices[i], Adj_matrices[j]):\n                    if node_features is None or np.allclose(node_features[i], node_features[j]):\n                        n_duplicates += 1\n                        print('duplicates %d/%d' % (n_duplicates, n_graphs * (n_graphs - 1) / 2))\n    if n_duplicates > 0:\n        raise ValueError('%d duplicates found in the dataset' % n_duplicates)\n    print('no duplicated graphs')\n\ndef get_node_features_Colors(N_nodes, N_green, dim, green_ch_index=1, new_colors=False):\n    node_features = np.zeros((N_nodes, dim))\n    idx_not_green = rnd.randint(0, dim - 1, size=N_nodes - N_green)\n    idx_non_zero = np.concatenate((idx_not_green, np.zeros(N_green, np.int) + dim - 1))\n    idx_non_zero_cp = idx_non_zero.copy()\n    idx_non_zero[idx_non_zero_cp == dim - 1] = green_ch_index\n    idx_non_zero[idx_non_zero_cp == green_ch_index] = dim - 1\n    rnd.shuffle(idx_non_zero)\n    node_features[np.arange(N_nodes), idx_non_zero] = 1\n    if new_colors:\n        for ind in np.where(idx_non_zero != green_ch_index)[0]:\n            node_features[ind] = rnd.randint(0, 2, size=dim)\n            node_features[ind, green_ch_index] = 0\n    label = np.sum((np.sum(node_features, 1) == node_features[:, green_ch_index]) & (node_features[:, green_ch_index] == 1))\n    gt_attn = (idx_non_zero == green_ch_index).reshape(-1, 1)\n    label2 = np.sum(gt_attn)\n    assert N_green == label == label2, ('invalid node features', N_green, label, label2)\n    return (node_features, idx_non_zero, gt_attn)\n\ndef generate_graphs_Colors(N_graphs, N_min, N_max, dim, args, rnd, new_colors=False):\n    Adj_matrices, node_features, GT_attn, graph_labels, N_edges = ([], [], [], [], [])\n    n_labels = args.label_max - args.label_min + 1\n    n_graphs_per_shape = int(np.ceil(N_graphs / (N_max - N_min + 1) / n_labels) * n_labels)\n    for n_nodes in np.array(range(N_min, N_max + 1)):\n        c = 0\n        while True:\n            labels = np.arange(args.label_min, n_labels)\n            labels = labels[labels <= n_nodes]\n            rnd.shuffle(labels)\n            for lbl in labels:\n                features, idx_non_zero, gt_attn = get_node_features_Colors(N_nodes=n_nodes, N_green=lbl, dim=dim, green_ch_index=args.green_ch_index, new_colors=new_colors)\n                n_edges = int((rnd.rand() + 1) * n_nodes)\n                A = nx.to_numpy_array(nx.gnm_random_graph(n_nodes, n_edges))\n                add = True\n                for k in range(len(Adj_matrices)):\n                    if A.shape[0] == Adj_matrices[k].shape[0] and np.allclose(A, Adj_matrices[k]):\n                        if np.allclose(node_features[k], features):\n                            add = False\n                            break\n                if add:\n                    Adj_matrices.append(A.astype(np.bool))\n                    graph_labels.append(lbl)\n                    node_features.append(features.astype(np.bool))\n                    GT_attn.append(gt_attn)\n                    N_edges.append(n_edges)\n                    c += 1\n                    if c >= n_graphs_per_shape:\n                        break\n            if c >= n_graphs_per_shape:\n                break\n    graph_labels = np.array(graph_labels, np.int32)\n    N_edges = np.array(N_edges, np.int32)\n    print(N_graphs, len(graph_labels))\n    return {'Adj_matrices': Adj_matrices, 'GT_attn': GT_attn, 'graph_labels': graph_labels, 'node_features': node_features, 'N_edges': N_edges}\n\ndef get_gt_atnn_triangles(args):\n    G, N = args\n    node_ids = []\n    if G is not None:\n        for clq in nx.enumerate_all_cliques(G):\n            if len(clq) == 3:\n                node_ids.extend(clq)\n    node_ids = np.array(node_ids)\n    gt_attn = np.zeros((N, 1), np.int32)\n    for i in np.unique(node_ids):\n        gt_attn[i] = int(np.sum(node_ids == i))\n    return gt_attn\n\ndef get_graph_triangles(args):\n    N_nodes, rnd = args\n    N_edges = int((rnd.rand() + 1) * N_nodes)\n    G = nx.dense_gnm_random_graph(N_nodes, N_edges, seed=None)\n    A = nx.to_numpy_array(G)\n    A_cube = A.dot(A).dot(A)\n    label = int(np.trace(A_cube) / 6.0)\n    return (A.astype(np.bool), label, N_edges, G)\n\ndef generate_graphs_Triangles(N_graphs, N_min, N_max, args, rnd):\n    N_nodes = rnd.randint(N_min, N_max + 1, size=int(N_graphs * 10))\n    print('generating %d graphs with %d-%d nodes' % (N_graphs * 10, N_min, N_max))\n    if args.threads > 0:\n        with mp.Pool(processes=args.threads) as pool:\n            data = pool.map(get_graph_triangles, [(N_nodes[i], rnd) for i in range(len(N_nodes))])\n    else:\n        data = [get_graph_triangles((N_nodes[i], rnd)) for i in range(len(N_nodes))]\n    labels = np.array([data[i][1] for i in range(len(data))], np.int32)\n    Adj_matrices, node_features, G, graph_labels, N_edges, node_degrees = ([], [], [], [], [], [])\n    for lbl in range(args.label_min, args.label_max + 1):\n        idx = np.where(labels == lbl)[0]\n        c = 0\n        for i in idx:\n            add = True\n            for k in range(len(Adj_matrices)):\n                if data[i][0].shape[0] == Adj_matrices[k].shape[0] and labels[i] == graph_labels[k] and np.allclose(data[i][0], Adj_matrices[k]):\n                    add = False\n                    break\n            if add:\n                Adj_matrices.append(data[i][0])\n                graph_labels.append(labels[i])\n                G.append(data[i][3])\n                N_edges.append(data[i][2])\n                node_degrees.append(data[i][0].astype(np.int32).sum(1).max())\n                c += 1\n                if c >= int(N_graphs / (args.label_max - args.label_min + 1)):\n                    break\n        print('label={}, number of graphs={}/{}, total number of generated graphs={}'.format(lbl, c, len(idx), len(Adj_matrices)))\n        assert c == int(N_graphs / (args.label_max - args.label_min + 1)), ('invalid data', c, int(N_graphs / (args.label_max - args.label_min + 1)))\n    print('computing GT attention for %d graphs' % len(Adj_matrices))\n    if args.threads > 0:\n        with mp.Pool(processes=args.threads) as pool:\n            GT_attn = pool.map(get_gt_atnn_triangles, [(G[i], Adj_matrices[i].shape[0]) for i in range(len(Adj_matrices))])\n    else:\n        GT_attn = [get_gt_atnn_triangles((G[i], Adj_matrices[i].shape[0])) for i in range(len(Adj_matrices))]\n    graph_labels = np.array(graph_labels, np.int32)\n    N_edges = np.array(N_edges, np.int32)\n    return {'Adj_matrices': Adj_matrices, 'GT_attn': GT_attn, 'graph_labels': graph_labels, 'N_edges': N_edges, 'Max_degree': np.max(node_degrees)}\nif __name__ == '__main__':\n    dt = datetime.datetime.now()\n    print('start time:', dt)\n    args = parse_args()\n    if not os.path.isdir(args.out_dir):\n        os.mkdir(args.out_dir)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    rnd = np.random.RandomState(args.seed)\n\n    def print_stats(data, split_name):\n        print('%s: %d graphs' % (split_name, len(data['graph_labels'])))\n        for lbl in np.unique(data['graph_labels']):\n            print('%s: label=%d, %d graphs' % (split_name, lbl, np.sum(data['graph_labels'] == lbl)))\n    if args.dataset.lower() == 'colors':\n        data_test_combined, Adj_matrices, node_features = ([], [], [])\n        for N_graphs, N_nodes_min, N_nodes_max, dim, name in zip([args.N_train + args.N_val + args.N_test, args.N_test, args.N_test], [args.N_min, args.N_max_train + 1, args.N_max_train + 1], [args.N_max_train, args.N_max, args.N_max], [args.dim, args.dim, args.dim + 1], ['test orig', 'test large', 'test large-c']):\n            data = generate_graphs_Colors(N_graphs, N_nodes_min, N_nodes_max, dim, args, rnd, new_colors=dim == args.dim + 1)\n            if name.find('orig') >= 0:\n                idx = rnd.permutation(len(data['graph_labels']))\n                data_train = copy_data(data, idx[:args.N_train])\n                print_stats(data_train, name.replace('test', 'train'))\n                node_features += data_train['node_features']\n                Adj_matrices += data_train['Adj_matrices']\n                data_val = copy_data(data, idx[args.N_train:args.N_train + args.N_val])\n                print_stats(data_val, name.replace('test', 'val'))\n                node_features += data_val['node_features']\n                Adj_matrices += data_val['Adj_matrices']\n                data_test = copy_data(data, idx[args.N_train + args.N_val:args.N_train + args.N_val + args.N_test])\n            else:\n                data_test = copy_data(data, rnd.permutation(len(data['graph_labels']))[:args.N_test])\n            Adj_matrices += data_test['Adj_matrices']\n            node_features += data_test['node_features']\n            data_test_combined.append(data_test)\n            print_stats(data_test, name)\n        check_graph_duplicates(Adj_matrices, node_features)\n        with open('%s/random_graphs_colors_dim%d_train.pkl' % (args.out_dir, args.dim), 'wb') as f:\n            pickle.dump(data_train, f, protocol=2)\n        with open('%s/random_graphs_colors_dim%d_val.pkl' % (args.out_dir, args.dim), 'wb') as f:\n            pickle.dump(data_val, f, protocol=2)\n        with open('%s/random_graphs_colors_dim%d_test.pkl' % (args.out_dir, args.dim), 'wb') as f:\n            pickle.dump(concat_data(data_test_combined), f, protocol=2)\n    elif args.dataset.lower() == 'triangles':\n        data = generate_graphs_Triangles(args.N_train + args.N_val + args.N_test, args.N_min, args.N_max_train, args, rnd)\n        idx_train, idx_val, idx_test = ([], [], [])\n        classes = np.unique(data['graph_labels'])\n        n_classes = len(classes)\n        for lbl in classes:\n            idx = np.where(data['graph_labels'] == lbl)[0]\n            rnd.shuffle(idx)\n            n_train = int(args.N_train / n_classes)\n            n_val = int(args.N_val / n_classes)\n            n_test = int(args.N_test / n_classes)\n            idx_train.append(idx[:n_train])\n            idx_val.append(idx[n_train:n_train + n_val])\n            idx_test.append(idx[n_train + n_val:n_train + n_val + n_test])\n        data_train = copy_data(data, np.concatenate(idx_train))\n        print_stats(data_train, 'train orig')\n        data_val = copy_data(data, np.concatenate(idx_val))\n        print_stats(data_val, 'val orig')\n        data_test = copy_data(data, np.concatenate(idx_test))\n        print_stats(data_test, 'test orig')\n        data = generate_graphs_Triangles(args.N_test, args.N_max_train + 1, args.N_max, args, rnd)\n        data_test_large = copy_data(data, rnd.permutation(len(data['graph_labels']))[:args.N_test])\n        print_stats(data_test_large, 'test large')\n        check_graph_duplicates(data_train['Adj_matrices'] + data_val['Adj_matrices'] + data_test['Adj_matrices'] + data_test_large['Adj_matrices'])\n        max_degree = np.max(np.array([d['Max_degree'] for d in (data_train, data_val, data_test, data_test_large)]))\n        data_train['Max_degree'] = max_degree\n        with open('%s/random_graphs_triangles_train.pkl' % args.out_dir, 'wb') as f:\n            pickle.dump(data_train, f, protocol=2)\n        data_val['Max_degree'] = max_degree\n        with open('%s/random_graphs_triangles_val.pkl' % args.out_dir, 'wb') as f:\n            pickle.dump(data_val, f, protocol=2)\n        data_test = concat_data((data_test, data_test_large))\n        data_test['Max_degree'] = max_degree\n        with open('%s/random_graphs_triangles_test.pkl' % args.out_dir, 'wb') as f:\n            pickle.dump(data_test, f, protocol=2)\n    else:\n        raise NotImplementedError('unsupported dataset: ' + args.dataset)\n    print('done in {}'.format(datetime.datetime.now() - dt))",
        "experimental_info": ""
      }
    },
    {
      "title": "Classification-Based Anomaly Detection for General Data",
      "full_text": "Published as a conference paper at ICLR 2020 CLASSIFICATION -BASED ANOMALY DETECTION FOR GENERAL DATA Liron Bergman Yedid Hoshen School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel ABSTRACT Anomaly detection, ﬁnding patterns that substantially deviate from those seen pre- viously, is one of the fundamental problems of artiﬁcial intelligence. Recently, classiﬁcation-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the appli- cability of transformation-based methods to non-image data using random afﬁne transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is exten- sively validated on multiple datasets from different domains. 1 I NTRODUCTION Detecting anomalies in perceived data is a key ability for humans and for artiﬁcial intelligence. Hu- mans often detect anomalies to give early indications of danger or to discover unique opportunities. Anomaly detection systems are being used by artiﬁcial intelligence to discover credit card fraud, for detecting cyber intrusion, alert predictive maintenance of industrial equipment and for discovering attractive stock market opportunities. The typical anomaly detection setting is a one class classi- ﬁcation task, where the objective is to classify data as normal or anomalous. The importance of the task stems from being able to raise an alarm when detecting a different pattern from those seen in the past, therefore triggering further inspection. This is fundamentally different from supervised learning tasks, in which examples of all data classes are observed. There are different possible scenarios for anomaly detection methods. In supervised anomaly de- tection, we are given training examples of normal and anomalous patterns. This scenario can be quite well speciﬁed, however obtaining such supervision may not be possible. For example in cyber security settings, we will not have supervised examples of new, unknown computer viruses making supervised training difﬁcult. On the other extreme, fully unsupervised anomaly detection, obtains a stream of data containing normal and anomalous patterns and attempts to detect the anomalous data. In this work we deal with the semi-supervised scenario. In this setting, we have a training set of normal examples (which contains no anomalies). After training the anomaly detector, we detect anomalies in the test data, containing both normal and anomalous examples. This supervision is easy to obtain in many practical settings and is less difﬁcult than the fully-unsupervised case. Many anomaly detection methods have been proposed over the last few decades. They can be broadly classiﬁed into reconstruction and statistically based methods.Recently, deep learning meth- ods based on classiﬁcation have achieved superior results. Most semi-supervised classiﬁcation- based methods attempt to solve anomaly detection directly, despite only having normal training data. One example is: Deep-SVDD (Ruff et al., 2018) - one-class classiﬁcation using a learned deep space. Another type of classiﬁcation-based methods is self-supervised i.e. methods that solve one or more classiﬁcation-based auxiliary tasks on the normal training data, and this is shown to be useful for solving anomaly detection, the task of interest e.g. (Golan & El-Yaniv, 2018). Self-supervised classiﬁcation-based methods have been proposed with the object of image anomaly detection, but we show that by generalizing the class of transformations they can apply to all data types. In this paper, we introduce a novel technique, GOAD, for anomaly detection which uniﬁes current state-of-the-art methods that use normal training data only and are based on classiﬁcation. Our method ﬁrst transforms the data into M subspaces, and learns a feature space such that inter-class 1 arXiv:2005.02359v1  [cs.LG]  5 May 2020Published as a conference paper at ICLR 2020 separation is larger than intra-class separation. For the learned features, the distance from the cluster center is correlated with the likelihood of anomaly. We use this criterion to determine if a new data point is normal or anomalous. We also generalize the class of transformation functions to include afﬁne transformation which allows our method to generalize to non-image data. This is signiﬁcant as tabular data is probably the most important for applications of anomaly detection. Our method is evaluated on anomaly detection on image and tabular datasets (cyber security and medical) and is shown to signiﬁcantly improve over the state-of-the-art. 1.1 P REVIOUS WORKS Anomaly detection methods can be generally divided into the following categories: Reconstruction Methods: Some of the most common anomaly detection methods are reconstruction- based. The general idea behind such methods is that every normal sample should be reconstructed accurately using a limited set of basis functions, whereas anomalous data should suffer from larger reconstruction costs. The choice of features, basis and loss functions differentiates between the different methods. Some of the earliest methods use: nearest neighbors (Eskin et al., 2002), low-rank PCA (Jolliffe, 2011; Cand`es et al., 2011) or K-means (Hartigan & Wong, 1979) as the reconstruction basis. Most recently, neural networks were used (Sakurada & Yairi, 2014; Xia et al., 2015) for learning deep basis functions for reconstruction. Another set of recent methods (Schlegl et al., 2017; Deecke et al., 2018) use GANs to learn a reconstruction basis function. GANs suffer from mode-collapse and are difﬁcult to invert, which limits the performance of such methods. Distributional Methods: Another set of commonly used methods are distribution-based. The main theme in such methods is to model the distribution of normal data. The expectation is that anomalous test data will have low likelihood under the probabilistic model while normal data will have higher likelihoods. Methods differ in the features used to describe the data and the probabilistic model used to estimate the normal distribution. Some early methods used Gaussian or Gaussian mixture models. Such models will only work if the data under the selected feature space satisﬁes the prob- abilistic assumptions implicied by the model. Another set of methods used non-parametric density estimate methods such as kernel density estimate (Parzen, 1962). Recently, deep learning methods (autoencoders or variational autoencoders) were used to learn deep features which are sometimes easier to model than raw features (Yang et al., 2017). DAGMM introduced by Zong et al. (2018) learn the probabilistic model jointly with the deep features therefore shaping the features space to better conform with the probabilistic assumption. Classiﬁcation-Based Methods: Another paradigm for anomaly detection is separation between space regions containing normal data from all other regions. An example of such approach is One-Class SVM (Scholkopf et al., 2000), which trains a classiﬁer to perform this separation. Learning a good feature space for performing such separation is performed both by the classic kernel methods as well as by the recent deep learning approach (Ruff et al., 2018). One of the main challenges in unsupervised (or semi-supervised) learning is providing an objective for learning features that are relevant to the task of interest. One method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive. Auxiliary tasks for learning high-quality image features include: video frame prediction (Mathieu et al., 2016), image colorization (Zhang et al., 2016; Larsson et al., 2016), puzzle solving (Noroozi & Favaro, 2016) - predicting the correct order of random permuted image patches. Recently, Gidaris et al. (2018) used a set of image processing transformations (rotation by 0,90,180,270 degrees around the image axis, and predicted the true image orientation has been used to learn high-quality image features. Golan & El-Yaniv (2018), have used similar image-processing task prediction for detecting anomalies in images. This method has shown good performance on detecting images from anomalous classes. In this work, we overcome some of the limitations of previous classiﬁcation-based methods and extend their applicability of self-supervised methods to general data types. We also show that our method is more robust to adversarial attacks. 2 C LASSIFICATION -BASED ANOMALY DETECTION Classiﬁcation-based methods have dominated supervised anomaly detection. In this section we will analyse semi-supervised classiﬁcation-based methods: 2Published as a conference paper at ICLR 2020 Let us assume all data lies in spaceRL (where Lis the data dimension). Normal data lie in subspace X ⊂RL. We assume that all anomalies lie outsideX. To detect anomalies, we would therefore like to build a classiﬁer C, such that C(x) = 1if x∈X and C(x) = 0if x∈RL\\X. One-class classiﬁcation methods attempt to learn C directly as P(x ∈X). Classical approaches have learned a classiﬁer either in input space or in a kernel space. Recently, Deep-SVDD (Ruff et al., 2018) learned end-to-end to i) transform the data to an isotropic feature space f(x) ii) ﬁt the minimal hypersphere of radius Rand center c0 around the features of the normal training data. Test data is classiﬁed as anomalous if the following normality score is positive: ∥f(x) −c0∥2 −R2. Learning an effective feature space is not a simple task, as the trivial solution of f(x) = 0 ∀ x results in the smallest hypersphere, various tricks are used to avoid this possibility. Geometric-transformation classiﬁcation (GEOM), proposed by Golan & El-Yaniv (2018) ﬁrst trans- forms the normal data subspace X into M subspaces X1..XM . This is done by transforming each image x ∈X using M different geometric transformations (rotation, reﬂection, translation) into T(x,1)..T(x,M). Although these transformations are image speciﬁc, we will later extend the class of transformations to all afﬁne transformations making this applicable to non-image data. They set an auxiliary task of learning a classiﬁer able to predict the transformation labelmgiven transformed data point T(x,m). As the training set consists of normal data only, each sample is x ∈X and the transformed sample is in ∪mXm. The method attempts to estimate the following conditional probability: P(m′|T(x,m)) = P(T(x,m) ∈Xm′ )P(m′)∑ ˜m P(T(x,m) ∈X˜m)P( ˜m) = P(T(x,m) ∈Xm′ )∑ ˜m P(T(x,m) ∈X˜m) (1) Where the second equality follows by design of the training set, and where every training sample is transformed exactly once by each transformation leading to equal priors. For anomalous data x∈RL\\X, by construction of the subspace, if the transformations T are one- to-one, it follows that the transformed sample does not fall in the appropriate subspace: T(x,m) ∈ RL\\Xm. GEOM uses P(m|T(x,m)) as a score for determining if x is anomalous i.e. that x ∈ RL\\X. GEOM gives samples with low probabilities P(m|T(x,m)) high anomaly scores. A signiﬁcant issue with this methodology, is that the learned classiﬁer P(m′|T(x,m)) is only valid for samples x ∈X which were found in the training set. For x ∈RL\\X we should in fact have P(T(x,m) ∈Xm′ ) = 0for all m= 1..M (as the transformed xis not in any of the subsets). This makes the anomaly score P(m′|T(x,m)) have very high variance for anomalies. One way to overcome this issue is by using examples of anomaliesxa and training P(m|T(x,m)) = 1 M on anomalous data. This corresponds to the supervised scenario and was recently introduced as Outlier Exposure (Hendrycks et al., 2018). Although getting such supervision is possible for some image tasks (where large external datasets can be used) this is not possible in the general case e.g. for tabular data which exhibits much more variation between datasets. 3 D ISTANCE -BASED MULTIPLE TRANSFORMATION CLASSIFICATION We propose a novel method to overcome the generalization issues highlighted in the previous section by using ideas from open-set classiﬁcation (Bendale & Boult, 2016). Our approach uniﬁes one-class and transformation-based classiﬁcation methods. Similarly to GEOM, we transform Xto X1..XM . We learn a feature extractor f(x) using a neural network, which maps the original input data into a feature representation. Similarly to deep OC methods, we model each subspace Xm mapped to the feature space {f(x)|x∈Xm}as a sphere with center cm. The probability of data point xafter transformation mis parameterized by P(T(x,m) ∈X′ m) = 1 Z e−(f(T(x,m))−c′ m)2 . The classiﬁer predicting transformation mgiven a transformed point is therefore: P(m′|T(x,m)) = e−∥f(T(x,m))−cm′ ∥2 ∑ ˜m e−∥f(T(x,m))−c ˜m∥2 (2) The centers cm are given by the average feature over the training set for every transformation i.e. cm = 1 N ∑ x∈X f(T(x,m)). One option is to directly learn f by optimizing cross-entropy between 3Published as a conference paper at ICLR 2020 P(m′|T(x,m)) and the correct label on the normal training set. In practice we obtained better results by training f using the center triplet loss (He et al., 2018), which learns supervised clus- ters with low intra-class variation, and high-inter-class variation by optimizing the following loss function (where sis a margin regularizing the distance between clusters): L= ∑ i max(∥f(T(xi,m)) −cm∥2 + s−minm′̸=m∥f(T(xi,m)) −cm′ ∥2,0) (3) Having learned a feature space in which the different transformation subspaces are well separated, we use the probability in Eq. 2 as a normality score. However, for data far away from the normal distributions, the distances from the means will be large. A small difference in distance will make the classiﬁer unreasonably certain of a particular transformation. To add a general prior for uncer- tainty far from the training set, we add a small regularizing constant ϵ to the probability of each transformation. This ensures equal probabilities for uncertain regions: ˜P(m′|T(x,m)) = e−∥f(T(x,m))−cm′ ∥2 + ϵ∑ ˜m e−∥f(T(x,m))−c ˜m∥2 + M ·ϵ (4) At test time we transform each sample by the M transformations. By assuming independence be- tween transformations, the probability thatxis normal (i.e. x∈X) is the product of the probabilities that all transformed samples are in their respective subspace. For log-probabilities the total score is given by: Score(x) =−log P(x∈X) =− ∑ m log ˜P(T(x,m) ∈Xm) =− ∑ m log ˜P(m|T(x,m)) (5) The score computes the degree of anomaly of each sample. Higher scores indicate a more anomalous sample. Algorithm 1:GOAD: Training Algorithm Input: Normal training data x1,x2...xN Transformations T(,1),T(,2)...T(,M) Output: Feature extractor f, centers c1,c2...cM T(xi,1),T(xi,2)...T(xi,M) ←xi // Transform each sample by all transformations 1 to M Find f,c1,c2...cM that optimize the triplet loss in Eq. 3 Algorithm 2:GOAD: Evaluation Algorithm Input: Test sample: x, feature extractor: f, centers: c1,c2...cM , transformations: T(,1),T(,2)...T(,M) Output: Score(x) T(x,1),T(x,2)...T(x,M) ←x // Transform test sample by all transformations 1 to M P(m|T(x,m)) ←f(T(x,m)),c1,c2...cM // Likelihood of predicting the correct transformation (Eq. 4) Score(x) ←P(1|T(x,1)),P(2|T(x,2))...P(M|T(x,M)) // Aggregate probabilities to compute anomaly score (Eq. 5) 4 P ARAMETERIZING THE SET OF TRANSFORMATIONS Geometric transformations have been used previously for unsupervised feature learning by Gidaris et al. (2018) as well as by GEOM (Golan & El-Yaniv, 2018) for classiﬁcation-based anomaly de- tection. This set of transformations is hand-crafted to work well with convolutional neural networks (CNNs) which greatly beneﬁt from preserving neighborhood between pixels. This is however not a requirement for fully-connected networks. 4Published as a conference paper at ICLR 2020 Anomaly detection often deals with non-image datasets e.g. tabular data. Tabular data is very commonly used on the internet e.g. for cyber security or online advertising. Such data consists of both discrete and continuous attributes with no particular neighborhoods or order. The data is one- dimensional and rotations do not naturally generalize to it. To allow transformation-based methods to work on general data types, we therefore need to extend the class of transformations. We propose to generalize the set of transformations to the class of afﬁne transformations (where we have a total of M transformations): T(x,m) =Wmx+ bm (6) It is easy to verify that all geometric transformations in Golan & El-Yaniv (2018) (rotation by a multiple of 90 degrees, ﬂips and translations) are a special case of this class ( xin this case is the set of image pixels written as a vector). The afﬁne class is however much more general than mere permutations, and allows for dimensionality reduction, non-distance preservation and random trans- formation by sampling W, bfrom a random distribution. Apart from reduced variance across different dataset types where no apriori knowledge on the cor- rect transformation classes exists, random transformations are important for avoiding adversarial examples. Assume an adversary wishes to change the label of a particular sample from anomalous to normal or vice versa. This is the same as requiring that ˜P(m′|T(x,m)) has low or high proba- bility for m′= m. If T is chosen deterministically, the adversary may create adversarial examples against the known class of transformations (even if the exact network parameters are unknown). Conversely, if T is unknown, the adversary must create adversarial examples that generalize across different transformations, which reduces the effectiveness of the attack. To summarize, generalizing the set of transformations to the afﬁne class allows us to: generalize to non-image data, use an unlimited number of transformations and choose transformations randomly which reduces variance and defends against adversarial examples. 5 E XPERIMENTS We perform experiments to validate the effectiveness of our distance-based approach and the per- formance of the general class of transformations we introduced for non-image data. 5.1 I MAGE EXPERIMENTS Cifar10: To evaluate the performance of our method, we perform experiments on the Cifar10 dataset. We use the same architecture and parameter choices of Golan & El-Yaniv (2018), with our distance-based approach. We use the standard protocol of training on all training images of a single digit and testing on all test images. Results are reported in terms of AUC. In our method, we used a margin of s = 0.1 (we also run GOAD with s = 1, shown in the appendix). Similarly to He et al. (2018), to stabilize training, we added a softmax + cross entropy loss, as well as L2 norm regularization for the extracted features f(x). We compare our method with the deep one- class method of Ruff et al. (2018) as well as Golan & El-Yaniv (2018) without and with Dirichlet weighting. We believe the correct comparison is without Dirichlet post-processing, as we also do not use it in our method. Our distance based approach outperforms the SOTA approach by Golan & El-Yaniv (2018), both with and without Dirichlet (which seems to improve performance on a few classes). This gives evidence for the importance of considering the generalization behavior outside the normal region used in training. Note that we used the same geometric transformations as Golan & El-Yaniv (2018). Random afﬁne matrices did not perform competitively as they are not pixel order preserving, this information is effectively used by CNNs and removing this information hurts performance. This is a special property of CNN architectures and image/time series data. As a rule of thumb, fully-connected networks are not pixel order preserving and can fully utilize random afﬁne matrices. FasionMNIST: In Tab. 2, we present a comparison between our method (GOAD) and the strongest baseline methods (Deep SVDD and GEOM) on the FashionMNIST dataset. We used exactly the same setting as Golan & El-Yaniv (2018). GOAD was run with s = 1. OCSVM and GEOM 5Published as a conference paper at ICLR 2020 Table 1: Anomaly Detection Accuracy on Cifar10 (ROC-AUC%) Class Method Deep-SVDD GEOM (no Dirichlet) GEOM (w. Dirichlet) Ours 0 61.7 ±1.3 76.0 ±0.8 74.7 ±0.4 77.2 ±0.6 1 65.9 ±0.7 83.0 ±1.6 95.7 ±0.0 96.7 ±0.2 2 50.8 ±0.3 79.5 ±0.7 78.1 ±0.4 83.3 ±1.4 3 59.1 ±0.4 71.4 ±0.9 72.4 ±0.5 77.7 ±0.7 4 60.9 ±0.3 83.5 ±1.0 87.8 ±0.2 87.8 ±0.7 5 65.7 ±0.8 84.0 ±0.3 87.8 ±0.1 87.8 ±0.6 6 67.7 ±0.8 78.4 ±0.7 83.4 ±0.5 90.0 ±0.6 7 67.3 ±0.3 89.3 ±0.5 95.5 ±0.1 96.1 ±0.3 8 75.9 ±0.4 88.6 ±0.6 93.3 ±0.0 93.8 ±0.9 9 73.1 ±0.4 82.4 ±0.7 91.3 ±0.1 92.0 ±0.6 Average 64.8 81.6 86.0 88.2 Table 2: Anomaly Detection Accuracy on FashionMNIST (ROC-AUC%) Class Method Deep-SVDD GEOM (no Dirichlet) GEOM (w. Dirichlet) Ours 0 98.2 77.8 ±5.9 99.4 ±0.0 94.1 ±0.9 1 90.3 79.1 ±16.3 97.6 ±0.1 98.5 ±0.3 2 90.7 80.8 ±6.9 91.1 ±0.2 90.8 ±0.4 3 94.2 79.2 ±9.1 89.9 ±0.4 91.6 ±0.9 4 89.4 77.8 ±3.3 92.1 ±0.0 91.4 ±0.3 5 91.8 58.0 ±29.4 93.4 ±0.9 94.8 ±0.5 6 83.4 73.6 ±8.7 83.3 ±0.1 83.4 ±0.4 7 98.8 87.4 ±11.4 98.9 ±0.1 97.9 ±0.4 8 91.9 84.6 ±5.6 90.8 ±0.1 98.9 ±0.1 9 99.0 99.5 ±0.0 99.2 ±0.0 99.2 ±0.3 Average 92.8 79.8 93.5 94.1 with Dirichlet were copied from their paper. We run their method without Dirichlet and presented it in the table (we veriﬁed the implementation by running their code with Dirichlet and replicated the numbers in the paper). It appears that GEOM is quite dependent on Dirichlet for this dataset, whereas we do not use it at all. GOAD outperforms all the baseline methods. Adversarial Robustness:Let us assume an attack model where the attacker knows the architecture and the normal training data and is trying to minimally modify anomalies to look normal. We exam- ine the merits of two settings i) the adversary knows the transformations used (non-random) ii) the adversary uses another set of transformations. To measure the beneﬁt of the randomized transfor- mations, we train three networks A, B, C. Networks A and B use exactly the same transformations but random parameter initialization prior to training. Network C is trained using other randomly se- lected transformations. The adversary creates adversarial examples using PGD (Madry et al., 2017) based on network A (making anomalies appear like normal data). On Cifar10, we randomly selected 8 transformations from the full set of 72 for Aand B, another randomly selected 8 transformations are used for C. We measure the increase of false classiﬁcation rate on the adversarial examples us- ing the three networks. The average increase in performance of classifying transformation correctly on anomalies (causing lower anomaly scores) on the original network A was 12.8%, the transfer performance for B causes an increase by 5.0% on network Bwhich shared the same set of transfor- mation, and 3% on network C that used other rotations. This shows the beneﬁts of using random transformations. 6Published as a conference paper at ICLR 2020 Table 3: Anomaly Detection Accuracy (%) Method Dataset Arrhythmia Thyroid KDD KDDRev F1 Score σ F 1 Score σ F 1 Score σ F 1 Score σ OC-SVM 45.8 38.9 79.5 83.2 E2E-AE 45.9 11.8 0.3 74.5 LOF 50.0 0.0 52.7 0.0 83.8 5.2 81.6 3.6 DAGMM 49.8 47.8 93.7 93.8 FB-AE 51.5 1.6 75.0 0.8 92.7 0.3 95.9 0.4 GOAD(Ours) 52.0 2.3 74.5 1.1 98.4 0.2 98.9 0.3 5.2 T ABULAR DATA EXPERIMENTS Datasets: We evaluate on small-scale medical datasets Arrhythmia, Thyroid as well as large-scale cyber intrusion detection datasets KDD and KDDRev. Our conﬁguration follows that of Zong et al. (2018). Categorical attributes are encoded as one-hot vectors. For completeness the datasets are described in the appendix A.2. We train all compared methods on 50% of the normal data. The methods are evaluated on 50% of the normal data as well as all the anomalies. Baseline methods: The baseline methods evaluated are: One-Class SVM (OC-SVM, Scholkopf et al. (2000)), End-to-End Autoencoder (E2E-AE), Local Outlier Factor (LOF, Breunig et al. (2000)). We also evaluated deep distributional method DAGMM (Zong et al., 2018), choosing their strongest variant. To compare against ensemble methods e.g. Chen et al. (2017), we implemented the Fea- ture Bagging Autoencoder (FB-AE) with autoencoders as the base classiﬁer, feature bagging as the source of randomization, and average reconstruction error as the anomaly score. OC-SVM, E2E-AE and DAGMM results are directly taken from those reported by Zong et al. (2018). LOF and FB-AE were computed by us. Implementation of GOAD: We randomly sampled transformation matrices using the normal distri- bution for each element. Each matrix has dimensionality L×r, where Lis the data dimension and ris a reduced dimension. For Arryhthmia and Thyroid we used r = 32, for KDD and KDDrev we used r = 128and r = 64respectively, the latter due to high memory requirements. We used 256 tasks for all datasets apart from KDD ( 64) due to high memory requirements. We set the bias term to 0. For C we used fully-connected hidden layers and leaky-ReLU activations (8 hidden nodes for the small datasets, 128 and 32 for KDDRev and KDD). We optimized using ADAM with a learning rate of 0.001. Similarly to He et al. (2018), to stabilize the triplet center loss training, we added a softmax + cross entropy loss. We repeated the large-scale experiments 5 times, and the small scale GOAD experiments500 times (due to the high variance). We report the mean and standard deviation (σ). Following the protocol in Zong et al. (2018), the decision threshold value is chosen to result in the correct number of anomalies e.g. if the test set contains Na anomalies, the threshold is selected so that the highest Na scoring examples are classiﬁed as anomalies. True positives and negatives are evaluated in the usual way. Some experiments copied from other papers did not measure standard variation and we kept the relevant cell blank. Results Arrhythmia: The Arrhythmia dataset was the smallest examined. A quantitative comparison on this dataset can be seen in Tab. 3. OC-SVM and DAGMM performed reasonably well. Our method is comparable to FB-AE. A linear classiﬁer C performed better than deeper networks (which suffered from overﬁtting). Early stopping after a single epoch generated the best results. Thyroid: Thyroid is a small dataset, with a low anomaly to normal ratio and low feature dimension- ality. A quantitative comparison on this dataset can be seen in Tab. 3. Most baselines performed about equally well, probably due to the low dimensionality. On this dataset, we also found that early stopping after a single epoch gave the best results. The best results on this dataset, were obtained with a linear classiﬁer. Our method is comparable to FB-AE and beat all other baselines by a wide margin. 7Published as a conference paper at ICLR 2020 Figure 1: Left: Classiﬁcation error for our method and DAGMM as a function of percentage of the anomalous examples in the training set (on the KDDCUP99 dataset). Our method consistently outperforms the baseline. Right: Classiﬁcation error as a function of the number of transforma- tions (on the KDDRev dataset). The error and instability decrease as a function of the number of transformations. For both, lower is better. KDDCUP99: The UCI KDD10% dataset is the largest dataset examined. A quantitative comparison on this dataset can be seen in Tab. 3. The strongest baselines are FB-AE and DAGMM. Our method signiﬁcantly outperformed all baselines. We found that large datasets have different dynamics from very small datasets. On this dataset, deep networks performed the best. We also, did not need early stopping. The results are reported after 25 epochs. KDD-Rev: The KDD-Rev dataset is a large dataset, but smaller than KDDCUP99 dataset. A quanti- tative comparison on this dataset can be seen in Tab. 3. Similarly to KDDCUP99, the best baselines are FB-AE and DAGMM, where FB-AE signiﬁcantly outperforms DAGMM. Our method signiﬁ- cantly outperformed all baselines. Due to the size of the dataset, we did not need early stopping. The results are reported after 25 epochs. Adversarial Robustness: Due to the large number of transformations and relatively small networks, adversarial examples are less of a problem for tabular data. PGD generally failed to obtain adversar- ial examples on these datasets. On KDD, transformation classiﬁcation accuracy on anomalies was increased by 3.7% for the network the adversarial examples were trained on, 1.3% when transfer- ring to the network with the same transformation and only0.2% on the network with other randomly selected transformations. This again shows increased adversarial robustness due to random transfor- mations. Further Analysis Contaminated Data: This paper deals with the semi-supervised scenario i.e. when the training dataset contains only normal data. In some scenarios, such data might not be available but instead we might have a training dataset that contains a small percentage of anomalies. To evaluate the robustness of our method to this unsupervised scenario, we analysed the KDDCUP99 dataset, when X% of the training data is anomalous. To prepare the data, we used the same normal training data as before and added further anomalous examples. The test data consists of the same proportions as before. The results are shown in Fig. 1. Our method signiﬁcantly outperforms DAGMM for all impurity values, and degrades more graceful than the baseline. This attests to the effectiveness of our approach. Results for the other datasets are presented in Fig. 3, showing similar robustness to contamination. Number of Tasks: One of the advantages of GOAD, is the ability to generate any number of tasks. We present the anomaly detection performance on the KDD-Rev dataset with different numbers of tasks in Fig. 1. We note that a small number of tasks (less than 16) leads to poor results. From 16 tasks, the accuracy remains stable. We found that on the smaller datasets (Thyroid, Arrhythmia) using a larger number of transformations continued to reduce F1 score variance between differently initialized runs (Fig. 2). 8Published as a conference paper at ICLR 2020 6 D ISCUSSION Openset vs. Softmax: The openset-based classiﬁcation presented by GOAD resulted in performance improvement over the closed-set softmax approach on Cifar10 and FasionMNIST. In our experi- ments, it has also improved performance in KDDRev. Arrhythmia and Thyroid were comparable. As a negative result, performance of softmax was better on KDD (F1 = 0.99). Choosing the margin parameter s: GOAD is not particularly sensitive to the choice of margin parameter s, although choosing sthat is too small might cause some instability. We used a ﬁxed value of s= 1in our experiments, and recommend this value as a starting point. Other transformations: GOAD can also work with other types of transformations such as rotations or permutations for tabular data. In our experiments, we observed that these transformation types perform comparably but a little worse than afﬁne transformations. Unsupervised training: Although most of our results are semi-supervised i.e. assume that no anoma- lies exist in the training set, we presented results showing that our method is more robust than strong baselines to a small percentage of anomalies in the training set. We further presented results in other datasets showing that our method degrades gracefully with a small amount of contamination. Our method might therefore be considered in the unsupervised settings. Deep vs. shallow classiﬁers: Our experiments show that for large datasets deep networks are ben- eﬁcial (particularly for the full KDDCUP99), but are not needed for smaller datasets (indicating that deep learning has not beneﬁted the smaller datasets). For performance critical operations, our approach may be used in a linear setting. This may also aid future theoretical analysis of our method. 7 C ONCLUSION In this paper, we presented a method for detecting anomalies for general data. This was achieved by training a classiﬁer on a set of random auxiliary tasks. Our method does not require knowledge of the data domain, and we are able to generate an arbitrary number of random tasks. Our method signiﬁcantly improve over the state-of-the-art. REFERENCES Arthur Asuncion and David Newman. Uci machine learning repository, 2007. Abhijit Bendale and Terrance E Boult. Towards open set deep networks. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 1563–1572, 2016. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J¨org Sander. Lof: identifying density- based local outliers. In ACM sigmod record, volume 29, pp. 93–104. ACM, 2000. Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? JACM, 2011. Jinghui Chen, Saket Sathe, Charu Aggarwal, and Deepak Turaga. Outlier detection with autoencoder ensembles. In ICDM, 2017. Lucas Deecke, Robert Vandermeulen, Lukas Ruff, Stephan Mandt, and Marius Kloft. Anomaly detection with generative adversarial networks. In ICLR, 2018. Eleazar Eskin, Andrew Arnold, Michael Prerau, Leonid Portnoy, and Sal Stolfo. A geometric frame- work for unsupervised anomaly detection. In Applications of data mining in computer security , pp. 77–101. Springer, 2002. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. ICLR, 2018. Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. In NeurIPS, 2018. 9Published as a conference paper at ICLR 2020 John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm.Journal of the Royal Statistical Society. Series C (Applied Statistics), 1979. Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai. Triplet-center loss for multi- view 3d object retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1945–1954, 2018. Dan Hendrycks, Mantas Mazeika, and Thomas G Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018. Ian Jolliffe. Principal component analysis. Springer, 2011. Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for auto- matic colorization. In ECCV, 2016. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. ICLR, 2016. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe- matical statistics, 1962. Shebuti Rayana. ODDS library http://odds.cs.stonybrook.edu, 2016. Lukas Ruff, Nico Gornitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert Vandermeulen, Alexan- der Binder, Emmanuel M¨uller, and Marius Kloft. Deep one-class classiﬁcation. In ICML, 2018. Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimen- sionality reduction. In MLSD. ACM, 2014. Thomas Schlegl, Philipp Seeb ¨ock, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International Conference on Information Processing in Medical Imaging, 2017. Bernhard Scholkopf, Robert C Williamson, Alex J Smola, John Shawe-Taylor, and John C Platt. Support vector method for novelty detection. In NIPS, 2000. Yan Xia, Xudong Cao, Fang Wen, Gang Hua, and Jian Sun. Learning discriminative reconstructions for unsupervised outlier removal. In ECCV, 2015. Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces: Simultaneous deep learning and clustering. In ICML, 2017. Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. ICLR, 2018. A A PPENDIX A.1 I MAGE EXPERIMENTS Sensitive to margins: We run Cifar10 experiments with s = 0.1 and s = 1 and presented the results in Fig. 4. The results were not affected much by the margin parameter. This is in-line with the rest of our empirical observations that GOAD is not very sensitive to the margin parameter. 10Published as a conference paper at ICLR 2020 Table 4: Anomaly Detection Accuracy on Cifar10 (%) Class Method GEOM (w. Dirichlet) GOAD( s= 0.1) GOAD( 1.0) 0 74.7 ±0.4 77.2 ±0.6 77.9 ±0.7 1 95.7 ±0.0 96.7 ±0.2 96.4 ±0.9 2 78.1 ±0.4 83.3 ±1.4 81.8 ±0.8 3 72.4 ±0.5 77.7 ±0.7 77.0 ±0.7 4 87.8 ±0.2 87.8 ±0.7 87.7 ±0.5 5 87.8 ±0.1 87.8 ±0.6 87.8 ±0.7 6 83.4 ±0.5 90.0 ±0.6 90.9 ±0.5 7 95.5 ±0.1 96.1 ±0.3 96.1 ±0.2 8 93.3 ±0.0 93.8 ±0.9 93.3 ±0.1 9 91.3 ±0.1 92.0 ±0.6 92.4 ±0.3 Average 86.0 88.2 88.1 A.2 T ABULAR DATASETS Following the evaluation protocol of Zong et al. (2018), 4 datasets are used in this comparison: Arrhythmia: A cardiology dataset from the UCI repository (Asuncion & Newman, 2007) contain- ing attributes related to the diagnosis of cardiac arrhythmia in patients. The datasets consists of 16 classes: class 1 are normal patients, 2-15 contain different arrhythmia conditions, and class 16 con- tains undiagnosed cases. Following the protocol established by ODDS (Rayana, 2016), the smallest classes: 3,4,5,7,8,9,14,15 are taken to be anomalous and the rest normal. Also following ODDS, the categorical attributes are dropped, the ﬁnal attributes total 274. Thyroid: A medical dataset from the UCI repository (Asuncion & Newman, 2007), containing at- tributes related to whether a patient is hyperthyroid. Following ODDS (Rayana, 2016), from the 3 classes of the dataset, we designate hyperfunction as the anomalous class and the rest as normal. Also following ODDS only the 6 continuous attributes are used. KDD: The KDD Intrusion Detection dataset was created by an extensive simulation of a US Air Force LAN network. The dataset consists of the normal and 4 simulated attack types: denial of service, unauthorized access from a remote machine, unauthorized access from local superuser and probing. The dataset consists of around 5 million TCP connection records. Following the evaluation protocol in Zong et al. (2018), we use the UCI KDD 10% dataset, which is a subsampled version of the original dataset. The dataset contains 41 different attributes. 34 are continuous and 7 are categorical. Following Zong et al. (2018), we encode the categorical attributes using1-hot encoding. Following Zong et al. (2018), we evaluate two different settings for the KDD dataset: KDDCUP99: In this conﬁguration we use the entire UCI 10% dataset. As the non-attack class consists of only 20% of the dataset, it is treated as the anomaly in this case, while attacks are treated as normal. KDDCUP99-Rev: To better correspond to the actual use-case, in which the non-attack scenario is normal and attacks are anomalous, Zong et al. (2018) also evaluate on the reverse conﬁguration, in which the attack data is sub-sampled to consist of 25% of the number of non-attack samples. The attack data is in this case designated as anomalous (the reverse of the KDDCUP99 dataset). In all the above datasets, the methods are trained on 50% of the normal data. The methods are evaluated on 50% of the normal data as well as all the anomalies. A.3 N UMBER OF TASKS We provide plots of the number of auxiliary tasks vs. the anomaly detection accuracy (measured by F1) for all datasets. The results are presented in Fig. 2. Performance increases rapidly up to a certain number of tasks (around 16). Afterwards more tasks reduce the variance of F1 scores between runs. 11Published as a conference paper at ICLR 2020 a) ‘ b) c) ‘ d) Figure 2: Plots of the number of auxiliary tasks vs. the anomaly detection accuracy (measured by F1) a) Arrhythmia b) Thyroid c) KDDRev d) KDDCup99 Accuracy often increases with the number of tasks, although the rate diminishes with the number of tasks. Figure 3: Plots of the degree of contamination vs. the anomaly detection accuracy (measured byF1) (left) KDDRev (center) KDDCup99 (right) Arrhythmia. GOAD is generally robust to the degree of contamination. A.4 C ONTAMINATION EXPERIMENTS We conduct contamination experiments for 3 datasets. Thyroid was omitted due to not having a sufﬁcient number of anomalies. The protocol is different than that of KDDRev as we do not have unused anomalies for contamination. Instead, we split the anomalies into train and test. Train anomalies are used for contamination, test anomalies are used for evaluation. As DAGMM did not present results for the other datasets, we only present GOAD. GOAD was reasonably robust to contamination on KDD, KDDRev and Arrhythmia. The results are presented in Fig. 3 12",
      "references": [
        "Towards open set deep networks.",
        "Lof: identifying density- based local outliers.",
        "Robust principal component analysis?",
        "Outlier detection with autoencoder ensembles.",
        "Anomaly detection with generative adversarial networks.",
        "A geometric framework for unsupervised anomaly detection.",
        "Unsupervised representation learning by predicting image rotations.",
        "Deep anomaly detection using geometric transformations.",
        "Algorithm as 136: A k-means clustering algorithm.",
        "Triplet-center loss for multi- view 3d object retrieval.",
        "Deep anomaly detection with outlier exposure.",
        "Principal component analysis.",
        "Learning representations for automatic colorization.",
        "Towards deep learning models resistant to adversarial attacks.",
        "Deep multi-scale video prediction beyond mean square error.",
        "Unsupervised learning of visual representations by solving jigsaw puzzles.",
        "On estimation of a probability density function and mode.",
        "Deep one-class classiﬁcation.",
        "Anomaly detection using autoencoders with nonlinear dimensionality reduction.",
        "Unsupervised anomaly detection with generative adversarial networks to guide marker discovery.",
        "Support vector method for novelty detection.",
        "Learning discriminative reconstructions for unsupervised outlier removal.",
        "Towards k-means-friendly spaces: Simultaneous deep learning and clustering.",
        "Colorful image colorization.",
        "Deep autoencoding gaussian mixture model for unsupervised anomaly detection."
      ],
      "meta_data": {
        "arxiv_id": "2005.02359v1",
        "authors": [
          "Liron Bergman",
          "Yedid Hoshen"
        ],
        "published_date": "2020-05-05T17:44:40Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces GOAD, a novel distance-based, open-set anomaly detection method that unifies classification-based approaches using normal training data only. It extends transformation-based techniques to general data types, including non-image data, by leveraging random affine transformations.",
        "methodology": "The approach transforms input data using multiple affine transformations to create auxiliary tasks. A deep feature extractor is trained using a center-triplet loss to learn a feature space where intra-class distances (within transformed normal data) are minimized and inter-class distances are maximized. An anomaly score is generated by aggregating the probabilities of each transformed sample belonging to its corresponding transformation-induced cluster, regularized with a constant to handle uncertainty.",
        "experimental_setup": "The method is validated on both image datasets (CIFAR10, FashionMNIST) and tabular datasets (Arrhythmia, Thyroid, KDD, KDDRev). Experiments compare GOAD against baselines like Deep-SVDD, GEOM (with and without Dirichlet weighting), DAGMM, OC-SVM, and autoencoder-based methods. Evaluation metrics include ROC-AUC and F1 scores. Additionally, robustness to adversarial attacks and training data contamination is assessed.",
        "limitations": "The approach can be sensitive to the choice and quality of transformations; for example, random affine transformations may not perform as well on CNNs for image data compared to carefully designed geometric transformations. It also requires hyperparameter tuning (e.g., the margin parameter) and may show different behavior across dataset sizes and network architectures. The open-set formulation, while enhancing generalization, may still be vulnerable if transformation parameters become known to adversaries.",
        "future_research_directions": "Future work could explore alternative transformation classes, including non-affine methods or learned transformations, to further enhance robustness and adaptability across different data types. Extensions to fully unsupervised scenarios and deeper theoretical analysis of the open-set distance-based framework are also promising directions, as is improving adversarial robustness and optimizing performance for both deep and shallow network architectures.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Explanation-based Data Augmentation for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Novel-View Acoustic Synthesis",
      "full_text": "Novel-View Acoustic Synthesis Changan Chen1,3 Alexander Richard2 Roman Shapovalov3 Vamsi Krishna Ithapu2 Natalia Neverova3 Kristen Grauman1,3 Andrea Vedaldi3 1University of Texas at Austin 2Reality Labs Research at Meta 3FAIR, Meta AI Abstract We introduce the novel-view acoustic synthesis (NVAS) task: given the sight and sound observed at a source view- point, can we synthesize the sound of that scene from an unseen target viewpoint? We propose a neural rendering approach: Visually-Guided Acoustic Synthesis (ViGAS) net- work that learns to synthesize the sound of an arbitrary point in space by analyzing the input audio-visual cues. To benchmark this task, we collect two first-of-their-kind large-scale multi-view audio-visual datasets, one synthetic and one real. We show that our model successfully reasons about the spatial cues and synthesizes faithful audio on both datasets. To our knowledge, this work represents the very first formulation, dataset, and approach to solve the novel- view acoustic synthesis task, which has exciting potential applications ranging from AR/VR to art and design. Un- locked by this work, we believe that the future of novel-view synthesis is in multi-modal learning from videos. 1. Introduction Replaying a video recording from a new viewpoint1 has many applications in cinematography, video enhancement, and virtual reality. For example, it can be used to edit a video, simulate a virtual camera, or, given a video of a per- sonal memory, even enable users to experience a treasured moment again—not just on a 2D screen, but in 3D in a vir- tual or augmented reality, thus ‘reliving’ the moment. While the applications are exciting, there are still many unsolved technical challenges. Recent advances in 3D re- construction and novel-view synthesis (NVS) address the problem of synthesizing new images of a given scene [32, 34, 44]. However, thus far, the view synthesis problem is concerned with creating visuals alone; the output is silent or at best naively adopts the sounds of the original video (from the “wrong” viewpoint). Without sound, the emotional and cognitive significance of the replay is severely diminished. In this work, we address this gap and introduce the new task of novel-view acoustic synthesis (NV AS). The goal of 1We use “viewpoint” to mean a camera or microphone pose. <latexit sha1_base64=\"DB6IHqvM66/U2Zrzj2obSpIFx38=\">AAACHHicbVDLSgMxFM34rOOr6tJNsAhdlZkKKq4KblxWsA9oS8lkbtvQTDIkmUoZ+iFu/BU3LhRx40Lwb0zbEbT1QOBwzrlJ7glizrTxvC9nZXVtfWMzt+Vu7+zu7ecPDutaJopCjUouVTMgGjgTUDPMcGjGCkgUcGgEw+up3xiB0kyKOzOOoRORvmA9RomxUjd/1g6gz0RKQRhQE5ckIZNXeH4/HjG4jyUTxm2DCH9C3XzBK3kz4GXiZ6SAMlS7+Y92KGkS2XHKidYt34tNJyXKMMph4rYTDTGhQ9KHlqWCRKA76Wy5CT61Soh7UtkjDJ6pvydSEmk9jgKbjIgZ6EVvKv7ntRLTu+ykTMSJAUHnD/USjo3E06ZwyBRQw8eWEKqY/SumA6IItR1o15bgL668TOrlkn9eKt+WC5ViVkcOHaMTVEQ+ukAVdIOqqIYoekBP6AW9Oo/Os/PmvM+jK042c4T+wPn8Bh/gopA=</latexit> audio: source viewpoint <latexit sha1_base64=\"XZLNGEBTmZWFfCrxLoBgXuXESXQ=\">AAACHHicbVDLSgMxFM34rOOr6tJNsAhdlZkKKq4KblxWsCq0pWQytzWYSYbkTqUM/RA3/oobF4q4cSH4N6YPQasHAodzzuXmniiVwmIQfHpz8wuLS8uFFX91bX1js7i1fWl1Zjg0uJbaXEfMghQKGihQwnVqgCWRhKvo9nTkX/XBWKHVBQ5SaCesp0RXcIZO6hQPWhH0hMo5KAQz9FkWC31CkZkeIO0LuEu1UOi3QMXfoU6xFFSCMehfEk5JiUxR7xTfW7HmWeLGuWTWNsMgxXbODAouYei3Mgsp47esB01HFUvAtvPxcUO675SYdrVxTyEdqz8ncpZYO0gil0wY3thZbyT+5zUz7B63c6HSDEHxyaJuJilqOmqKxsIARzlwhHEj3F8pv2GGcdeB9V0J4ezJf8lltRIeVqrn1VKtPK2jQHbJHimTkByRGjkjddIgnNyTR/JMXrwH78l79d4m0TlvOrNDfsH7+AIPgqKG</latexit> audio: target viewpoint <latexit sha1_base64=\"FqwoHxlVQf+RHmpskhy/1w+4r/I=\">AAACD3icbVDLSsNAFJ3UV42vqEs3waJ0VZIu1GXBjcsK9gFNKJPJTTt0MgkzE6GE/oEbf8WNC0XcunXn3zhpI2jrgYHDOfdw554gZVQqx/kyKmvrG5tb1W1zZ3dv/8A6POrKJBMEOiRhiegHWAKjHDqKKgb9VACOAwa9YHJd+L17EJIm/E5NU/BjPOI0ogQrLQ2tcy+AEeU5Aa5AzEwdDikpPNMDHv7oQ6vmNJw57FXilqSGSrSH1qcXJiSLdZwwLOXAdVLl51goShjMTC+TkGIywSMYaMpxDNLP5/fM7DOthHaUCP24sufq70SOYymncaAnY6zGctkrxP+8QaaiKz+nPM0UcLJYFGXMVoldlGOHVABRbKoJJoLqv9pkjAUmugNp6hLc5ZNXSbfZcC8azdtmrVUv66iiE3SK6shFl6iFblAbdRBBD+gJvaBX49F4Nt6M98VoxSgzx+gPjI9vFHedPQ==</latexit> prediction <latexit sha1_base64=\"wgq9kiYeoddM9HQ0RQef4oYy1Z8=\">AAACCnicbVDLSsNAFJ3UV42vqEs30SJ0VZIu1GXBjcsK9gFNKJPJTTt0MgkzE6GErt34K25cKOLWL3Dn3zhNI2jrgYHDOfdw554gZVQqx/kyKmvrG5tb1W1zZ3dv/8A6POrKJBMEOiRhiegHWAKjHDqKKgb9VACOAwa9YHI993v3ICRN+J2apuDHeMRpRAlWWhpap14AI8pzAlyBmJmUp5kyPeDhjzS0ak7DKWCvErckNVSiPbQ+vTAhWazjhGEpB66TKj/HQlHCYGZ6mYQUkwkewUBTjmOQfl6cMrPPtRLaUSL048ou1N+JHMdSTuNAT8ZYjeWyNxf/8waZiq78vDgPOFksijJmq8Se92KHVABRbKoJJoLqv9pkjAUmugNp6hLc5ZNXSbfZcC8azdtmrVUv66iiE3SG6shFl6iFblAbdRBBD+gJvaBX49F4Nt6M98VoxSgzx+gPjI9vCkWbCg==</latexit> input <latexit sha1_base64=\"gmuaOXNBHdGGiasD0JLjq1fYTJ0=\">AAACMXicbVBNSwMxFMz6WdevqkcvwSLUg2W3B/VY8dKTVLS10C0lm762wWyyJNlKWfqXvPhPxEsPinj1T5itFbQ6EJjMvOHxJow508bzJs7C4tLyympuzV3f2Nzazu/sNrRMFIU6lVyqZkg0cCagbpjh0IwVkCjkcBveXWT+7RCUZlLcmFEM7Yj0BesxSoyVOvlqEEKfiZSCMKDGrpBD4MdDBvdBgAmViTaMYsv1SJgBaKazT/GycX595AYgut/JTr7glbwp8F/iz0gBzVDr5J+CrqRJZOOUE61bvhebdkqUXchh7AaJhpjQO9KHlqWCRKDb6fTiMT60Shf3pLJPGDxVfyZSEmk9ikI7GREz0PNeJv7ntRLTO2unTMSJAUG/FvUSjo3EWX24yxRQw0eWEKpYVg4dEEWo7UC7tgR//uS/pFEu+Sel8lW5UCnO6sihfXSAishHp6iCqqiG6oiiB/SMXtCr8+hMnDfn/Wt0wZll9tAvOB+fFfSqAg==</latexit> novel-view acoustic synthesis (NVAS) <latexit sha1_base64=\"M/oXgzzvTTQZBpW0tUZ/skQulpQ=\">AAACHXicbVDLSgMxFM34rONr1KWbYBG6KjNFVFwV3LisYB/QlpJJb9vQTDIkmUoZ+iNu/BU3LhRx4Ub8G9N2BG09EDiccy4394QxZ9r4/pezsrq2vrGZ23K3d3b39r2Dw5qWiaJQpZJL1QiJBs4EVA0zHBqxAhKFHOrh8Hrq10egNJPizoxjaEekL1iPUWKs1PHOWiH0mUgpCANq4o6YTgi/wvMFeMTgPpZMGLcFovuT6nh5v+jPgJdJkJE8ylDpeB+trqRJZMcpJ1o3Az827ZQowyiHidtKNMSEDkkfmpYKEoFup7PrJvjUKl3ck8o+YfBM/T2RkkjrcRTaZETMQC96U/E/r5mY3mU7ZSJODAg6X9RLODYST6vCXaaAGj62hFDF7F8xHRBFqO1Au7aEYPHkZVIrFYPzYum2lC8Xsjpy6BidoAIK0AUqoxtUQVVE0QN6Qi/o1Xl0np03530eXXGymSP0B87nNyKCoxw=</latexit> visual: source viewpoint <latexit sha1_base64=\"9UpGCzo0YEBsWgEaPdo5vXYYl9I=\">AAACI3icbVDLSsNAFJ34Nr6qLt0MFqGrknSh0lXBjUsFawtNKJPJbR2cTMLMTaWE/osbf8WNC0XcuPBfnD4EbT0wcDjnXO7cE2VSGPS8T2dpeWV1bX1j093a3tndK+0f3Jo01xyaPJWpbkfMgBQKmihQQjvTwJJIQiu6vxj7rQFoI1J1g8MMwoT1legJztBK3VI9iKAvVMFBIeiROxAmZ7IeBC4y3QekQUCtCA9ZKhS6Aaj4J9stlb2qNwFdJP6MlMkMV93SexCnPE/sOJfMmI7vZRgWTKPgEkZukBvIGL9nfehYqlgCJiwmN47oiVVi2ku1fQrpRP09UbDEmGES2WTC8M7Me2PxP6+TY+88LITKcgTFp4t6uaSY0nFhNBYaOMqhJYxrYf9K+R3TjNsOjGtL8OdPXiS3tap/Wq1d18qNyqyODXJEjkmF+OSMNMgluSJNwskjeSav5M15cl6cd+djGl1yZjOH5A+cr2+pi6TS</latexit> visual: target viewpoint <latexit sha1_base64=\"VvksTeSlNE+VBbTxSJ14XZHmE4M=\">AAACHHicbVC7SgNBFJ31GddX1NJmMAixCbsR1DJgYxnBPCAbwuzs3WTI7MwyMyuEJR9i46/YWChiYyH4N04egiae6nDOPdx7T5hypo3nfTkrq2vrG5uFLXd7Z3dvv3hw2NQyUxQaVHKp2iHRwJmAhmGGQztVQJKQQyscXk/81j0ozaS4M6MUugnpCxYzSoyVesXzIIQ+EzkFYUCN3bIUfISDwI2lwgpiUCAonLkBiOhnqFcseRVvCrxM/DkpoTnqveJHEEmaJTZOOdG643up6eZEGUY5jN0g05ASOiR96FgqSAK6m0+fG+NTq0R4ck4shcFT9XciJ4nWoyS0kwkxA73oTcT/vE5m4qtuzkSaGfvjbFGccWwknjSFI6aAGttGxAhVzN6K6YAoQm0H2rUl+IsvL5NmteJfVKq31VKtPK+jgI7RCSojH12iGrpBddRAFD2gJ/SCXp1H59l5c95noyvOPHOE/sD5/AaTR6GZ</latexit> (only for reference) <latexit sha1_base64=\"6lJBr/qgL1rvMh3XYJx3cXo7qa0=\">AAACHXicbVDLSgMxFM3UVx1foy7dBIvQVZkpoi4LblxWsA/olJJJb9vQTDIkmUoZ+iNu/BU3LhRx4Ub8G9OHoK0HAodzziX3nijhTBvf/3Jya+sbm1v5bXdnd2//wDs8qmuZKgo1KrlUzYho4ExAzTDDoZkoIHHEoRENr6d+YwRKMynuzDiBdkz6gvUYJcZKHe88jKDPREZBGFAT1xDVB4NHDO4TyYTBYegmUoMbguj+pDpewS/5M+BVEixIAS1Q7XgfYVfSNLbjlBOtW4GfmHZGlGGUw8QNUw0JoUPSh5algsSg29nsugk+s0oX96Syzy40U39PZCTWehxHNhkTM9DL3lT8z2ulpnfVzphIUgOCzj/qpRwbiadV4S5TQA0fW0KoYnZXTAdEEWo70K4tIVg+eZXUy6XgolS+LRcqxUUdeXSCTlERBegSVdANqqIaougBPaEX9Oo8Os/Om/M+j+acxcwx+gPn8xuOFaK9</latexit> target viewpoint pose Figure 1. Novel-view acoustic synthesis task.Given audio-visual observations from one viewpoint and the relative target viewpoint pose, render the sound received at the target viewpoint. Note that the target is expressed as the desired pose of the microphones; the image at that pose (right) is neither observed nor synthesized. this task is to synthesize the sound in a scene from a new acoustic viewpoint, given only the visual and acoustic input from another source viewpoint in the same scene (Fig. 1). NV AS is very different from the existing NVS task, where the goal is to reconstruct images instead of sounds, and these differences present new challenges. First, the 3D geometry of most real-life scenes changes in a limited man- ner during the recording. On the contrary, sound changes substantially over time, so the reconstruction target is highly dynamic. Secondly, visual and audio sensors are very dif- ferent. A camera matrix captures the light in a highly- directional manner, and a single image comprises a large 2D array of pixels. In contrast, sounds are recorded with one or two microphones which are at best weakly-directional, pro- viding only a coarse sampling of the sound field. Thirdly, the frequency of light waves is much higher than that of sound waves; the length of audio waves is thus larger to the point of being comparable to the size of geometric features of the scene, meaning that effects such as diffraction are often dominant, and spatial resolution is low. As a result, techniques that require spatial precision, such as triangula- tion and segmentation, are not applicable to audio. Lastly, sounds mix together, making it difficult to segment them, arXiv:2301.08730v3  [cs.CV]  24 Oct 2023and they are affected by environmental effects such as re- verberation that are distributed and largely unobservable. While the NVS and NV AS tasks are indeed very dif- ferent, we hypothesize that NV AS is an inherently multi- modal task. In fact, vision can play an important role in achieving accurate sound synthesis. First, establishing cor- respondences between sounds and their sources as they ap- pear in images can provide essential cues for resynthesiz- ing the sounds realistically. For instance, human speech is highly directional and sounds very differently if one faces the speaker or their back, which can only be inferred from visual cues. In addition, the environment acoustics also af- fect the sound one hears as a function of the scene geom- etry, materials, and emitter/receiver locations. The same source sounds very differently if it is located in the center of a room, at the corner, or in a corridor, for example. In short, vision provides cues about space and geometry that affect sound, and are difficult to estimate from the sound alone. In order to validate our hypothesis, we propose a novel visually-guided acoustic synthesis networkthat analyzes au- dio and visual features and synthesizes the audio at a target location. More specifically, the network first takes as in- put the image observed at the source viewpoint in order to infer global acoustic and geometric properties of the envi- ronment along with the bounding box of the active speaker. The network then reasons how the speaker and scene geom- etry change in 3D based on the relative target pose with a fusion network. We inject the fused features into audio with a gated multi-modal fusion network and model the acoustic changes between viewpoints with a time-domain model. In order to conduct our experiments on the new NV AS task, we require suitable training and benchmarking data, of which currently there is none available. To address that, we contribute two new datasets: one real (Replay-NV AS) and one synthetic (SoundSpaces-NV AS). The key feature of these datasets is to record the sight and sound of dif- ferent scenes from multiple cameras/viewpoints. Replay- NV AS contains video recordings of groups of people per- forming social activities (e.g., chatting, watching TV , doing yoga, playing instruments) from 8 surrounding viewpoints simultaneously. It contains 37 hours of highly realistic ev- eryday conversation and social interactions in one home- like environment. To our knowledge, Replay-NV AS repre- sents the first large-scale real-world dataset enabling NV AS. This dataset would also greatly benefit many other exist- ing tasks including NVS, active speaker localization, etc. For SoundSpaces-NV AS, we render 1.3K hours of audio- visual data based on the SoundSpaces [7] platform. Using this simulator, one can easily change the scene geometry and the positions of speakers, cameras, and microphones. This data serves as a powerful test bed with clean ground truth for a large collection of home environments, offer- ing a good complement to Replay-NV AS. For both datasets, we capture binaural audio, which is what humans perceive with two ears. Together the datasets contain 1,337 hours of audio-visual capture, with 1,032 speakers across 121 3D scenes. Datasets are publicly available for future research.2 We show that our model outperforms traditional signal processing approaches as well as learning-based baselines, often by a substantial margin, in a quantitative evaluation and a human study. We show qualitative examples where the model predicts acoustic changes according to the view- point changes, e.g., left channel becomes louder when the viewpoint changes from left to right. In a nutshell, we present the first work that deals with novel-view acoustic synthesis, and contribute two large-scale datasets along with a novel neural rendering approach for solving the task. 2. Related Work Novel-view synthesis (NVS). Kickstarted by advances in neural rendering [34, 51], many recent works consider vari- ants of the NVS problem. Most approaches assume dozens of calibrated images for reconstructing a single static scene. Closer to monocular video NVS, authors have considered reducing the number of input views [20, 25, 38, 46, 59] and modelling dynamic scenes [27,28,42, 43,53,55]. However, none of these works tackle audio. Acoustic matching and spatialization. NV AS requires accounting for (1) the environmental acoustics and (2) the geometric configuration of the target microphone(s) (e.g., monaural vs binaural). Modelling environmental acous- tics has been addressed extensively by the audio commu- nity [4, 26]. Room impulse response (RIR) functions char- acterize the environment acoustics as a transfer function be- tween the emitter and receiver, accounting for the scene geometry, materials, and emitter/receiver locations. Esti- mating the direct-to-reverberant ratio and the reverberation time, is sufficient to synthesize simple RIRs that match au- dio in a plausible manner [12,16,22,30,37,58]. These meth- ods do not synthesize for a target viewpoint, rather they resynthesize to match an audio sample. In [47, 48] sound from a moving emitter is spatialized towards a receiver con- ditioned on the tracked 3D location of the emitter. Recently, the vision community explores using visual information to estimate environmental acoustics [6, 8, 50]. However, these works only synthesize acoustics for a given viewpoint rather than a novel viewpoint. In addition, they have only addressed monaural audio, which is more forgiv- ing than binaural because humans do not perceive subtle ab- solute acoustic properties, but can detect easily inconsisten- cies in the sounds perceived by the two ears. Recent work spatializes monaural sounds by upmixing them to multiple channels conditioned on the video, where the sound emit- ters are static [17, 36]. Because the environment, emitter and receiver are static, so are the acoustics. Other work 2https://replay-dataset.github.iopredicts impulse responses in simulation either for a single environment [29], or by using few-shot egocentric obser- vations [31], or by using the 3D scene mesh [45]. While simulated results are satisfying, those models’ impact on real-world data is unknown, especially for scenarios where human speakers move and interact with each other. Unlike any of the above, we introduce and tackle the NV AS prob- lem, accounting for both acoustics and spatialization, and we propose a model that addresses the problem effectively on both synthetic and real-world data. Audio-visual learning. Recent advances in multi-modal video understanding enable new forms of self-supervised cross-modal feature learning from video [2, 24, 35], sound source localization [19,21,54], and audio-visual speech en- hancement and source separation [1, 14, 33, 40, 60]. All of these existing tasks and datasets only deal with a single viewpoint. We introduce the first audio-visual learning task and dataset that deals with multi-view audio-visual data. 3. The Novel-view Acoustic Synthesis Task We introduce a new task, novel-view acoustic synthesis (NV AS). Assuming there areN sound emitters in the scene (emitter i emits sound Ci from location Li), given the audio AS and video VS observed at the source viewpoint S, the goal is to synthesize the audioAT at the target viewpointT, as it would sound from the target location, specified by the relative pose PT of the target microphone (translation and orientation) with respect to the source view (Fig. 1). Fur- thermore, we assume that the active sound emitters in the environment are visible in the source camera, but we make no assumptions about the camera at the target location. The sound at any point R is a function of the space: AR = F(L1,...,N , C1,...,N , R| E), (1) where R is the receiver location ( S or T) and E is the environment. The emitted sounds Ci are not restricted to speech but can be ambient noise, sounding objects, etc. Our goal here is to learn a transfer function T (·) defined as AT = T (AS, VS, PT ), where S, T, L1,...,N , C1,...,N , E are not directly given and need to be inferred from VS and PT , which makes the task inherently multi-modal. This task is challenging because the goal is to model the sound field of a dynamic scene and capture acoustic changes between viewpoints given one pair of audio-visual measure- ments. While traditional signal processing methods can be applied, we show in Sec. 6 that they perform poorly. In this work, we present a learning-based rendering approach. 4. Datasets We introduce two datasets for the NV AS task: live recordings (Sec. 4.1), and simulated audio in scanned real- world environments (Sec. 4.2) (see Fig. 2). The former is Figure 2. Example source and target views for the two introduced datasets: Replay-NV AS (left) and SoundSpaces-NV AS (right). real and covers various social scenarios, but offers limited diversity of sound sources, viewpoints and environments, and is noisy. The latter has a realism gap, but allows perfect control over these aforementioned elements. Both datasets focus on human speech given its relevance in applications. However, our model design is not specific to speech. For both datasets, we capture binaural audio, which best aligns with human perception. Note that for both datasets, we collect multiple multi-modal views for train- ing and evaluation; during inference the target viewpoint(s) (and in some cases target environment) are withheld. 4.1. The Replay-NV AS Dataset Replay-NV AS contains multi-view captures of acted scenes in apartments. We capture 46 different scenarios (e.g., having a conversation, having dinner, or doing yoga) from 8 different viewpoints. In total, we collect 37 hours of video data, involving 32 participants across all scenarios. In each scenario, we invite 2–4 participants to act on a given topic. Each participant wears a near-range micro- phone, providing a clean recording of their own speech. The scene is captured by 8 DLSR cameras, each augmented with a 3Dio binaural microphone. In this way, the data captures video and audio simultaneously from multiple cameras, re- sulting in 56 possible source/target viewpoint combinations for each scene. The videos are recorded at 30 FPS and the audio is recorded with a 48k sampling rate. We use a clap- per at the beginning of the recording for temporal synchro- nization. Each scenario lasts 3–8 min. We use off-the-shelf software for multi-view camera calibration (see Supp.). To construct the dataset, we extract one-second long clips from each video with overlapping windows. We auto- matically remove silent and noisy clips based on the energy of near-range microphones, which results in 77K/12K/2K clips in total for train/val/test (details in Supp.) During training, for one sample, we randomly select two out of eight viewpoints, one as the source and one as the target. This dataset is very challenging. It covers a wide range of social activities. It is harrowed by ambient sound, room reverberation, overlapping speech and non-verbal sounds such as clapping and instruments. Participants can movefreely in the environment. We believe that this data will be useful to the community beyond the NV AS task as it can be used for benchmarking many other problems, including active speaker localization, source separation, and NVS. 4.2. The SoundSpaces-NV AS Dataset In this dataset, we synthesize multi-view audio-visual data of two people having conversations in 3D scenes. In total, we construct 1.3K hours of audio-visual data for a to- tal of 1,000 speakers, 120 3D scenes and 200K viewpoints. Our goal is to construct audio-visual data with strong spatial and acoustic correspondences across multiple view- points, meaning that the visual information should indicate what the audio should sound like, e.g., observing speaker on the left should indicate the left ear is louder and ob- serving speaker at a distance should indicate there is higher reverberation. We use the SoundSpaces 2.0 platform [7], which allows highly realistic audio and visual rendering for arbitrary camera and microphone locations in 3D scans of real-world environments [5, 52, 57]. It accounts for all ma- jor real-world acoustics phenomena: direct sounds, early specular/diffuse reflections, reverberation, binaural spatial- ization, and effects from materials and air absorption. We use the Gibson dataset [57] for scene meshes and LibriSpeech [41] for speech samples. As we are simulating two people having conversations, for a given environment, we randomly sample two speaker locations within 3 m and insert two copyright-free mannequins (one male and one fe- male) at these two locations. 3 We then randomly sample four nearby viewpoints facing the center of the two speak- ers at a height of 1.5 m (Fig. 2, right). For each speaker, we select a speech sample from LibriSpeech with matching gender. We render images at all locations as well as binau- ral impulse response for all pairs of points between speakers and viewpoints. The received sound is obtained by convolv- ing the binaural impulse response with the speech sample. During training, for one sample, we randomly sample two out of four rendered viewpoints, one as the source and one as the target. We also randomly choose one speaker to be active, simulating what we observe on the real data (i.e., usually only one person speaks at a time). 5. Visually-Guided Acoustic Synthesis We introduce a new method, Visually-Guided Acoustic Synthesis (ViGAS), to address the NV AS problem, taking as input sound and an image and outputting the sound from a different target microphone pose. ViGAS consists of five components: ambient sound sep- aration, active speaker localization, visual acoustic network, acoustic synthesis, and temporal alignment. The high-level idea is to separate the observed sound into primary and am- bient, extract useful visual information (active speaker and 3https://renderpeople.com/free-3d-people acoustic features), and use this information to guide acous- tic synthesis for the primary sound. Temporal alignment is performed during training for better optimization. ViGAS is discussed in detail next and summarised in Fig. 3. 5.1. Ambient Sound Separation ViGAS starts by decomposing the input sound into pri- mary and ambient (traffic, electric noise from a fridge or the A/C, etc.). Ambient sound is important for realism, but it also interferes with learning the model because it can carry significant energy, making the model focus on it rather than on the primary sounds, and its spatial distribution is very different from the primary sounds. By explicitly separating primary and ambient sounds, ViGAS: (1) accounts for the fact that the transfer func- tions of primary and ambient sounds are very different and thus difficult to model together; (2) avoids wasting repre- sentational power on modelling ambient sounds that might be difficult to reconstruct accurately and depend less on the viewpoint; and (3) prevents ambient sounds, which are noise-like and high-energy, from dominating learning and reconstruction. In practice, as we show in Sec. 6, without the ambient sound separation, the model performs poorly. The goal of ambient sound separation is thus to con- struct a function (AC, AN ) = P(AS) that separates the input sound AS into primary sound AC and ambient sound AN . Existing approaches to this problem are based on sig- nal processing [3, 13] or learning [11, 15]. We find that pre- trained speech enhancement models such as Denoiser [11] tend to aggressively remove the noise including the primary sound, which hinders re-synthesis. We thus opt for band- pass filtering, passing frequencies within a certain range and rejecting/attenuating frequencies outside of it, which we found to work well. We cut frequencies below 80 Hz for SoundSpaces-NV AS and 150 Hz for Replay-NV AS. 5.2. Active Speaker Localization Knowing where the emitters of different primary sounds are located in the environment can help to solve the NV AS task. In this paper, we focus on localizing the active speaker, although there can be other important primary sound events like instruments playing, speakers interacting with objects, etc. The goal of active speaker localization is to predict the bounding box of the active speaker in each frame of the video (examples in Fig. 4). The bounding box is in the format of (ymin, ymax, xmin, xmax) and x, yare normalized to [0, 1] by the image width and height, respectively. On SoundSpaces-NV AS, this task is relatively easy be- cause of the strong correspondence between the appearance of the speaker and the gender of the speech sample, which enables to easily train a classifier for active speakers. How- ever, this is much harder on Replay-NV AS because cameras record speakers from a distance and from diverse angles,<latexit sha1_base64=\"i6ibyaT6sbkgzAjxQSwPAMQpfqc=\">AAACEHicbVC7TsMwFHXKq4RXgJHFokJ0qpIOwFiJhbFI9CG1UeU4t61Vx4lsp6iK+gks/AoLAwixMrLxN7htkKDlSJaOzrnH9j1BwpnSrvtlFdbWNza3itv2zu7e/oFzeNRUcSopNGjMY9kOiALOBDQ00xzaiQQSBRxaweh65rfGIBWLxZ2eJOBHZCBYn1GijdRzzrsBDJjIKAgNcmovLsZjBvd2F0T4Y/Sckltx58CrxMtJCeWo95zPbhjTNDJxyolSHc9NtJ8RqRnlMLW7qYKE0BEZQMdQQSJQfjZfaIrPjBLifizNERrP1d+JjERKTaLATEZED9WyNxP/8zqp7l/5GRNJqkHQxUP9lGMd41k7OGQSqOYTQwiVzPwV0yGRhJoOlG1K8JZXXiXNasW7qFRvq6VaOa+jiE7QKSojD12iGrpBddRAFD2gJ/SCXq1H69l6s94XowUrzxyjP7A+vgGf0Z2C</latexit> source view <latexit sha1_base64=\"ujCEHc1Ew85al5RM7+4eHXWgOus=\">AAACGHicbVDLSsNAFJ34rPEVdekmWISuatKFuiy4cVnBPqAJZTK5aYdOJmFmIsTQz3Djr7hxoYjb7vwbp20EbT0wcDjnXO7cE6SMSuU4X8ba+sbm1nZlx9zd2z84tI6OOzLJBIE2SVgiegGWwCiHtqKKQS8VgOOAQTcY38z87gMISRN+r/IU/BgPOY0owUpLA+vCC2BIeUGAKxATU+ZcjUDSRwg9z8RZSBPTAx7+BAZW1ak7c9irxC1JFZVoDaypFyYki/U4YVjKvuukyi+wUJQwmJheJiHFZIyH0NeU4xikX8wPm9jnWgntKBH6cWXP1d8TBY6lzONAJ2OsRnLZm4n/ef1MRdd+QXmaKeBksSjKmK0Se9aSHVIBRLFcE0wE1X+1yQgLTHQH0tQluMsnr5JOo+5e1ht3jWqzVtZRQafoDNWQi65QE92iFmojgp7QC3pD78az8Wp8GJ+L6JpRzpygPzCm3xKjoPQ=</latexit> synthesized audio <latexit sha1_base64=\"rVGWyDmLrPr+5h/dczSSJIljmJ8=\">AAACCnicbVDLSsNAFJ3UV42vqEs30VboqiRdqMuKG1dSwT6gKWUyuWmHTiZhZiKU0LUbf8WNC0Xc+gXu/BunbQRtPTBwOOce7tzjJ4xK5ThfRmFldW19o7hpbm3v7O5Z+wctGaeCQJPELBYdH0tglENTUcWgkwjAkc+g7Y+upn77HoSkMb9T4wR6ER5wGlKClZb61rHnw4DyjABXICZm+bJ/UzY94MGP1LdKTtWZwV4mbk5KKEejb316QUzSSMcJw1J2XSdRvQwLRQmDiemlEhJMRngAXU05jkD2stkpE/tUK4EdxkI/ruyZ+juR4UjKceTryQiroVz0puJ/XjdV4UUvozxJFXAyXxSmzFaxPe3FDqgAothYE0wE1X+1yRALTHQH0tQluIsnL5NWreqeVWu3tVK9ktdRREfoBFWQi85RHV2jBmoigh7QE3pBr8aj8Wy8Ge/z0YKRZw7RHxgf34CpmhA=</latexit> A N <latexit sha1_base64=\"IP/vO/ywPq3pjLbhe+NUGhi4mQo=\">AAACDHicbVDLSsNAFJ3UV42vqks3wVboqiRdqMuKIC4r2Ac0sUwmN+3QySTMTIQS+gFu/BU3LhRx6we482+cthG09cDA4ZxzuXOPnzAqlW1/GYWV1bX1jeKmubW9s7tX2j9oyzgVBFokZrHo+lgCoxxaiioG3UQAjnwGHX90OfU79yAkjfmtGifgRXjAaUgJVlrql8quDwPKMwJcgZiYlYv+1d2oYrrAgx9Rp+yaPYO1TJyclFGOZr/06QYxSSM9ThiWsufYifIyLBQlDCamm0pIMBnhAfQ05TgC6WWzYybWiVYCK4yFflxZM/X3RIYjKceRr5MRVkO56E3F/7xeqsJzL6M8SRVwMl8UpsxSsTVtxgqoAKLYWBNMBNV/tcgQC0x0B9LUJTiLJy+Tdr3mnNbqN/Vyo5rXUURH6BhVkYPOUANdoyZqIYIe0BN6Qa/Go/FsvBnv82jByGcO0R8YH98Im5rl</latexit> A k F <latexit sha1_base64=\"E4YMEUvlD1UR2eqfZ9n5U2UQvhM=\">AAACEHicbVDLSsNAFJ3UV42vqks3wVYsCCXpQl1WBHFZwT6giWUyvWmHTiZhZiKUkE9w46+4caGIW5fu/BunD0FbDwwczrmHO/f4MaNS2faXkVtaXlldy6+bG5tb2zuF3b2mjBJBoEEiFom2jyUwyqGhqGLQjgXg0GfQ8oeXY791D0LSiN+qUQxeiPucBpRgpaVu4dj1oU95SoArEJlZuuhe3aXDEycrmS7w3o/RLRTtij2BtUicGSmiGerdwqfbi0gS6jhhWMqOY8fKS7FQlDDITDeREGMyxH3oaMpxCNJLJwdl1pFWelYQCf24sibq70SKQylHoa8nQ6wGct4bi/95nUQF515KeZwo4GS6KEiYpSJr3I7VowKIYiNNMBFU/9UiAyww0R1IU5fgzJ+8SJrVinNaqd5Ui7XyrI48OkCHqIwcdIZq6BrVUQMR9ICe0At6NR6NZ+PNeJ+O5oxZZh/9gfHxDdStnGE=</latexit> A k +1 F <latexit sha1_base64=\"Esc4nwi8/betjAhRM/2r9ODUg9Q=\">AAACEHicbVDLSgMxFM34rONr1KWbYCsWhDLThbqsuHFZwT6gHYdMetuGZjJDkhHK0E9w46+4caGIW5fu/BvTh6CtBwKHc+7h5p4w4Uxp1/2ylpZXVtfWcxv25tb2zq6zt19XcSop1GjMY9kMiQLOBNQ00xyaiQQShRwa4eBq7DfuQSoWi1s9TMCPSE+wLqNEGylwTtoh9JjIKAgNcmQXLoPqXTY49UYFuw2i82METt4tuRPgReLNSB7NUA2cz3Ynpmlk4pQTpVqem2g/I1IzymFkt1MFCaED0oOWoYJEoPxsctAIHxulg7uxNE9oPFF/JzISKTWMQjMZEd1X895Y/M9rpbp74WdMJKkGQaeLuinHOsbjdnCHSaCaDw0hVDLzV0z7RBJqOlC2KcGbP3mR1Msl76xUvinnK8VZHTl0iI5QEXnoHFXQNaqiGqLoAT2hF/RqPVrP1pv1Ph1dsmaZA/QH1sc35J2caw==</latexit> A k +1 P <latexit sha1_base64=\"ry75i/5N7LClX7jRLkXusVgZyuE=\">AAACCnicbVDLSsNAFJ34rPEVdekm2gpdlaQLdVlx47KCfUAbwmRy0w6dTMLMRCihazf+ihsXirj1C9z5N07bCNp6YOBwzj3cuSdIGZXKcb6MldW19Y3N0pa5vbO7t28dHLZlkgkCLZKwRHQDLIFRDi1FFYNuKgDHAYNOMLqe+p17EJIm/E6NU/BiPOA0ogQrLfnWST+AAeU5Aa5ATMzKle9UzD7w8EfyrbJTc2awl4lbkDIq0PStz36YkCzWccKwlD3XSZWXY6EoYTAx+5mEFJMRHkBPU45jkF4+O2Vin2kltKNE6MeVPVN/J3IcSzmOAz0ZYzWUi95U/M/rZSq69HLK00wBJ/NFUcZsldjTXuyQCiCKjTXBRFD9V5sMscBEdyBNXYK7ePIyaddr7nmtflsvN6pFHSV0jE5RFbnoAjXQDWqiFiLoAT2hF/RqPBrPxpvxPh9dMYrMEfoD4+MbUY2Z8g==</latexit> A 0 <latexit sha1_base64=\"L11FAcBTE+tTajH3mJd/8rD6Kw8=\">AAACEHicbVDLSsNAFJ3UV42vqks3wVbsqiRdqMuKG5cV+oImhMnkth06mYSZiVBCPsGNv+LGhSJuXbrzb5w+BG09MHA45x7u3BMkjEpl219GYW19Y3OruG3u7O7tH5QOjzoyTgWBNolZLHoBlsAoh7aiikEvEYCjgEE3GN9M/e49CElj3lKTBLwIDzkdUIKVlvzSuRvAkPKMAFcgcrPijrDKrnO/VTFd4OGP4ZfKds2ewVolzoKU0QJNv/TphjFJIx0nDEvZd+xEeRkWihIGuemmEhJMxngIfU05jkB62eyg3DrTSmgNYqEfV9ZM/Z3IcCTlJAr0ZITVSC57U/E/r5+qwZWXUZ6kCjiZLxqkzFKxNW3HCqkAothEE0wE1X+1yAgLTHQH0tQlOMsnr5JOveZc1Op39XKjuqijiE7QKaoiB12iBrpFTdRGBD2gJ/SCXo1H49l4M97nowVjkTlGf2B8fAOjW5zj</latexit> ˆA T <latexit sha1_base64=\"JhhRXf6OIJ7FR7laLiqV5rpIbcs=\">AAACCnicbVC7TsMwFHXKq4RXgJEl0CJ1qpIOwFjEwsBQJPqQmqpynJvWquNEtoNURZ1Z+BUWBhBi5QvY+BvcNkjQciRLR+fco+t7/IRRqRznyyisrK6tbxQ3za3tnd09a/+gJeNUEGiSmMWi42MJjHJoKqoYdBIBOPIZtP3R1dRv34OQNOZ3apxAL8IDTkNKsNJS3zr2fBhQnhHgCsTELF/2b8qmBzz4kfpWyak6M9jLxM1JCeVo9K1PL4hJGuk4YVjKruskqpdhoShhMDG9VEKCyQgPoKspxxHIXjY7ZWKfaiWww1jox5U9U38nMhxJOY58PRlhNZSL3lT8z+umKrzoZZQnqQJO5ovClNkqtqe92AEVQBQba4KJoPqvNhligYnuQJq6BHfx5GXSqlXds2rttlaqV/I6iugInaAKctE5qqNr1EBNRNADekIv6NV4NJ6NN+N9Plow8swh+gPj4xt9hZoO</latexit> A L <latexit sha1_base64=\"l/3sTLmnaAwMyjpS03Ydw/hjEHo=\">AAACCnicbVDLSsNAFJ3UV42vqks30VboqiRdqMuKG5cV+oKmhMnkph06mYSZiVBC1278FTcuFHHrF7jzb5y2EbT1wMDhnHu4c4+fMCqVbX8ZhbX1jc2t4ra5s7u3f1A6POrIOBUE2iRmsej5WAKjHNqKKga9RACOfAZdf3wz87v3ICSNeUtNEhhEeMhpSAlWWvJKp64PQ8ozAlyBmJqVa69VMV3gwY/klcp2zZ7DWiVOTsooR9MrfbpBTNJIxwnDUvYdO1GDDAtFCYOp6aYSEkzGeAh9TTmOQA6y+SlT61wrgRXGQj+urLn6O5HhSMpJ5OvJCKuRXPZm4n9eP1Xh1SCjPEkVcLJYFKbMUrE168UKqACi2EQTTATVf7XICAtMdAfS1CU4yyevkk695lzU6nf1cqOa11FEJ+gMVZGDLlED3aImaiOCHtATekGvxqPxbLwZ74vRgpFnjtEfGB/fihWaFg==</latexit> A T <latexit sha1_base64=\"8k4/P6idjAkZ0FY+ekkRyAoZD64=\">AAACCnicbVC7TsMwFHXKq4RXgJEl0CJ1qpIOwFjEwlgEfUhNVDnuTWvVcSLbQaqiziz8CgsDCLHyBWz8DW4bJGg5kqWjc+7R9T1BwqhUjvNlFFZW19Y3ipvm1vbO7p61f9CScSoINEnMYtEJsARGOTQVVQw6iQAcBQzawehq6rfvQUga8zs1TsCP8IDTkBKstNSzjr0ABpRnBLgCMTHLl73bsukB7/9IPavkVJ0Z7GXi5qSEcjR61qfXj0ka6ThhWMqu6yTKz7BQlDCYmF4qIcFkhAfQ1ZTjCKSfzU6Z2Kda6dthLPTjyp6pvxMZjqQcR4GejLAaykVvKv7ndVMVXvgZ5UmqgJP5ojBltortaS92nwogio01wURQ/VebDLHARHcgTV2Cu3jyMmnVqu5ZtXZTK9UreR1FdIROUAW56BzV0TVqoCYi6AE9oRf0ajwaz8ab8T4fLRh55hD9gfHxDYiDmhU=</latexit> A S <latexit sha1_base64=\"ZkPVCX7tH349aamrjEpL6w6m7qE=\">AAACCnicbVDLSsNAFJ34rPEVdekm2gpdlaQLdVlw47KifUAbwmRy0w6dTMLMRCihazf+ihsXirj1C9z5N07bCNp6YOBwzj3cuSdIGZXKcb6MldW19Y3N0pa5vbO7t28dHLZlkgkCLZKwRHQDLIFRDi1FFYNuKgDHAYNOMLqa+p17EJIm/E6NU/BiPOA0ogQrLfnWST+AAeU5Aa5ATMxK27+tmH3g4Y/kW2Wn5sxgLxO3IGVUoOlbn/0wIVms44RhKXuukyovx0JRwmBi9jMJKSYjPICephzHIL18dsrEPtNKaEeJ0I8re6b+TuQ4lnIcB3oyxmooF72p+J/Xy1R06eWUp5kCTuaLoozZKrGnvdghFUAUG2uCiaD6rzYZYoGJ7kCaugR38eRl0q7X3PNa/aZeblSLOkroGJ2iKnLRBWqga9RELUTQA3pCL+jVeDSejTfjfT66YhSZI/QHxsc3qaeaKg==</latexit> V S <latexit sha1_base64=\"KZK578rulgonHj67tqTkRqOvdb4=\">AAACCnicbVDLSsNAFJ34rPEVdekm2gpdlaQLdVkQwWUF+4A2hMnkph06mYSZiVBC1278FTcuFHHrF7jzb5y2EbT1wMDhnHu4c0+QMiqV43wZK6tr6xubpS1ze2d3b986OGzLJBMEWiRhiegGWAKjHFqKKgbdVACOAwadYHQ19Tv3ICRN+J0ap+DFeMBpRAlWWvKtk34AA8pzAlyBmJiVtn9dMfvAwx/Jt8pOzZnBXiZuQcqoQNO3PvthQrJYxwnDUvZcJ1VejoWihMHE7GcSUkxGeAA9TTmOQXr57JSJfaaV0I4SoR9X9kz9nchxLOU4DvRkjNVQLnpT8T+vl6no0sspTzMFnMwXRRmzVWJPe7FDKoAoNtYEE0H1X20yxAIT3YE0dQnu4snLpF2vuee1+m293KgWdZTQMTpFVeSiC9RAN6iJWoigB/SEXtCr8Wg8G2/G+3x0xSgyR+gPjI9vk6uaHA==</latexit> V E <latexit sha1_base64=\"FkjI3s9bLFmcxXZa4Gl9tsOh5SI=\">AAACCnicbVDLSsNAFJ34rPEVdekm2gpdlaQLdVkQxGUF+4A2hMnkph06mYSZiVBC1278FTcuFHHrF7jzb5y2EbT1wMDhnHu4c0+QMiqV43wZK6tr6xubpS1ze2d3b986OGzLJBMEWiRhiegGWAKjHFqKKgbdVACOAwadYHQ19Tv3ICRN+J0ap+DFeMBpRAlWWvKtk34AA8pzAlyBmJiVtn9dMfvAwx/Jt8pOzZnBXiZuQcqoQNO3PvthQrJYxwnDUvZcJ1VejoWihMHE7GcSUkxGeAA9TTmOQXr57JSJfaaV0I4SoR9X9kz9nchxLOU4DvRkjNVQLnpT8T+vl6no0sspTzMFnMwXRRmzVWJPe7FDKoAoNtYEE0H1X20yxAIT3YE0dQnu4snLpF2vuee1+m293KgWdZTQMTpFVeSiC9RAN6iJWoigB/SEXtCr8Wg8G2/G+3x0xSgyR+gPjI9vlT2aHQ==</latexit> V F <latexit sha1_base64=\"69RFrmJixyeY2H3J4q/W9aZKu/M=\">AAACCnicbVDLSsNAFJ34rPEVdekm2gpdlaQLdVlw48JFBfuANoTJ5KYdOpmEmYlQQtdu/BU3LhRx6xe482+cthG09cDA4Zx7uHNPkDIqleN8GSura+sbm6Utc3tnd2/fOjhsyyQTBFokYYnoBlgCoxxaiioG3VQAjgMGnWB0NfU79yAkTfidGqfgxXjAaUQJVlryrZN+AAPKcwJcgZiYlbZ/UzH7wMMfybfKTs2ZwV4mbkHKqEDTtz77YUKyWMcJw1L2XCdVXo6FooTBxOxnElJMRngAPU05jkF6+eyUiX2mldCOEqEfV/ZM/Z3IcSzlOA70ZIzVUC56U/E/r5ep6NLLKU8zBZzMF0UZs1ViT3uxQyqAKDbWBBNB9V9tMsQCE92BNHUJ7uLJy6Rdr7nntfptvdyoFnWU0DE6RVXkogvUQNeoiVqIoAf0hF7Qq/FoPBtvxvt8dMUoMkfoD4yPb56pmiM=</latexit> V L <latexit sha1_base64=\"/JFQG01EDRf1x0BlqZtO7qkeaNg=\">AAACCnicbVDLSsNAFJ34rPEVdekm2gpdlaQLdVlw47JCX9CGMJnctEMnkzAzEUro2o2/4saFIm79Anf+jdM2grYeGDiccw937glSRqVynC9jbX1jc2u7tGPu7u0fHFpHxx2ZZIJAmyQsEb0AS2CUQ1tRxaCXCsBxwKAbjG9mfvcehKQJb6lJCl6Mh5xGlGClJd86GwQwpDwnwBWIqVlp+q2KOQAe/ki+VXZqzhz2KnELUkYFmr71OQgTksU6ThiWsu86qfJyLBQlDKbmIJOQYjLGQ+hrynEM0svnp0ztC62EdpQI/biy5+rvRI5jKSdxoCdjrEZy2ZuJ/3n9TEXXXk55mingZLEoypitEnvWix1SAUSxiSaYCKr/apMRFpjoDqSpS3CXT14lnXrNvazV7+rlRrWoo4RO0TmqIhddoQa6RU3URgQ9oCf0gl6NR+PZeDPeF6NrRpE5QX9gfHwDocGaJQ==</latexit> P T <latexit sha1_base64=\"dTOkQrhl+o5NiJu6ZRGkMkvATeA=\">AAACIHicbVDLSgMxFM3UVx1fVZdugkXoqsx0YV0W3LisYB/QDiWT3mlDM8mQZAp16Ke48VfcuFBEd/o1pg9BWw8EDuecy809YcKZNp736eQ2Nre2d/K77t7+weFR4fikqWWqKDSo5FK1Q6KBMwENwwyHdqKAxCGHVji6nvmtMSjNpLgzkwSCmAwEixglxkq9QrUbwoCJjIIwoKYuoYaNAesEyAgU5pISzu7nYbcLov8T7BWKXtmbA68Tf0mKaIl6r/DR7UuaxnaccqJ1x/cSE2REGUY5TN1uqiEhdEQG0LFUkBh0kM0PnOILq/RxJJV9wuC5+nsiI7HWkzi0yZiYoV71ZuJ/Xic10VWQMZGkBgRdLIpSjo3Es7Zwnymghk8sIVQx+1dMh0TZlmynri3BXz15nTQrZf+yXLmtFGulZR15dIbOUQn5qIpq6AbVUQNR9ICe0At6dR6dZ+fNeV9Ec85y5hT9gfP1DZcXpGY=</latexit> active speaker localization <latexit sha1_base64=\"/JFQG01EDRf1x0BlqZtO7qkeaNg=\">AAACCnicbVDLSsNAFJ34rPEVdekm2gpdlaQLdVlw47JCX9CGMJnctEMnkzAzEUro2o2/4saFIm79Anf+jdM2grYeGDiccw937glSRqVynC9jbX1jc2u7tGPu7u0fHFpHxx2ZZIJAmyQsEb0AS2CUQ1tRxaCXCsBxwKAbjG9mfvcehKQJb6lJCl6Mh5xGlGClJd86GwQwpDwnwBWIqVlp+q2KOQAe/ki+VXZqzhz2KnELUkYFmr71OQgTksU6ThiWsu86qfJyLBQlDKbmIJOQYjLGQ+hrynEM0svnp0ztC62EdpQI/biy5+rvRI5jKSdxoCdjrEZy2ZuJ/3n9TEXXXk55mingZLEoypitEnvWix1SAUSxiSaYCKr/apMRFpjoDqSpS3CXT14lnXrNvazV7+rlRrWoo4RO0TmqIhddoQa6RU3URgQ9oCf0gl6NR+PZeDPeF6NrRpE5QX9gfHwDocGaJQ==</latexit> P T <latexit sha1_base64=\"faeeVAXIfr0SaoLAp3Zr0roz9nE=\">AAACI3icbVDLSgMxFM3UVx1foy7dBIvQVZnpQsVVwY3LCvYBTSmZ9LYNzWSGJCOUof/ixl9x40Ipblz4L6btCNp6IXA49xxO7gkTwbXx/U+nsLG5tb1T3HX39g8Oj7zjk6aOU8WgwWIRq3ZINQguoWG4EdBOFNAoFNAKx7fzfesRlOaxfDCTBLoRHUo+4IwaS/W8GxLCkMuMgTSgpq51cgsJcZcRhGBXQ0LVQu8SkP0fbc8r+RV/MXgdBDkooXzqPW9G+jFLI2tngmrdCfzEdDOqDGcCpi5J50lsTIfQsVDSCHQ3W9w4xReW6eNBrOyTBi/Y346MRlpPotAqI2pGenU3J//bdVIzuO5mXCapAcmWQYNUYBPjeWG4zxUwIyYWUKa4/StmI9sHsx1o15YQrJ68DprVSnBZqd5XS7VyXkcRnaFzVEYBukI1dIfqqIEYekIv6A29O8/OqzNzPpbSgpN7TtGfcb6+ASaCpR8=</latexit> ambient source separation <latexit sha1_base64=\"SdE1iMYbzzo9fpE9pQ5ERseTK0g=\">AAACDHicbVDLSgMxFM34rOOr6tJNsAhdlZku1GXBjcsK9gHtUDKZO21oJhmSjFCGfoAbf8WNC0Xc+gHu/BvTdgRtPRA4nHMvN+eEKWfaeN6Xs7a+sbm1Xdpxd/f2Dw7LR8dtLTNFoUUll6obEg2cCWgZZjh0UwUkCTl0wvH1zO/cg9JMijszSSFIyFCwmFFirDQoV/ohDJnIKQgDauqCoDIC5fZBRD+infJq3hx4lfgFqaACzUH5sx9JmiV2nXKidc/3UhPkRBlGOUzdfqYhJXRMhtCzVJAEdJDPw0zxuVUiHEtlnzB4rv7eyEmi9SQJ7WRCzEgvezPxP6+XmfgqyJlIM2NTLg7FGcdG4lkzOGIKqOETSwhVzP4V0xFRhNoOtGtL8Jcjr5J2veZf1Oq39UqjWtRRQqfoDFWRjy5RA92gJmohih7QE3pBr86j8+y8Oe+L0TWn2DlBf+B8fAN4m5vO</latexit> encoder <latexit sha1_base64=\"SdE1iMYbzzo9fpE9pQ5ERseTK0g=\">AAACDHicbVDLSgMxFM34rOOr6tJNsAhdlZku1GXBjcsK9gHtUDKZO21oJhmSjFCGfoAbf8WNC0Xc+gHu/BvTdgRtPRA4nHMvN+eEKWfaeN6Xs7a+sbm1Xdpxd/f2Dw7LR8dtLTNFoUUll6obEg2cCWgZZjh0UwUkCTl0wvH1zO/cg9JMijszSSFIyFCwmFFirDQoV/ohDJnIKQgDauqCoDIC5fZBRD+infJq3hx4lfgFqaACzUH5sx9JmiV2nXKidc/3UhPkRBlGOUzdfqYhJXRMhtCzVJAEdJDPw0zxuVUiHEtlnzB4rv7eyEmi9SQJ7WRCzEgvezPxP6+XmfgqyJlIM2NTLg7FGcdG4lkzOGIKqOETSwhVzP4V0xFRhNoOtGtL8Jcjr5J2veZf1Oq39UqjWtRRQqfoDFWRjy5RA92gJmohih7QE3pBr86j8+y8Oe+L0TWn2DlBf+B8fAN4m5vO</latexit> encoder <latexit sha1_base64=\"SdE1iMYbzzo9fpE9pQ5ERseTK0g=\">AAACDHicbVDLSgMxFM34rOOr6tJNsAhdlZku1GXBjcsK9gHtUDKZO21oJhmSjFCGfoAbf8WNC0Xc+gHu/BvTdgRtPRA4nHMvN+eEKWfaeN6Xs7a+sbm1Xdpxd/f2Dw7LR8dtLTNFoUUll6obEg2cCWgZZjh0UwUkCTl0wvH1zO/cg9JMijszSSFIyFCwmFFirDQoV/ohDJnIKQgDauqCoDIC5fZBRD+infJq3hx4lfgFqaACzUH5sx9JmiV2nXKidc/3UhPkRBlGOUzdfqYhJXRMhtCzVJAEdJDPw0zxuVUiHEtlnzB4rv7eyEmi9SQJ7WRCzEgvezPxP6+XmfgqyJlIM2NTLg7FGcdG4lkzOGIKqOETSwhVzP4V0xFRhNoOtGtL8Jcjr5J2veZf1Oq39UqjWtRRQqfoDFWRjy5RA92gJmohih7QE3pBr86j8+y8Oe+L0TWn2DlBf+B8fAN4m5vO</latexit> encoder <latexit sha1_base64=\"9K8EjuVb564e1GiJg9fewBDKeuw=\">AAACDHicbVDLSgMxFM34rOOr6tJNsAhdlZku1GXBjcsK9gHtUDKZO21oJhmSjFCGfoAbf8WNC0Xc+gHu/BvTdgRtPRA4nHMuN/eEKWfaeN6Xs7a+sbm1Xdpxd/f2Dw7LR8dtLTNFoUUll6obEg2cCWgZZjh0UwUkCTl0wvH1zO/cg9JMijszSSFIyFCwmFFirDQoV/ohDJnIKQgDaupGQGUEyu2DiH5Em/Jq3hx4lfgFqaACzUH5sx9JmiV2nHKidc/3UhPkRBlGOUzdfqYhJXRMhtCzVJAEdJDPj5nic6tEOJbKPmHwXP09kZNE60kS2mRCzEgvezPxP6+XmfgqyJlIMwOCLhbFGcdG4lkzOGIKqOETSwhVzP4V0xFRhNoOtGtL8JdPXiXtes2/qNVv65VGtaijhE7RGaoiH12iBrpBTdRCFD2gJ/SCXp1H59l5c94X0TWnmDlBf+B8fANovpvE</latexit> decoder <latexit sha1_base64=\"7nTsYd0XNM+FMedCdCgnFMFu9R8=\">AAACGXicbVDLSsNAFJ34rPEVdekmWISuStKFuiy4cVnBPqAJZTK5SYdOJmFmIpTQ33Djr7hxoYhLXfk3TtoI2npg4HDOuTNzT5AxKpXjfBlr6xubW9u1HXN3b//g0Do67sk0FwS6JGWpGARYAqMcuooqBoNMAE4CBv1gcl36/XsQkqb8Tk0z8BMccxpRgpWWRpbjBRBTXhDgCsTMVJBkqcDM80zMaMwTrZse8PAnMbLqTtOZw14lbkXqqEJnZH14YUry8iLCsJRD18mUX2ChKGEwM71cQobJBMcw1JTjBKRfzDeb2edaCe0oFfpwZc/V3xMFTqScJoFOJliN5bJXiv95w1xFV35BeZYr4GTxUJQzW6V2WZMdUgFEsakmmAiq/2qTMRaY6A6kqUtwl1deJb1W071otm5b9XajqqOGTtEZaiAXXaI2ukEd1EUEPaAn9IJejUfj2Xgz3hfRNaOaOUF/YHx+A8qGoVU=</latexit> temporal alignment <latexit sha1_base64=\"pD/evznJ6yVcH49kwpsXMq4quT8=\">AAACFXicbVDLSsNAFJ3UV42vqEs3wSJ0ISXpQl0WdOGygn1AE8pkctMOnUzCzKRQQn/Cjb/ixoUibgV3/o3TNoK2Hhg4nHMPd+4JUkalcpwvo7S2vrG5Vd42d3b39g+sw6O2TDJBoEUSlohugCUwyqGlqGLQTQXgOGDQCUbXM78zBiFpwu/VJAU/xgNOI0qw0lLfOvcCGFCeE+AKxNQMKcMKQs8zScLH7o3pAQ9/3L5VcWrOHPYqcQtSQQWafevTCxOSxTpOGJay5zqp8nMsFCUMpqaXSUgxGeEB9DTlOAbp5/OrpvaZVkI7SoR+XNlz9Xcix7GUkzjQkzFWQ7nszcT/vF6mois/pzzNFHCyWBRlzFaJPavIDqkAothEE0wE1X+1yRALTHQH0tQluMsnr5J2veZe1Op39UqjWtRRRifoFFWRiy5RA92iJmohgh7QE3pBr8aj8Wy8Ge+L0ZJRZI7RHxgf36ognww=</latexit> dilated conv1D <latexit sha1_base64=\"pD/evznJ6yVcH49kwpsXMq4quT8=\">AAACFXicbVDLSsNAFJ3UV42vqEs3wSJ0ISXpQl0WdOGygn1AE8pkctMOnUzCzKRQQn/Cjb/ixoUibgV3/o3TNoK2Hhg4nHMPd+4JUkalcpwvo7S2vrG5Vd42d3b39g+sw6O2TDJBoEUSlohugCUwyqGlqGLQTQXgOGDQCUbXM78zBiFpwu/VJAU/xgNOI0qw0lLfOvcCGFCeE+AKxNQMKcMKQs8zScLH7o3pAQ9/3L5VcWrOHPYqcQtSQQWafevTCxOSxTpOGJay5zqp8nMsFCUMpqaXSUgxGeEB9DTlOAbp5/OrpvaZVkI7SoR+XNlz9Xcix7GUkzjQkzFWQ7nszcT/vF6mois/pzzNFHCyWBRlzFaJPavIDqkAothEE0wE1X+1yRALTHQH0tQluMsnr5J2veZe1Op39UqjWtRRRifoFFWRiy5RA92iJmohgh7QE3pBr8aj8Wy8Ge+L0ZJRZI7RHxgf36ognww=</latexit> dilated conv1D <latexit sha1_base64=\"hKEecPvLsfTEpWdHIYrSPwOWz+A=\">AAACC3icbVDLSsNAFJ3UV42vqEs3oUXoqiRdqMuCLlxWsA9oQplMbtqhk0mYmRRK6N6Nv+LGhSJu/QF3/o3TNoK2Hhg4nHMPd+4JUkalcpwvo7SxubW9U9419/YPDo+s45OOTDJBoE0SlohegCUwyqGtqGLQSwXgOGDQDcbXc787ASFpwu/VNAU/xkNOI0qw0tLAqngBDCnPCXAFYmaShE/cG9MDHv5oA6vq1J0F7HXiFqSKCrQG1qcXJiSLdZwwLGXfdVLl51goShjMTC+TkGIyxkPoa8pxDNLPF7fM7HOthHaUCP24shfq70SOYymncaAnY6xGctWbi/95/UxFV35OeZop4GS5KMqYrRJ7XowdUgFEsakmmAiq/2qTERaY6A6kqUtwV09eJ51G3b2oN+4a1WatqKOMzlAF1ZCLLlET3aIWaiOCHtATekGvxqPxbLwZ78vRklFkTtEfGB/fL2abDw==</latexit> conv1D <latexit sha1_base64=\"hKEecPvLsfTEpWdHIYrSPwOWz+A=\">AAACC3icbVDLSsNAFJ3UV42vqEs3oUXoqiRdqMuCLlxWsA9oQplMbtqhk0mYmRRK6N6Nv+LGhSJu/QF3/o3TNoK2Hhg4nHMPd+4JUkalcpwvo7SxubW9U9419/YPDo+s45OOTDJBoE0SlohegCUwyqGtqGLQSwXgOGDQDcbXc787ASFpwu/VNAU/xkNOI0qw0tLAqngBDCnPCXAFYmaShE/cG9MDHv5oA6vq1J0F7HXiFqSKCrQG1qcXJiSLdZwwLGXfdVLl51goShjMTC+TkGIyxkPoa8pxDNLPF7fM7HOthHaUCP24shfq70SOYymncaAnY6xGctWbi/95/UxFV35OeZop4GS5KMqYrRJ7XowdUgFEsakmmAiq/2qTERaY6A6kqUtwV09eJ51G3b2oN+4a1WatqKOMzlAF1ZCLLlET3aIWaiOCHtATekGvxqPxbLwZ78vRklFkTtEfGB/fL2abDw==</latexit> conv1D <latexit sha1_base64=\"hKEecPvLsfTEpWdHIYrSPwOWz+A=\">AAACC3icbVDLSsNAFJ3UV42vqEs3oUXoqiRdqMuCLlxWsA9oQplMbtqhk0mYmRRK6N6Nv+LGhSJu/QF3/o3TNoK2Hhg4nHMPd+4JUkalcpwvo7SxubW9U9419/YPDo+s45OOTDJBoE0SlohegCUwyqGtqGLQSwXgOGDQDcbXc787ASFpwu/VNAU/xkNOI0qw0tLAqngBDCnPCXAFYmaShE/cG9MDHv5oA6vq1J0F7HXiFqSKCrQG1qcXJiSLdZwwLGXfdVLl51goShjMTC+TkGIyxkPoa8pxDNLPF7fM7HOthHaUCP24shfq70SOYymncaAnY6xGctWbi/95/UxFV35OeZop4GS5KMqYrRJ7XowdUgFEsakmmAiq/2qTERaY6A6kqUtwV09eJ51G3b2oN+4a1WatqKOMzlAF1ZCLLlET3aIWaiOCHtATekGvxqPxbLwZ78vRklFkTtEfGB/fL2abDw==</latexit> conv1D <latexit sha1_base64=\"hKEecPvLsfTEpWdHIYrSPwOWz+A=\">AAACC3icbVDLSsNAFJ3UV42vqEs3oUXoqiRdqMuCLlxWsA9oQplMbtqhk0mYmRRK6N6Nv+LGhSJu/QF3/o3TNoK2Hhg4nHMPd+4JUkalcpwvo7SxubW9U9419/YPDo+s45OOTDJBoE0SlohegCUwyqGtqGLQSwXgOGDQDcbXc787ASFpwu/VNAU/xkNOI0qw0tLAqngBDCnPCXAFYmaShE/cG9MDHv5oA6vq1J0F7HXiFqSKCrQG1qcXJiSLdZwwLGXfdVLl51goShjMTC+TkGIyxkPoa8pxDNLPF7fM7HOthHaUCP24shfq70SOYymncaAnY6xGctWbi/95/UxFV35OeZop4GS5KMqYrRJ7XowdUgFEsakmmAiq/2qTERaY6A6kqUtwV09eJ51G3b2oN+4a1WatqKOMzlAF1ZCLLlET3aIWaiOCHtATekGvxqPxbLwZ78vRklFkTtEfGB/fL2abDw==</latexit> conv1D <latexit sha1_base64=\"6NEt3c1DCGGEW8w6MlItZZjclPs=\">AAACDXicbVC7TsMwFHV4lvAKMLJEtEidqqQDMFZiYSwSfUhNVDnOTWvVcSLbQaqi/AALv8LCAEKs7Gz8DW4bJGg5kqXjc+6RfU+QMiqV43wZa+sbm1vblR1zd2//4NA6Ou7KJBMEOiRhiegHWAKjHDqKKgb9VACOAwa9YHI983v3ICRN+J2apuDHeMRpRAlWWhpaNS+AEeU5Aa5AFGbNk/pW1EwPePijDq2q03DmsFeJW5IqKtEeWp9emJAs1nHCsJQD10mVn2OhKGFQmF4mIcVkgkcw0JTjGKSfz7cp7HOthHaUCH24sufq70SOYymncaAnY6zGctmbif95g0xFV35OeZop4GTxUJQxWyX2rBo7pAKIYlNNMBFU/9UmYyww0R1IU5fgLq+8SrrNhnvRaN42q616WUcFnaIzVEcuukQtdIPaqIMIekBP6AW9Go/Gs/FmvC9G14wyc4L+wPj4Bq5mm94=</latexit> sin <latexit sha1_base64=\"ZQikQkBrACqb6ZHsiC5ZSmW0bSU=\">AAACDnicbVDLSsNAFJ3UV42vqEs3wbbQVUm6UJcFNy4r2Ac0oUwmt+3QySTMTIQS8gVu/BU3LhRx69qdf+O0jaCtBwYO59zDnXuChFGpHOfLKG1sbm3vlHfNvf2DwyPr+KQr41QQ6JCYxaIfYAmMcugoqhj0EwE4Chj0gun13O/dg5A05ndqloAf4TGnI0qw0tLQqnkBjCnPCHAFIjernsJ8kuVV0wMe/shDq+I0nAXsdeIWpIIKtIfWpxfGJI10nDAs5cB1EuVnWChKGOSml0pIMJniMQw05TgC6WeLc3K7ppXQHsVCP67shfo7keFIylkU6MkIq4lc9ebif94gVaMrP6M8SRVwslw0SpmtYnvejR1SAUSxmSaYCKr/apMJFpjoDqSpS3BXT14n3WbDvWg0b5uVVr2oo4zO0DmqIxddoha6QW3UQQQ9oCf0gl6NR+PZeDPel6Mlo8icoj8wPr4BdHOcSQ==</latexit> tanh <latexit sha1_base64=\"Tga8KFJxQLA4We9GdKXUhSp4Xrg=\">AAACD3icbVDLSsNAFJ3UV42vqks3g63SVUm6UJcFNy4r2Ac0pUwmN+3QySTMTIQS8gdu/BU3LhRx69adf+P0IWjrgYHDOfdw5x4/4Uxpx/myCmvrG5tbxW17Z3dv/6B0eNRWcSoptGjMY9n1iQLOBLQ00xy6iQQS+Rw6/vh66nfuQSoWizs9SaAfkaFgIaNEG2lQOvd8GDKRURAaZG5XPMWGEcnyiu2BCH70Qans1JwZ8CpxF6SMFmgOSp9eENM0MnHKiVI910l0PyNSM8oht71UQULomAyhZ6ggEah+Nrsnx2dGCXAYS/OExjP1dyIjkVKTyDeTEdEjtexNxf+8XqrDq37GRJJqEHS+KEw51jGeloMDJoFqPjGEUMnMXzEdEUmo6UDZpgR3+eRV0q7X3Ita/bZeblQXdRTRCTpFVeSiS9RAN6iJWoiiB/SEXtCr9Wg9W2/W+3y0YC0yx+gPrI9vQt+cuQ==</latexit> \u0000 <latexit sha1_base64=\"oHHJ8SUbj17cqHTTpdWI0BNF2UY=\">AAACCXicbVDLSsNAFJ34rPEVdelmsAhdlaQLdVlw47KCfUATymRy0w6dTMLMRCihWzf+ihsXirj1D9z5N07bCNp6YOBwzj3cuSfMOFPadb+stfWNza3tyo69u7d/cOgcHXdUmksKbZryVPZCooAzAW3NNIdeJoEkIYduOL6e+d17kIql4k5PMggSMhQsZpRoIw0c7IcwZKKgIDTIqc1TpWwfRPSjDJyqW3fnwKvEK0kVlWgNnE8/SmmemDjlRKm+52Y6KIjUjHKY2n6uICN0TIbQN1SQBFRQzC+Z4nOjRDhOpXlC47n6O1GQRKlJEprJhOiRWvZm4n9eP9fxVVAwkeUaBF0sinOOdYpnteCISaCaTwwhVDLzV0xHRBJqOlC2KcFbPnmVdBp176LeuG1Um7Wyjgo6RWeohjx0iZroBrVQG1H0gJ7QC3q1Hq1n6816X4yuWWXmBP2B9fENL4SakQ==</latexit> loss <latexit sha1_base64=\"rVGWyDmLrPr+5h/dczSSJIljmJ8=\">AAACCnicbVDLSsNAFJ3UV42vqEs30VboqiRdqMuKG1dSwT6gKWUyuWmHTiZhZiKU0LUbf8WNC0Xc+gXu/BunbQRtPTBwOOce7tzjJ4xK5ThfRmFldW19o7hpbm3v7O5Z+wctGaeCQJPELBYdH0tglENTUcWgkwjAkc+g7Y+upn77HoSkMb9T4wR6ER5wGlKClZb61rHnw4DyjABXICZm+bJ/UzY94MGP1LdKTtWZwV4mbk5KKEejb316QUzSSMcJw1J2XSdRvQwLRQmDiemlEhJMRngAXU05jkD2stkpE/tUK4EdxkI/ruyZ+juR4UjKceTryQiroVz0puJ/XjdV4UUvozxJFXAyXxSmzFaxPe3FDqgAothYE0wE1X+1yRALTHQH0tQluIsnL5NWreqeVWu3tVK9ktdRREfoBFWQi85RHV2jBmoigh7QE3pBr8aj8Wy8Ge/z0YKRZw7RHxgf34CpmhA=</latexit> A N <latexit sha1_base64=\"wRrCu739iBVsMJarGlC+apQaqhA=\">AAACCXicbVDLSsNAFJ3UV42vqks3g0XoqiRdqMuCIC4r9gVNKZPJTTt0MgkzE6GEbt34K25cKOLWP3Dn3zhtI2jrgYHDOfdw5x4/4Uxpx/myCmvrG5tbxW17Z3dv/6B0eNRWcSoptGjMY9n1iQLOBLQ00xy6iQQS+Rw6/vhq5nfuQSoWi6aeJNCPyFCwkFGijTQoYc+HIRMZBaFBTu275nXT9kAEP8qgVHaqzhx4lbg5KaMcjUHp0wtimkYmTjlRquc6ie5nRGpGOUxtL1WQEDomQ+gZKkgEqp/NL5niM6MEOIyleULjufo7kZFIqUnkm8mI6JFa9mbif14v1eFlP2MiSTUIulgUphzrGM9qwQGTQDWfGEKoZOavmI6IJNR0oGxTgrt88ipp16ruebV2WyvXK3kdRXSCTlEFuegC1dENaqAWougBPaEX9Go9Ws/Wm/W+GC1YeeYY/YH18Q1mR5oR</latexit> STFT <latexit sha1_base64=\"JH+RPPbGUJ/mb/y//qfB3/MkPf4=\">AAACC3icbVDLSsNAFJ3UV42vqEs3oUXoqiRdqMuCG5cV7AOaUCaTm3boZBJmJkIJ3bvxV9y4UMStP+DOv3HSRtDWAwOHc87lzj1ByqhUjvNlVDY2t7Z3qrvm3v7B4ZF1fNKTSSYIdEnCEjEIsARGOXQVVQwGqQAcBwz6wfS68Pv3ICRN+J2apeDHeMxpRAlWWhpZNS+AMeU5Aa5AzM0oK6KmBzz80UZW3Wk6C9jrxC1JHZXojKxPL0xIFutxwrCUQ9dJlZ9joShhMDe9TEKKyRSPYagpxzFIP1/cMrfPtRLaUSL048peqL8nchxLOYsDnYyxmshVrxD/84aZiq78nPI0U8DJclGUMVsldlGMHVIBRLGZJpgIqv9qkwkWmOgOpKlLcFdPXie9VtO9aLZuW/V2o6yjis5QDTWQiy5RG92gDuoigh7QE3pBr8aj8Wy8Ge/LaMUoZ07RHxgf39Q5m3g=</latexit> fusion <latexit sha1_base64=\"6lXNLMjRLEKZ5zvAroB2lo8Rmpg=\">AAACEHicbVDLSsNAFJ3UV42vqEs3g63YVUm6UJcFNy4r2Ac0oUwmt+3QySTMTIQS+glu/BU3LhRx69Kdf+P0IWjrgYHDOfdw554w5Uxp1/2yCmvrG5tbxW17Z3dv/8A5PGqpJJMUmjThieyERAFnApqaaQ6dVAKJQw7tcHQ99dv3IBVLxJ0epxDEZCBYn1GijdRzzv0QBkzkFIQGObHLnq9ZDAp7ZdsHEf0YPafkVt0Z8CrxFqSEFmj0nE8/SmgWmzjlRKmu56Y6yInUjHKY2H6mICV0RAbQNVQQszTIZwdN8JlRItxPpHlC45n6O5GTWKlxHJrJmOihWvam4n9eN9P9qyBnIs00CDpf1M841gmetoMjJoFqPjaEUMnMXzEdEkmo6UDZpgRv+eRV0qpVvYtq7bZWqlcWdRTRCTpFFeShS1RHN6iBmoiiB/SEXtCr9Wg9W2/W+3y0YC0yx+gPrI9v0OKcXg==</latexit> 1 ⇥ 1 <latexit sha1_base64=\"JBIJCCkvG516QTuGo/epn1O7tpI=\">AAACGXicbVDLSsNAFJ34rPEVdelmsAhdlaQLdVlw47KCfUATymRy0w6dTMLMRAihv+HGX3HjQhGXuvJvnD4EbT0wcDjnXO7cE2acKe26X9ba+sbm1nZlx97d2z84dI6OOyrNJYU2TXkqeyFRwJmAtmaaQy+TQJKQQzccX0/97j1IxVJxp4sMgoQMBYsZJdpIA8f1QxgyUVIQGuTEJjTNlWbU921VCD0CxZTtg4h+EgOn6tbdGfAq8RakihZoDZwPP0ppnphxyolSfc/NdFASabZwmNh+riAjdEyG0DdUkARUUM4um+Bzo0Q4TqV5QuOZ+nuiJIlSRRKaZEL0SC17U/E/r5/r+CoomchyDYLOF8U5xzrF05pwxCRQzQtDCJXM/BXTEZGEmg6UbUrwlk9eJZ1G3buoN24b1WZtUUcFnaIzVEMeukRNdINaqI0oekBP6AW9Wo/Ws/Vmvc+ja9Zi5gT9gfX5DQBBoXc=</latexit> acoustic synthesis <latexit sha1_base64=\"U3GzXtl9IeBxcKEp71YtNkOC23k=\">AAACCnicbVC7TsMwFHXKq4RXgJEl0CIxVUkHYCxiYSyClkpNFDnOTWvVcSLbQaqiziz8CgsDCLHyBWz8De4DCVqOZOnonHt0fU+YMSqV43wZpaXlldW18rq5sbm1vWPt7rVlmgsCLZKyVHRCLIFRDi1FFYNOJgAnIYO7cHA59u/uQUia8ls1zMBPcI/TmBKstBRYh14IPcoLAlyBGJnVi+CmanrAox8psCpOzZnAXiTujFTQDM3A+vSilOSJjhOGpey6Tqb8AgtFCYOR6eUSMkwGuAddTTlOQPrF5JSRfayVyI5ToR9X9kT9nShwIuUwCfVkglVfzntj8T+vm6v43C8oz3IFnEwXxTmzVWqPe7EjKoAoNtQEE0H1X23SxwIT3YE0dQnu/MmLpF2vuae1+nW90nBmdZTRATpCJ8hFZ6iBrlATtRBBD+gJvaBX49F4Nt6M9+loyZhl9tEfGB/fiuuaHQ==</latexit> A S <latexit sha1_base64=\"16l0KI7Wzbr4pizR5It6hCC6vPY=\">AAACCnicbVDLSsNAFJ3UV42vqEs30VZwVZIu1GWhG5cV7APaECaTm3boZBJmJkIJXbvxV9y4UMStX+DOv3HaRtDWAwOHc+7hzj1ByqhUjvNllNbWNza3ytvmzu7e/oF1eNSRSSYItEnCEtELsARGObQVVQx6qQAcBwy6wbg587v3ICRN+J2apODFeMhpRAlWWvKt00EAQ8pzAlyBmJrVjt+smgPg4Y/kWxWn5sxhrxK3IBVUoOVbn4MwIVms44RhKfuukyovx0JRwmBqDjIJKSZjPIS+phzHIL18fsrUPtdKaEeJ0I8re67+TuQ4lnISB3oyxmokl72Z+J/Xz1R07eWUp5kCThaLoozZKrFnvdghFUAUm2iCiaD6rzYZYYGJ7kCaugR3+eRV0qnX3Mta/bZeaThFHWV0gs7QBXLRFWqgG9RCbUTQA3pCL+jVeDSejTfjfTFaMorMMfoD4+Mbku+aIg==</latexit> V C <latexit sha1_base64=\"sGBOsguUjG2mz2Wb6X2z5dMUg3o=\">AAACDnicbVDLSsNAFJ3UV42vqEs3wbbgqiRdqMuCGzdCBfuAJpTJ5KYdOpmEmYlQQr/Ajb/ixoUibl2782+cthG09cDA4Zx7uHNPkDIqleN8GaW19Y3NrfK2ubO7t39gHR51ZJIJAm2SsET0AiyBUQ5tRRWDXioAxwGDbjC+mvndexCSJvxOTVLwYzzkNKIEKy0NrJoXwJDynABXIKZm9cZTNAZZNT3g4Y88sCpO3ZnDXiVuQSqoQGtgfXphQrJYxwnDUvZdJ1V+joWihMHU9DIJKSZjPIS+phzrlX4+P2dq17QS2lEi9OPKnqu/EzmOpZzEgZ6MsRrJZW8m/uf1MxVd+jnlaaaAk8WiKGO2SuxZN3ZIBRDFJppgIqj+q01GWGCiO5CmLsFdPnmVdBp197zeuG1Umk5RRxmdoFN0hlx0gZroGrVQGxH0gJ7QC3o1Ho1n4814X4yWjCJzjP7A+PgGJJKcHQ==</latexit> M ⇥ <latexit sha1_base64=\"WqtrRHiV2pJ3nyxqKVgOYImWIi4=\">AAACCnicbVDLSsNAFJ34rPEVdekm2gquStKFuqx047KCfUAbwmRy0w6dTMLMRCihazf+ihsXirj1C9z5N07bCNp6YOBwzj3cuSdIGZXKcb6MldW19Y3N0pa5vbO7t28dHLZlkgkCLZKwRHQDLIFRDi1FFYNuKgDHAYNOMGpM/c49CEkTfqfGKXgxHnAaUYKVlnzrpB/AgPKcAFcgJmbl2m9UzD7w8EfyrbJTdWawl4lbkDIq0PStz36YkCzWccKwlD3XSZWXY6EoYTAx+5mEFJMRHkBPU45jkF4+O2Vin2kltKNE6MeVPVN/J3IcSzmOAz0ZYzWUi95U/M/rZSq68nLK00wBJ/NFUcZsldjTXuyQCiCKjTXBRFD9V5sMscBEdyBNXYK7ePIyadeq7kW1dlsr152ijhI6RqfoHLnoEtXRDWqiFiLoAT2hF/RqPBrPxpvxPh9dMYrMEfoD4+MbccuaDQ==</latexit> A C <latexit sha1_base64=\"J0MPtOeG6lFlUBy2uh5ioV5ycLI=\">AAACInicbVDLSgMxFM34dnxVXboJFsFVmenCx67gxmUFa4VOKZn0tg3NJENyp1KGfosbf8WNC0VdCX6M6UNQ64HA4ZxzubknTqWwGAQf3sLi0vLK6tq6v7G5tb1T2N27sTozHGpcS21uY2ZBCgU1FCjhNjXAklhCPe5fjP36AIwVWl3jMIVmwrpKdARn6KRW4TyKoStUzkEhmJGPzHQBaSK40WlPK4giOhBwl2qh0I9Atb+jrUIxKAUT0HkSzkiRzFBtFd6ituZZ4sa5ZNY2wiDFZs4MCi5h5EeZhZTxPutCw1HFErDNfHLiiB45pU072rinkE7UnxM5S6wdJrFLJgx79q83Fv/zGhl2zpq5UGmGoPh0USeTFDUd90XbwgBHOXSEcSPcXynvMcO468D6roTw78nz5KZcCk9K5atysRLM6lgjB+SQHJOQnJIKuSRVUiOc3JNH8kxevAfvyXv13qfRBW82s09+wfv8AnQHpWo=</latexit> target microphone viewpoint Figure 3. Visually Guided Acoustic Synthesis (ViGAS). Given the input audio AS, we first separate out the ambient sound to focus on the sound of interest. We take the source audio and source visual to localize the active speaker on the 2D image. We also extract the visual acoustic features of the environment by running an encoder on the source visual. We concatenate the active speaker feature, source visual features, and the target pose, and fuse these features with a MLP. We feed both the audio stream AC and fused visual feature VC into the acoustic synthesis network, which has M stacked audio-visual fusion blocks. In each block, the audio sequence is processed by dilated conv1d layers and the visual features are processed by conv1d layers. Lastly, the previously separated ambient sound is added back to the waveform. During training, our temporal alignment module shifts the prediction by the amount of delay estimated between the source and the target audio to align the prediction well with the target. meaning that lip motion, the main cue used by speaker lo- calization methods [21, 49, 54], is often not visible. Hence, the model has to rely on other cues to identify the speaker (such as body motion, gender or identity). Furthermore, sometimes people speak or laugh over each other. Since our focus is not speaker localization, for the Replay-NV AS we assume that this problem is solved by an external module that does audio-visual active speaker localization. To approximate the output of such a mod- ule automatically, we rely on the near-range audio record- ings. Specifically, we first run an off-the-shelf detection and tracker [10] on the video at 5 FPS and obtain, with some manual refinement, bounding boxes Bi t for i = 1 , . . . , N at each frame t. We manually assign the near-range mi- crophone audio Ai N to each tracked person. We select the active speaker D based on the maximum energy of each near-range microphone, i.e., D = argmax i \bPAi N [t : t + ∆t]2\t , where ∆t is the time interval we use to calcu- late the audio energy. We output bounding box BD as the localization feature VL. 5.3. Visual Acoustic Network and Fusion The active speaker bounding box BD only disam- biguates the active speaker from all visible humans on 2D, which is not enough to indicate where the speaker is in 3D. To infer that, the visual information is also needed. Since there is usually not much movement in one second (the length of the input video clip), the video clip does not pro- vide much extra information compared to a single frame. Thus, we choose the middle frame to represent the clip and extract the visual acoustic features VE from the input RGB image with a pretrained ResNet18 [18] before the average pooling layer to preserve spatial information. To reduce the feature size, we feed VE into a 1D convolution with kernel size 1 and output channel size 8. We then flatten the visual features to obtain feature VF . The target pose is specified as the translation along x, y, zaxes plus difference between orientations of the source “view” and the target “view” expressed via rotation angles: +y (roll), +x (pitch) and +z (yaw). We encode each angle α as its sinusoidal value: (sin(α), cos(α)). Similarly, the target pose is not enough by itself to in- dicate where the target viewpoint T is in the 3D space; to infer that, the source viewVS is again needed. For example, in top row of Fig 4, for target viewpoint 3, “two meters to the right and one meter forward” is not enough to indicate the target location is in the corridor, while the model can reason that based on the source view. We use a fusion network to predict a latent representa- tion of the scene variables S, T, LD, E(cf . Sec. 3) by first concatenating [VL, PT , VF ] and then feeding it through a multilayer perceptron (MLP). See Fig. 3 for the network. 5.4. Acoustic Synthesis With the separated primary sound AC and the visual acoustic feature VC as input, the goal of the acoustic syn- thesis module is to transform AC guided by VC. We design the acoustic synthesis network to learn a non-linear trans- fer function (implicitly) that captures these major acoustic phenomena, including the attenuation of sound in space, the directivity of sound sources (human speech is directional), the reverberation level, the head-related transfer function, as well as the frequency-dependent acoustic phenomena. Training end-to-end makes it possible to capture these sub- tle and complicated changes in the audio. Inspired by recent advances in time-domain signal mod-eling [39, 48], we design the network as M stacked synthe- sis blocks, where each block consists of multiple conv1D layers. We first encode the input audio AC into a latent space, which is then fed into the synthesis block. The key of the synthesis block is a gated multimodal fusion network that injects the visual information into the audio as follows: z =tanh(pk A(Ak F )+pk V (VC))⊙σ(qk A(Ak F )+qk V (VC)), (2) where ⊙ indicates element-wise multiplication, σ is a logis- tic sigmoid function, k = 1, . . . , Mis the layer index and p, qare both learnable 1D convolutions. After passing z through a sinusoidal activation function, the network uses two separate conv1D layers to process the feature, one producing the residual connection Ak+1 F and one producing the skip connection Ak+1 P . All skip connec- tions Ak+1 P are mean pooled and fed into a decoder to pro- duce the output AO. We add back the separated ambient sound AN as the target audio estimate: ˆAT = AO + AN . See Supp. for more details on the architecture. 5.5. Temporal Alignment In order for the model to learn well, it is important that input and output sounds are temporally aligned. While the Replay-NV AS data is already synchronised based on the clapper sound, due to the finite speed of sound, the sounds emitted from different locations may still arrive at micro- phones with a delay slightly different from the one of the clapper, causing misalignments that affect training. To align source and target audio for training, we find the delay τ that maximizes the generalized cross-correlation: RAS,AT (τ) = Et[hS(t) · hT (t − τ)], (3) where hS and hT are the feature embedding for AS and AT respectively at time t. We use the feature extractor h from the generalized cross-correlation phase transform (GCC-PHAT) algorithm [23], which whitens the audio by dividing by the magnitude of the cross-power spectral den- sity. After computing τ, we shift the prediction AO by τ samples to align with the AT and obtain AL. Note that alignment is already exact for SoundSpaces-NV AS. 5.6. Loss To compute the loss, we first encode the audio with the short-time Fourier transform (STFT), a complex-valued matrix representation of the audio where the y axis repre- sents frequency and thex axis is time. We then compute the magnitude of the STFT, and optimize the L1 loss between the the predicted and ground truth magnitudes as follows: L = \f\f||STFT(AL)||2 − ||STFT(A′ T )||2 \f\f, (4) where A′ T is the primary sound separated from AT with P(·). By taking the magnitude, we do not model the ex- act phase values, which we find hinders learning if being included in the loss. See implementation details in Supp. 6. Experiments We compare with several traditional and learning-based baselines and show that ViGAS outperforms them in both a quantitative evaluation and a human subject study. Evaluation. We measure performance from three aspects: 1. closeness to GT as measured by the magnitude spectro- gram distance (Mag). 2. correctness of the spatial sound as measured by the left-right energy ratio error (LRE), i.e., the difference of ratio of energy between left and right chan- nels and 3. correctness of the acoustic properties measured by RT60 error (RTE)[6, 50], i.e., the error in reverbera- tion time decaying by 60dB (RT60). We use a pretrained model [6] to estimate RT60 directly from speech. Baselines. We consider the following baselines: 1. In- put audio. Copying the input to the output. 2. TF Es- timator [56] + Nearest Neighbor, i.e. storing the transfer function estimated during training and retrieving the nearest neighbor during test time. We estimate transfer functions with a Wiener filter [56] and index them with the ground- truth locations of the speaker, source viewpoint, and target viewpoint for the single environment setup and their rel- ative pose for the novel environment setup. At test time, this method searches the database to find the nearest trans- fer function and applies it on the input audio. 3. Digital Signal Processing (DSP) [9]approach that takes the dis- tance, azimuth, and elevation of the sound source, applies an inverse a head-related transfer function (HRTF) to es- timate the speech spoken by the speaker and then applies another HRTF to estimate the audio at the target micro- phone location. This baseline adjusts the loudness of the left and right channels based on where the speaker is in the target view. We supply GT coordinates for SoundSpaces- NV AS and speakers’ head positions estimated with trian- gulation on Replay-NV AS. 4. Visual Acoustic Matching (V AM) [6], recently proposed for a related task of matching acoustics of input audio with a target image. This task only deals with single viewpoint and single-channel audio. We adapt their model with minimal modification by feeding in the image from the source viewpoint and concatenating the position offset of the target microphone at the multimodal fusion step. See Supp. for details. 6.1. Results on SoundSpaces-NV AS Table 1 shows the results. For synthetic data, we con- sider two evaluation setups: 1. single environment: train and test on the same environment and 2. novel environ- ment: train and test on multiple non-overlapping Gibson environments (90/10/20 for train/val/test). In the single environment setup, our model largely out- performs all baselines as well as our audio-only ablation on all metrics. TF Estimator performs poorly despite be- ing indexed by the ground truth location values because es-SoundSpaces-NV AS Replay-NV AS Single Environment Novel Environment Single Environment Mag LRE RTE Mag LRE RTE Mag LRE RTE Input audio 0.225 1.473 0.032 0.216 1.408 0.039 0.159 1.477 0.046 TF Estimator [56] 0.359 2.596 0.059 0.440 3.261 0.092 0.327 2.861 0.147 DSP [9] 0.302 3.644 0.044 0.300 3.689 0.047 0.463 1.300 0.067 V AM [6] 0.220 1.198 0.041 0.235 1.131 0.051 0.161 0.924 0.070 ViGAS w/o visual 0.173 0.973 0.031 0.181 1.007 0.036 0.146 0.877 0.046 ViGAS 0.159 0.782 0.029 0.175 0.971 0.034 0.142 0.716 0.048 Table 1. Results on SoundSpaces-NV AS and Replay-NV AS.We report the magnitude spectrogram distance (Mag), left-right energy ratio error (LRE), and RT60 error (RTE). Replay-NV AS does not have novel environment setup due to data being collected in a single environment. For all metrics, lower is better. In addition to baselines, we also evaluate ViGAS w/o visual by removing the active speaker localization and visual features. Note that reverberation time is mostly invariant of the receiver location in the same room and thus input audio has low RTE. A good model should preserve this property while synthesizing the desired acoustics for the target viewpoint. timating a transfer function directly from two audio clips is non-trivial and noisy for low-energy parts of the signal. DSP also performs badly despite having the ground truth 3D coordinates of the sound source. This is because head related transfer functions are typically recorded in anechoic chambers, which does not account for acoustics of differ- ent environments, e.g., reverberation. Both traditional ap- proaches perform worse than simply copying the input au- dio, indicating that learning-based models are needed for this challenging task. The recent model V AM [6] performs much better compared to the traditional approaches but still underperforms our model. There is a significant difference between ViGAS w/o visual and the full model; this shows that the visual knowledge about the speaker location and the environment is important for this task. Fig. 4 shows an example where given the same input source viewpoint, our model synthesizes audio for three different target viewpoints. The model reasons about how the geometry and speaker locations changes based on the source view and the target pose, and predicts the acoustic difference accordingly. See Supp. video to listen to sounds. For the novel environment setup, our model again out- performs all baselines. Compared to ViGAS in the single environment setup, both the magnitude spectrogram dis- tance and the left-right energy ratio error increase. This is expected because for novel (unseen) environments, single images capture limited geometry and acoustic information. The model fails sometime when there is a drastic viewpoint change, e.g., target viewpoint 3 in Fig. 4. This setup re- quires the model to reason or “imagine” the environment based on single audio-visual observation, which poses great challenge for NV AS as well as NVS, where typically syn- thesis is performed in a fully observed environment. Ablations. Table 2 shows ablations on the model design. To understand if the model uses visual information, we ab- late the visual features VF and the active speaker feature SS-NV AS Replay-NV AS ViGAS Mag LRE Mag LRE full model 0.159 0.782 0.142 0.716 w/o visual features 0.171 0.897 0.146 0.920 w/o ASL 0.161 0.814 0.143 0.757 w/o alignment 0.176 0.771 0.144 0.706 w/o separation 0.165 0.840 0.182 0.859 Table 2. Ablations of the model on both datasets. VL. Removing the active speaker feature leads to less dam- age on the model performance, because without the explic- itly localized active speaker, the model can still implicitly reason about the active speaker location based on the im- age and audio. If both are removed (“ViGAS w/o visual” in Table 1), the performance suffers most. To study the effectiveness of the temporal alignment and ambient sound separation modules, we ablate them sepa- rately. Removing the temporal alignment leads to higher Mag error and slightly lower LRE. As for ambient sound separation, the results show that optimizing for the high- energy noise-like ambient sound degrades the performance. 6.2. Results on Replay-NV AS Table 1 (right) shows the Replay-NV AS results. Com- pared to SoundSpaces-NV AS, the magnitudes of all errors are smaller because there are less drastic acoustic changes between viewpoints (8 DLSR cameras form a circle around the participants). Traditional approaches like TF Estima- tor and DSP still perform poorly despite using the 3D co- ordinates of the camera and the speaker (triangulated from multiple cameras). V AM performs better due to end-to-end learning; however, our model outperforms it. Compared to ViGAS w/o visual, the full model has much lower left-right energy ratio error and slightly higher reverberation time er- ror, showing that the model takes into account the speaker position and viewpoint change for synthesizing the audio.Source viewpoint Target viewpoint 1Input PredictionTarget viewpoint 2Target viewpoint 3 Source viewpoint InputTarget viewpointAV speech enhancement Target Prediction Input/pred/target Target Target Prediction Target Prediction L R L R L R L R L R L R L R L R L R L R L R L R L R Figure 4. Qualitative examples. For all binaural audio, we show the left-channel and the right-channel waveforms side-by-side. Row 1: SoundSpaces-NV AS example where given the source viewpoint and input audio, the model synthesizes audio for three different target viewpoints (target views are for reference only). In this case, the active speaker is the male speaker as indicated by the bounding box. For target viewpoint 1, the view rotates about 90 degrees and the male speaker is on the left side and the predicted left channel is louder than the right channel. Viewpoint 2 moves away from the speaker and thus yields lower amplitude compared to the first prediction. For target viewpoint 3, it is completely located outside of the living room, in which case, the sound could only come from the door open on the right (louder right channel) and the reverberation also greatly increases due to the vanishing direct sound. Row 2: Replay-NV AS example where the speaker is located on the left in the source viewpoint which becomes the right and further from the camera in target viewpoint 2, the model also predicts lower amplitude and louder right channel. On the right side, we show an example of the audio-visual speech enhancement for the active speaker. The model enhances the speech to largely match with the near-range audio (target). Mag RTE Input 0.279 0.376 ViGAS (ours) 0.234 0.122 Table 3. Speech enhancement on Replay-NV AS. Fig. 4 (row 2, left) shows a qualitative example. In the source viewpoint, the active speaker is on the left, while in the target viewpoint, he is further from the camera and on the right. The model synthesizes an audio waveform that captures the corresponding acoustic change, showing that our model successfully learns from real videos. Audio-visual speech enhancement. In some real-world applications, e.g., hearing aid devices, the goal is to obtain the enhanced clean speech of the active speaker. This can be seen as a special case of NV AS, where the target view- point is the active speaker. Our model is capable of perform- ing audio-visual speech enhancement without any modifica- tion. We simply set the target audio to the near-range audio recording for the active speaker. We show the results in Ta- ble 3. Our model obtains cleaner audio compared to the input audio (example in Fig. 4, row 2, right). Human subject study. To supplement the quantitative metrics and evaluate how well our synthesized audio cap- tures the acoustic change between viewpoints, we conduct a human subject study. We show participants the image of the target viewpoint VT as well as the audio AT as refer- ence. We provide three audio samples: the input, the pre- diction of ViGAS, and the prediction of DSP (the most nat- Dataset Input DSP ViGAS SoundSpaces-NV AS 24% 2% 74% Replay-NV AS 43% 6% 51% Table 4. Human Study. Participants favor our approach over the two most realistic sounding baselines, (1) copying the input signal, and (2) a digital signal processing baseline. urally sounding baseline) and ask them to select a clip that sounds closest to the target audio. We select 20 examples from SoundSpaces-NV AS and 20 examples from Replay- NV AS and invite 10 participants to perform the study. See Table 4 for the results. On the synthetic dataset SoundSpaces-NV AS, our approach is preferred over the baselines by a large margin. This margin is lower on the real-world Replay-NV AS dataset but is still significant. 7. Conclusion We introduce the challenging novel-view acoustic syn- thesis task and a related benchmark in form of both real and synthetic datasets. We propose a neural rendering model that learns to transform the sound from the source view- point to the target viewpoint by reasoning about the ob- served audio and visual stream. Our model surpasses all baselines on both datasets. We believe this research un- locks many potential applications and research in multi- modal novel-view synthesis. In the future, we plan to incor- porate active-speaker localization model into the approach and let the model jointly learn to localize and synthesize.References [1] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisser- man. The conversation: Deep audio-visual speech enhance- ment. In INTERSPEECH, 2018. 3 [2] Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-supervised learning by cross-modal audio-video clustering. In NeurIPS, 2020. 3 [3] M. Berouti, Richard Schwartz, and John Makhoul. Enhance- ment of speech corrupted by acoustic noise. In IEEE Inter- national Conference on Acoustics, Speech, and Signal Pro- cessing, 1979. 4 [4] J.S. Bradley. Review of objective room acoustics measures and future needs. Applied Acoustics, 2011. 2 [5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal- ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb- d data in indoor environments. 3DV, 2017. MatterPort3D dataset license available at: http://kaldir.vc.in. tum.de/matterport/MP_TOS.pdf. 4 [6] Changan Chen, Ruohan Gao, Paul Calamia, and Kristen Grauman. Visual acoustic matching. In CVPR, 2022. 2, 6, 7 [7] Changan Chen, Carl Schissler, Sanchit Garg, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip W Robinson, and Kristen Grauman. Soundspaces 2.0: A simulation platform for visual-acoustic learning. In NeurIPS 2022 Datasets and Benchmarks Track, 2022. 2, 4 [8] Changan Chen, Wei Sun, David Harwath, and Kristen Grau- man. Learning audio-visual dereverberation. In ICASSP, 2023. 2 [9] Corey I. Cheng and Gregory H. Wakefield. Introduction to head-related transfer functions (hrtfs): representations of hrtfs in time, frequency, and space. journal of the audio en- gineering society, 49(4):231–249, april 2001. 6, 7 [10] MMTracking Contributors. MMTracking: OpenMMLab video perception toolbox and benchmark. https:// github.com/open-mmlab/mmtracking, 2020. 5 [11] Alexandre Defossez, Gabriel Synnaeve, and Yossi Adi. Real time speech enhancement in the waveform domain. In Inter- speech, 2020. 4 [12] James Eaton, Nikolay Gaubitch, Allistair Moore, and Patrick Naylor. Estimation of room acoustic parameters: The ACE challenge. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(10), 2016. 2 [13] Yariv Ephraim and Harry L. Van Trees. A signal subspace approach for speech enhancement. IEEE Transactions on Speech and Audio Processing, 1995. 4 [14] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech sepa- ration. In SIGGRAPH, 2018. 3 [15] Szu-Wei Fu, Chien-Feng Liao, Yu Tsao, and Shou-De Lin. Metricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement. InInter- national Conference on Machine Learning (ICML), 2019. 4 [16] Hannes Gamper and Ivan J Tashev. Blind reverberation time estimation using a convolutional neural network. In 2018 16th International Workshop on Acoustic Signal Enhance- ment (IWAENC), pages 136–140, 2018. 2 [17] Ruohan Gao and Kristen Grauman. 2.5d visual sound. In CVPR, 2019. 2 [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 5 [19] Di Hu, Rui Qian, Minyue Jiang, Xiao Tan, Shilei Wen, Errui Ding, Weiyao Lin, and Dejing Dou. Discriminative sounding objects localization via self-supervised audiovisual match- ing. In NeurIPS, 2020. 3 [20] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In Proc. ICCV, 2021. 2 [21] Hao Jiang, Calvin Murdock, and Vamsi Krishna Ithapu. Ego- centric deep multi-channel audio-visual active speaker local- ization. In CVPR, 2022. 3, 5 [22] Florian Klein, Annika Neidhardt, and Marius Seipel. Real- time estimation of reverberation time for selection of suit- able binaural room impulse responses. In Audio for Virtual, Augmented and Mixed Realities: Proceedings of 5th Interna- tional Conference on Spatial Audio (ICSA), pages 145–150, 2019. 2 [23] C. Knapp and G. Carter. The generalized correlation method for estimation of time delay. IEEE Transactions on Acous- tics, Speech, and Signal Processing, 1976. 6 [24] Bruno Korbar, Du Tran, and Lorenzo Torresani. Coopera- tive learning of audio and video models from self-supervised synchronization. In NeurIPS, 2018. 3 [25] Jon ´aˇs Kulh ´anek, Erik Derner, Torsten Sattler, and Robert Babuˇska. ViewFormer: NeRF-free neural rendering from few images using transformers. In Proc. ECCV, 2022. 2 [26] Heinrich Kuttruff. Room Acoustics. Boca Raton, 6th edition, 2016. 2 [27] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dy- namic scenes. arXiv.cs, abs/2011.13084, 2020. 2 [28] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt. Neural humans: Pose-controlled free-view synthesis of human actors with template-guided neural radiance fields. In arXiv, 2021. 2 [29] Andrew Luo, Yilun Du, Michael J Tarr, Joshua B Tenen- baum, Antonio Torralba, and Chuang Gan. Learning neural acoustic fields. In NeurIPS, 2022. 3 [30] Wolfgang Mack, Shuwen Deng, and Emanu ¨el AP Habets. Single-channel blind direct-to-reverberation ratio estimation using masking. In INTERSPEECH, pages 5066–5070, 2020. 2 [31] Sagnik Majumder, Changan Chen, Ziad Al-Halah, and Kris- ten Grauman. Few-shot audio-visual learning of environ- ment acoustics. In Thirty-Sixth Conference on Neural Infor- mation Processing Systems, 2022. 3 [32] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck- worth. Nerf in the wild: Neural radiance fields for uncon- strained photo collections. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 7210–7219, June 2021. 1 [33] Daniel Michelsanti, Zheng-Hua Tan, Shi-Xiong Zhang, Yong Xu, Meng Yu, Dong Yu, and Jesper Jensen. An overview of deep-learning-based audio-visual speech en- hancement and separation. In arXiv, 2020. 3 [34] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. In ECCV, 2020. 1, 2 [35] Pedro Morgado, Yi Li, and Nuno Vasconcelos. Learn- ing representations from audio-visual spatial alignment. In NeurIPS, 2020. 3 [36] Pedro Morgado, Nono Vasconcelos, Timothy Langlois, and Oliver Wang. Self-supervised generation of spatial audio for 360 video. In NeurIPS, 2018. 2 [37] Prateek Murgai, Mark Rau, and Jean-Marc Jot. Blind estima- tion of the reverberation fingerprint of unknown acoustic en- vironments. In Audio Engineering Society Convention 143 . Audio Engineering Society, 2017. 2 [38] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan. RegNeRF: Regularizing neural radiance fields for view syn- thesis from sparse inputs. In Proc. CVPR, 2022. 2 [39] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A gen- erative model for raw audio, 2016. cite arxiv:1609.03499. 6 [40] Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisensory features. In ECCV, 2018. 3 [41] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public do- main audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206–5210. IEEE, 2015. 4 [42] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B. Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Deformable neural radiance fields. CoRR, abs/2011.12948, 2020. 2 [43] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural radiance fields for dynamic scenes. arXiv.cs, abs/2011.13961, 2020. 2 [44] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR) , pages 10318–10327, June 2021. 1 [45] Anton Ratnarajah, Zhenyu Tang, Rohith Chandrashekar Ar- alikatti, and Dinesh Manocha. Mesh2ir: Neural acoustic im- pulse response generator for complex 3d scenes. In ACM Multimedia, 2022. 3 [46] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Com- mon Objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction. In Proc. CVPR, 2021. 2 [47] Alexander Richard, Peter Dodds, and Vamsi Krishna Ithapu. Deep impulse responses: Estimating and parameterizing fil- ters with deep networks. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 2 [48] Alexander Richard, Dejan Markovic, Israel D Gebru, Steven Krenn, Gladstone Butler, Fernando de la Torre, and Yaser Sheikh. Neural synthesis of binaural speech from mono au- dio. In International Conference on Learning Representa- tions, 2021. 2, 6 [49] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Rad- hika Marvin, Andrew Gallagher, Liat Kaver, Sharadh Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid, Zhonghua Xi, and Caroline Pantofaru. Ava-activespeaker: An audio-visual dataset for active speaker detection. In ICASSP, 2020. 5 [50] Nikhil Singh, Jeff Mentch, Jerry Ng, Matthew Beveridge, and Iddo Drori. Image2reverb: Cross-modal reverb impulse response synthesis. In ICCV, 2021. 2, 6 [51] Vincent Sitzmann, Michael Zollh ¨ofer, and Gordon Wet- zstein. Scene representation networks: Continuous 3D- structure-aware neural scene representations.Proc. NeurIPS, 2019. 2 [52] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The Replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. 4 [53] Shih-Yang Su, Frank Yu, Michael Zollh ¨ofer, and Helge Rhodin. A-NeRF: Surface-free human 3d pose refinement via neural rendering. arXiv.cs, abs/2102.06199, 2021. 2 [54] Ruijie Tao, Zexu Pan, Rohan Kumar Das, Xinyuan Qian, Mike Zheng Shou, and Haizhou Li. Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection. In Proceedings of the 29th ACM Interna- tional Conference on Multimedia, page 3927–3935, 2021. 3, 5 [55] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh¨ofer, Christoph Lassner, and Christian Theobalt. Non- rigid neural radiance fields: Reconstruction and novel view synthesis of a deforming scene from monocular video. arXiv.cs, abs/2012.12247, 2020. 2 [56] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series. Report of the Services 19, Research Project DIC-6037 MIT, 1942. 6, 7 [57] Fei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Jiten- dra Malik, and Silvio Savarese. Gibson Env: real-world per- ception for embodied agents. In Computer Vision and Pat- tern Recognition (CVPR), 2018 IEEE Conference on. IEEE, 2018. 4 [58] Feifei Xiong, Stefan Goetze, Birger Kollmeier, and Bernd T Meyer. Joint estimation of reverberation time and early-to- late reverberation ratio from single-channel speech signals. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(2):255–267, 2018. 2 [59] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. PixelNeRF: Neural radiance fields from one or few images. In Proc. CVPR, 2021. 2[60] Hang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, and Xiaogang Wang. Vision-infused deep audio inpainting. InICCV, 2019. 3",
      "references": [
        "The conversation: Deep audio-visual speech enhancement",
        "Self-supervised learning by cross-modal audio-video clustering",
        "Enhancement of speech corrupted by acoustic noise",
        "Review of objective room acoustics measures and future needs",
        "Matterport3d: Learning from rgb- d data in indoor environments",
        "Visual acoustic matching",
        "SoundSpaces 2.0: A simulation platform for visual-acoustic learning",
        "Learning audio-visual dereverberation",
        "Introduction to head-related transfer functions (hrtfs): representations of hrtfs in time, frequency, and space",
        "MMTracking: OpenMMLab video perception toolbox and benchmark",
        "Real time speech enhancement in the waveform domain",
        "Estimation of room acoustic parameters: The ACE challenge",
        "A signal subspace approach for speech enhancement",
        "Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation",
        "Metricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement",
        "Blind reverberation time estimation using a convolutional neural network",
        "2.5d visual sound",
        "Deep residual learning for image recognition",
        "Discriminative sounding objects localization via self-supervised audiovisual matching",
        "Putting nerf on a diet: Semantically consistent few-shot view synthesis",
        "Ego- centric deep multi-channel audio-visual active speaker localization",
        "Real- time estimation of reverberation time for selection of suitable binaural room impulse responses",
        "The generalized correlation method for estimation of time delay",
        "Cooperative learning of audio and video models from self-supervised synchronization",
        "ViewFormer: NeRF-free neural rendering from few images using transformers",
        "Room Acoustics",
        "Neural scene flow fields for space-time view synthesis of dynamic scenes",
        "Neural humans: Pose-controlled free-view synthesis of human actors with template-guided neural radiance fields",
        "Learning neural acoustic fields",
        "Single-channel blind direct-to-reverberation ratio estimation using masking",
        "Few-shot audio-visual learning of environment acoustics",
        "NeRF in the wild: Neural radiance fields for unconstrained photo collections",
        "An overview of deep-learning-based audio-visual speech enhancement and separation",
        "Nerf: Representing scenes as neural radiance fields for view synthesis",
        "Learning representations from audio-visual spatial alignment",
        "Self-supervised generation of spatial audio for 360 video",
        "Blind estimation of the reverberation fingerprint of unknown acoustic environments",
        "RegNeRF: Regularizing neural radiance fields for view synthesis from sparse inputs",
        "Wavenet: A generative model for raw audio",
        "Audio-visual scene analysis with self-supervised multisensory features",
        "Librispeech: an asr corpus based on public do- main audio books",
        "Deformable neural radiance fields",
        "D-NeRF: Neural radiance fields for dynamic scenes",
        "Mesh2ir: Neural acoustic impulse response generator for complex 3d scenes",
        "Common Objects in 3D: Large-scale learning and evaluation of real-life 3D category reconstruction",
        "Deep impulse responses: Estimating and parameterizing filters with deep networks",
        "Neural synthesis of binaural speech from mono au- dio",
        "Ava-activespeaker: An audio-visual dataset for active speaker detection",
        "Image2reverb: Cross-modal reverb impulse response synthesis",
        "Scene representation networks: Continuous 3D- structure-aware neural scene representations",
        "The Replica dataset: A digital replica of indoor spaces",
        "A-NeRF: Surface-free human 3d pose refinement via neural rendering",
        "Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection",
        "Non- rigid neural radiance fields: Reconstruction and novel view synthesis of a deforming scene from monocular video",
        "Extrapolation, interpolation, and smoothing of stationary time series",
        "Gibson Env: real-world perception for embodied agents",
        "Joint estimation of reverberation time and early-to- late reverberation ratio from single-channel speech signals",
        "PixelNeRF: Neural radiance fields from one or few images",
        "Vision-infused deep audio inpainting"
      ],
      "meta_data": {
        "arxiv_id": "2301.08730v3",
        "authors": [
          "Changan Chen",
          "Alexander Richard",
          "Roman Shapovalov",
          "Vamsi Krishna Ithapu",
          "Natalia Neverova",
          "Kristen Grauman",
          "Andrea Vedaldi"
        ],
        "published_date": "2023-01-20T18:49:58Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces the novel-view acoustic synthesis (NVAS) task, which aims to synthesize audio as it would be heard from an unseen target viewpoint based on input audio-visual cues from a source viewpoint. It presents the first formulation of NVAS along with two large-scale multi-view audio-visual datasets (a real dataset Replay-NVAS and a synthetic dataset SoundSpaces-NVAS) and the Visually-Guided Acoustic Synthesis (ViGAS) network that outperforms traditional signal processing and learning-based baselines.",
        "methodology": "The proposed approach decomposes the source audio into primary and ambient components, localizes the active speaker via visual cues using a pretrained network, and extracts visual acoustic features from the source image. A fusion network (via MLP and concatenation of active speaker features, visual features, and encoded target pose) integrates these modalities. The acoustic synthesis module uses stacked conv1D and gated multimodal fusion blocks to transform the primary audio, and a temporal alignment module (using GCC-PHAT cross-correlation) ensures synchrony between source and target audio. The network is trained end-to-end using an L1 STFT magnitude loss.",
        "experimental_setup": "Experiments were conducted on two datasets: (1) Replay-NVAS, a real-world dataset capturing multi-view recordings of social activities with 37 hours of video and binaural audio; and (2) SoundSpaces-NVAS, a synthetic dataset with 1.3K hours of rendered audio-visual data. Evaluation metrics include magnitude spectrogram distance, left-right energy ratio error, and RT60 error. Additional ablation studies and a human subject study were performed to assess perceptual quality compared to baseline methods such as digital signal processing approaches and visual acoustic matching.",
        "limitations": "The approach assumes accurate active speaker localization and relies on single-frame visual cues, which might be insufficient for highly dynamic scenes or drastic viewpoint changes. The method may struggle in scenarios with overlapping speech and limited 3D information from sparse visual inputs, making it challenging to model complex environmental acoustics and spatial variations in novel (unseen) environments.",
        "future_research_directions": "Future work could integrate active speaker localization in an end-to-end manner, improve multi-view and multi-modal fusion, extend the framework to handle more dynamic and complex environments, and enhance the modeling of reverberation and head-related transfer functions. Further exploration may focus on bridging gaps between synthetic and real-world scenarios and on generalizing the model to a broader range of acoustic events.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Rapid Task-Solving in Novel Environments",
      "full_text": "Published as a conference paper at ICLR 2021 RAPID TASK -SOLVING IN NOVEL ENVIRONMENTS Samuel Ritter∗, Ryan Faulkner ∗, Laurent Sartran, Adam Santoro, Matthew Botvinick, David Raposo DeepMind London, UK {ritters, rfaulk, lsartran, adamsantoro, botvinick, draposo}@google.com ABSTRACT We propose the challenge of rapid task-solving in novel environments (RTS), wherein an agent must solve a series of tasks as rapidly as possible in an unfa- miliar environment. An effective RTS agent must balance between exploring the unfamiliar environment and solving its current task, all while building a model of the new environment over which it can plan when faced with later tasks. While modern deep RL agents exhibit some of these abilities in isolation, none are suit- able for the full RTS challenge. To enable progress toward RTS, we introduce two challenge domains: (1) a minimal RTS challenge called the Memory&Planning Game and (2) One-Shot StreetLearn Navigation, which introduces scale and com- plexity from real-world data. We demonstrate that state-of-the-art deep RL agents fail at RTS in both domains, and that this failure is due to an inability to plan over gathered knowledge. We develop Episodic Planning Networks (EPNs) and show that deep-RL agents with EPNs excel at RTS, outperforming the nearest baseline by factors of 2-3 and learning to navigate held-out StreetLearn maps within a sin- gle episode. We show that EPNs learn to execute a value iteration-like planning algorithm and that they generalize to situations beyond their training experience. 1 I NTRODUCTION An ideal AI system would be useful immediately upon deployment in a new environment, and would become more useful as it gained experience there. Consider for example a household robot deployed in a new home. Ideally, the new owner could turn the robot on and ask it to get started, say, by cleaning the bathroom. The robot would use general knowledge about household layouts to ﬁnd the bathroom and cleaning supplies. As it carried out this task, it would gather information for use in later tasks, noting for example where the clothes hampers are in the rooms it passes. When faced with its next task, say, doing the laundry, it would use its newfound knowledge of the hamper locations to efﬁciently collect the laundry. Humans make this kind of rapid task-solving in novel environments (RTS) look easy (Lake et al., 2017), but as yet it remains an aspiration for AI. Prominent deep RL systems display some of the key abilities required, namely, exploration and planning. But, they need many episodes over which to explore (Ecoffet et al., 2019; Badia et al., 2020) and to learn models for planning (Schrittwieser et al., 2019). This is in part because they treat each new environment in isolation, relying on generic exploration and planning algorithms. We propose to overcome this limitation by treating RTS as a meta-reinforcement learning (RL) problem, where agents learn exploration policies and planning algorithmsfrom a distribution over RTS challenges. Our contributions are to: 1. Develop two domains for studying meta-learned RTS: the minimal and interpretable Mem- ory&Planning Game and the scaled-up One-Shot StreetLearn. 2. Show that previous meta-RL agents fail at RTS because of limitations in their ability to plan using recently gathered information. ∗Equal contribution. 1 arXiv:2006.03662v3  [cs.LG]  19 Apr 2021Published as a conference paper at ICLR 2021 Prev. action Prev. state Goal Embeddings State Observation  Last K elements Query Retrieve Multi-head attention Iterate Append goal Append state Shared MLP Feature-wise max Choose action turn left Policy network Residual Optional direct path Episodic storage Write new memory (at time t+1) a) b) GoalState Episode n – 1 Task m Task 1 Task 1Task 2 ... Task m Episode n Episode n + 1 sample environment sample environment Figure 1: (a) Rapid Task Solving in Novel Environments(RTS) setup. A new environment is sampled in every episode. Each episode consists of a sequence of tasks which are deﬁned by sampling a new goal state and a new initial state. The agent has a ﬁxed number of steps per episode to complete as many tasks as possible. (b) Episodic Planning Network(EPN) architecture. The EPN uses multiple iterations of a single shared self-attention function over memories retrieved from an episodic storage. 3. Design a new architecture – the Episodic Planning Network (EPN) – that overcomes this limitation, widely outperforming prior agents across both domains. 4. Show that EPNs learn exploration and planning algorithms that generalize to larger prob- lems than those seen in training. 5. Demonstrate that EPNs learn a value iteration-like planning algorithm that iteratively prop- agates information about state-state connectivity outward from the goal state. 2 P ROBLEM FORMULATION Our objective is to build agents that can maximize reward over a sequence of tasks in a novel en- vironment. Our basic approach is to have agents learn to do this through exposure to distributions over multi-task environments. To deﬁne such a distribution, we ﬁrst formalize the notion of an envi- ronment eas a 4-tuple (S,A,P a,R) consisting of states, actions, a state-action transition function, and a distribution overreward functions. We then deﬁne the notion of a task tin environment eas a Markov decision process (MDP) (S,A,P a,r) that results from sampling a reward function rfrom R. We can now deﬁne a framework for learning to solve tasks in novel environments as a simple gener- alization of the popular meta-RL framework (Wang et al., 2017; Duan et al., 2016). In meta-RL, the agent is trained on MDPs (S,A,P a,r) sampled from a task distribution D. In the rapid task-solving in novel environments (RTS) paradigm, we instead sample problems by ﬁrst sampling an environ- ment efrom an environment distribution E, then sequentially sampling tasks, i.e. MDPs, from that environment’s reward function distribution (see Figure 1). An agent can be trained for RTS by maximizing the following objective: Ee∼E[Er∼Re [Je,r(θ)]], where Je,r is the expected reward in environment ewith reward function r. When there is only one reward function per environment, the inner expectation disappears and we recover the usual meta- RL objective Ee∼E[Je(θ)]. RTS can be viewed as meta-RL with the added complication that the reward function changes within the inner learning loop. 2Published as a conference paper at ICLR 2021 Observation Sampled environment State Goal 1st task unvisited visited once visited more than once action direction reward collected starting state 2nd task 3rd task EPNMRAMerlinLSTM 0.0 0.2 0.4 0.6 0.8 1.0Fraction of oracle a) b) c) e)d) Oracle with perfect information Optimal exploration within each task Number of steps to goal 0 4 8 12 16 20 24 28 0 2 4 6 8 10 Exploration + optimal planning Optimal exploration within each task Oracle EPN LSTM Merlin MRA Training steps Average reward 0.0 0.5 1.0 1.5 2.0 1e9 0 5 10 15 20 25 30 Nth task in episode Figure 2: Memory&Planning Game. (a) Example 4×4 environment (not observable by the agent) and state-goal observation. (b) Training curves. Performance measured by the average reward per episode, which corresponds to the average number of tasks completed within a 100-step episode (showing the best runs from a large hyper-parameter sweep for each model). (c) Performance mea- sured in the last third of the episodes (post-training), relative to an oracle with perfect information that takes the shortest path to the goal. (d) Example trajectory of a trained EPN agent in the ﬁrst three tasks of an episode. In the ﬁrst task, the agent explores optimally without repeating states. In the subsequent tasks, the agent takes the shortest path to the goal. (e) Number of steps taken by an agent to reach the nth goal in an episode. While meta-RL formalizes the problem of learning to learn, RTS formalizes both the problems of learning to learn and learning to plan. To maximize reward while the reward function is constantly changing in non-trivial novel environments, agents must learn to (1) efﬁciently explore and effec- tively encode the information discovered during exploration (i.e., learn) and (2) use that encoded knowledge to select actions by predicting trajectories it has never experienced (i.e., plan1). 3 T HE LIMITATIONS OF PRIOR AGENTS To test whether past deep-RL agents can explore and plan in novel environments, we introduce a minimal problem that isolates the challenges of (1) exploring and remembering the dynamics of novel environments and (2) planning over those memories. The problem we propose is a simple variation of the well-known Memory Game2, wherein players must remember the locations of cards in a grid. The variation we propose, which we call the Memory&Planning Game, extends the chal- lenge to require planning as well as remembering (see Figure 2). In the Memory&Planning Game, the agent occupies an environment consisting of a grid of symbols (4×4). The observation consists of two symbols – one which corresponds to the agent’s current location, and another that corresponds to the ”goal” the agent is tasked with navigating to. The agent can not see its relative location with respect to other symbols in the grid. At each step the agent selects one of 5 possible actions: move left, move right, move up, move down, and collect. If the agent chooses the ”collect” action when its current location symbol matches the goal symbol, a reward of 1 is received. Otherwise, the agent receives a reward of 0. At the beginning of each episode, a new set of symbols is sampled, effectively inducing a new transition function. The agent is allowed a ﬁxed number of steps (100) per episode to ”collect” as many goals as possible. Each time the agent collects a goal – which corresponds to completing a task –, a new goal is sampled in and the transition function stays ﬁxed. 1Following Sutton & Barto (1998), we refer to the ability to choose actions based on predictions of not-yet- experienced events as “planning”. 3Published as a conference paper at ICLR 2021 Nth task in episode Number of steps to goal Training steps Average reward Merlin EPN Oracle LSTM MRA Merlin EPN LSTM MRA 5 intersections 7 intersections 9 intersections 12 intersections a) c) d) e) b) Oracle 0 5 10 15 20 25 30 0 5 10 15 20 25 30 35 40 45 0.0 0.2 0.4 0.6 0.8 1.0 EPNMRAMerlinLSTM Fraction of oracle 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 1e9 0 5 10 15 20 25 30 Figure 3: One-Shot StreetLearn. (a) Four example states from two randomly sampled neighbor- hoods. (b) Example connectivity graphs. (c) Evaluation performance measured on neighborhoods of a held-out city throughout the course of training (showing the best run from a large hyper-parameter sweep for each model). (d) Performance in last third of episode relative to an oracle with perfect information. (e) Number of steps taken by an agent to reach the nth goal in an episode. A successful agent will (1) efﬁciently explore the grid to discover the current set of symbols and their connectivity and (2) plan shortest paths to goal symbol if it has seen (or can infer) all of the transitions it needs to connect its current location and the current goal location. This setup supplies a minimal case of the RTS problem: at the beginning of each episode the agent faces a new transition structure (S,A,P a) and must solve the current task while ﬁnding information that will be useful for solving future tasks on that same transition structure. In subsequent tasks, the agent must use its stored knowledge of the current grid’s connectivity to plan shortest paths. In this symbolic domain, we ﬁnd evidence that previous agents learn to explore but not to plan. Speciﬁcally, they match the within-trial planning optimum; that is, a strategy that explores opti- mally within each task, but forgets everything about the current environment when the task ends (Figure 2). We hypothesize that this failure is the result of the limited expressiveness of past archi- tectures’ mechanisms for processing retrieved memories. We designed Episodic Planning Networks to overcome this limitation. 4 E PISODIC PLANNING NETWORKS Past episodic memory agents used their memory stores by querying the memories, summing the retrieved slots, then projecting the result through a multi-layered perceptron (MLP) (Fortunato et al., 2019; Wayne et al., 2018). We hypothesize that these agents fail to plan because the weighted sum of retrieved slots is not sufﬁciently representationally expressive, and the MLP is not, on its own, sufﬁciently computationally expressive to support planning in non-trivial environments. To test this hypothesis, we replace the weighted sum and MLP with an an iterative self-attention-based architecture designed to support implicit planning. The architecture, which we call the Episodic Planning Network (EPN, see Figure 1b) consists of three main components: (1) an episodic memory which reﬂects the agent’s experience in the episode so far; (2) an iterative self-attention module that operates over memories, followed by a pooling operation (we will refer to this component as the planner); and (3) a policy network. Episodic memory – At each timestep, we start by appending the embedding of the current goal to each slot in the episodic memory. The result is passed as input to the planner. We then add a new 2In a striking parallel, the Memory Game played a key role in the development of memory abilities in early episodic memory deep-RL agents (Wayne et al., 2018). 4Published as a conference paper at ICLR 2021 memory to the episodic memory, which represents that timestep’s transition: it is the concatenation of embeddings of the current observation, the previous action, and the previous observation. Planner – A self-attention-based update function is applied over the memories to produce a pro- cessed representation of the agent’s past experience in the episode and reﬂecting the agent’s belief about the environment. This self-attention operation is iterated some number of times, sharing parameters across iterations. The current state is appended to each element resulting from the self- attention, each of which is then passed through a shared MLP. The resulting vectors are aggregated by a feature-wise max operation. Note that the self-attention (SA) does not have access to the current state until the very end – it does all of its updates with access only to the goal state and episodic memories. This design reﬂects the intuition that the SA function might learn to compute something like a value map, which represents the distance to goal from all states in memory. If the ﬁnal SA state does in fact come to represent such a value map, then a simple MLP should be sufﬁcient to compute the action that, from the current state, leads to the nearest/highest value state with respect to the goal. We ﬁnd in Section 5, Figure 4 evidence that the ﬁnal SA state comes in fact to resemble an iteratively improved value map. The speciﬁc self-attention update function we used was: Bi+1 = f(Bi + φ(Bi)) φ(x) =MHA(LayerNorm(x)) where MHA is the multi-head dot-product attention described by Vaswani et al. (2017). Layer nor- malization was applied to the inputs of each iteration, and residual connections were added around the multi-head attention operation.3 In our experiments, f was a ReLU followed by a 2-layer MLP shared across rows. Policy network – A 2-layer multi-layer perceptron (MLP) was applied to the output of the planner to produce policy logits and a baseline. When using pixel inputs, embeddings of the observations were produced by a 3-layer convolutional neural network. We trained all agents to optimize an actor-critic objective function using IMPALA, a framework for distributed RL training (Espeholt et al., 2018). For further details, please refer to the Supplemental Material. Our results show that the EPN succeeds in both exploration and planning in the Memory&Planning Game. It matches the shortest-path optimum after only a few tasks in a novel environment (Figure 2c), and its exploration and planning performance closely matches the performance of a hand-coded agent which combines a strong exploration strategy – avoiding visiting the same state twice – with an optimal planning policy (Figure 2d,e). For these results and the ones we will present in the following section, all of the episodic memories attend to all of the others. This update function scales quadratically with the number of memories (and the number of steps per episode). In the Supplemental Material we present results showing that we recover 92% of the performance using a variant of the update function that scales linearly with the size of the memory (see Figure 5 in the Suppl. Material). 5 O NE-SHOT STREET LEARN We now test whether our agent can extend its success in the Memory&Planning Game to a domain with high-dimensional pixel-based state observations, varied real-world state-connectivity, longer planning depths, and larger time scales. We introduce the One-Shot StreetLearn domain (see Figure 1a), wherein environments are sampled as neighborhoods from the StreetLearn dataset of Google StreetView images and their connectivity (Mirowski et al., 2019). Tasks are then sampled by select- ing a position and orientation that the agent must navigate to from its current location. In past work with StreetLearn, agents were faced with a single large map (e.g. a city), in which they could learn to navigate over billions of experiences (Mirowski et al., 2018). Here, we partition the 3Applying the layer normalization to the output of the multi-head attention, or removing the layer normal- ization altogether, did not produce good results in our experiments. 5Published as a conference paper at ICLR 2021 Oracle with perfect information 0 1 2 3 4 5 6 7 8 Distance to goal (in steps) 0.0 0.2 0.4 0.6 0.8 1.0Classification accuracy 0 1 2 3 4 5 6 Training steps 1e9 0 5 10 15 20 25 30 Average reward Likelihood 0.0 0.2 0.4 0.6 0.8 1.0 Number of iterations 1 2 3 4 5 6 Number of iterations 1 2 4 Number of iterations 1 2 4 a) d) b) c) 5 7 Nr. of intersections Fraction of oracle 9 0.0 0.2 0.4 0.6 0.8 1.0 1 iteration 2 iterations 3 iterations 4 iterations generalization test Figure 4: Iteration analysis and generalization. (a) Performance of an EPN agent on held-out neigh- borhoods with 5 intersections using planners with 1, 2 and 4 self-attention iterations (showing 3 runs for each condition). (b) Performance on new neighborhoods that were larger (7 and 9 intersections) than the ones used during training (5 intersections). (c) Distance-to-goal decoding accuracy from the output of the planner after 1 to 6 iterations. (d) The ability of EPN activations to predict distance to goal expands out from the goal state (blue arrow) as the number of self-attention iterations increases. See section 6.2 for details. StreetLearn data into a many different maps (e.g. neighborhoods of different cities), so that agents can be trained to rapidly solve navigation tasks in new maps. This approach is analogous to the partitioning of the ImageNet data into many one-shot classiﬁcation tasks that spurred advances in one-shot learning (Vinyals et al., 2016). In this case, rather than learning to classify in one shot, agents should learn to plan in one shot, that is, to plan after one or fewer observations of each transition in the environment. In One-Shot StreetLearn, the agent’s observations consist of an image representing the current state and an image representing the goal state (Figure 1b). The agent receives no other information from the environment. The available actions are turn right, which orients the agent clockwise toward the next available direction of motion from its current node; turn left, which does the same in the other direction; and move forward, which moves the agent along the direction it is facing to the next available node. In each episode, we sample a new neighborhood with 5 intersections from one of 12 cities. To reduce the exploration problem while keeping the planning difﬁculty constant, we removed all the locations between intersections that corresponded to degree-2 nodes. This resulted in graphs with a median of 12 nodes (see example connectivity graph in Figure 3b, leftmost panel). Every time the agent reaches a goal, a new starting- and goal-state pair is sampled, initiating a new task, until the ﬁxed episode step limit is reached (200 steps). Neighborhoods with 5 intersections allow us to sample from approximately 380 possible tasks (i.e. starting- and goal-state pairs) ensuring a small chance of repeating any single task. A step that takes the agent to the goal state results in a reward of 1. Any other step results in a reward of 0. One-Shot StreetLearn has several important characteristics for developing novel-environment plan- ning agents. First, we can generate many highly varied environments for training by sampling neighborhoods from potentially any place in the world. This allows us to generate virtually unlim- ited maps with real-world connectivity structure for agents to learn to take advantage of. Second, the agent’s observations are rich, high-dimensional visual inputs that simulate a person’s experience when navigating. These inputs can in principle be used by an agent to infer some of the structure of a new environment. Third, we can scale the planning depth arbitrarily by changing the size of the sampled neighborhoods – a useful feature for iteratively developing increasing capable planning systems. Fourth, the time-scale of planning can be varied independently of the planning depth by varying the number of nodes between intersections. This is a useful feature for developing tempo- rally abstract (i.e. jumpy) planning abilities (see Section 7 for discussion). 6Published as a conference paper at ICLR 2021 EPNs Outperform All Baselines – We trained EPN-based agents and previous deep-RL agents on One-Shot StreetLearn neighborhoods with 5 intersections. Our agent signiﬁcantly outperformed the baselines, successfully reaching 28.7 goals per episode (averaged over 100 consecutive episodes, Figure 3c). It is important to note that this performance was measured on neighborhoods of a held-out city, which the agent was visiting for the ﬁrst time. Baseline agents with an episodic memory system (Merlin and MRA) did not exceed 14.5 goals per episode, performing better than ones without (LSTM) that were only able to reach 10.0. EPNs optimally navigate new maps after 10-15 tasks –Plotting the average number of steps taken by the agent to complete the nth task of an episode reveals that the agents needs fewer and fewer steps to complete a new task, matching the minimum number of steps required to reach a new goal after 10–15 tasks (Figure 3e). In the last third of every episode, EPNs achieve on average 88% ±2% of the performance of an oracle with perfect information that systematically takes the shortest-path to the goals (Figure 3d). We chose the last third of the episodes for this comparison as a way to exclude steps used for exploration and thus have a better estimate of the agent’s planning capabilities. EPNs Generalize to Larger Maps –We trained EPN agents on neighborhoods with 5 intersections and evaluated them (with gradient learning turned off) on larger neigbhoroods with 7 and 9 intersec- tions. We found that the trained agents achieved strong performance relative to the oracle, suffering only small marginal drops in performance even on maps nearly double the size of those seen during training (Figure 4b, in blue). More “thinking time” improves performance – A desireable property for a planning algorithm would be for performance to increase as “thinking time” increases, allowing the user to obtain better performance simply by allowing more computation time. We test whether EPNs have this property by evaluating the effect of the number of self-attention iterations (i.e., “thinking time”) on evalua- tion performance. We observed a systematic performance boost with each additional iteration used in training4 (see Figure 4a). When evaluated on neighborhoods with 5 intersections (same neigh- borhood size as the ones used during training) the EPN agent’s performance approaches saturation with 2 or more iterations, as indicated by the fraction-of-oracle performance measure in Figure 4b (leftmost points). When we increase the size of the neighborhoods (7 and 9 intersections) the performance difference between 2 and 4 iterations becomes more pronounced, as indicated by the divergence between the blue and green dashed lines in Figure 4b. Trained EPNs recursively reﬁne a value-like map – To demonstrate this we froze the weights of a trained EPN agent and replaced its policy network with an MLP decoder of the same size. We then trained the decoder (with a stop gradient to the planner) to output the distance from a random state to a random goal, while providing the planner with memories containing all the transitions of a randomly sampled environment. We repeated this experiment with six decoders that received inputs from the planner after a ﬁxed number of iterations, from 1 to 6.5 The training loss of the different decoders revealed a steady increase in the ability to compute dis- tance to goal as we increase the number of self-attention iterations from 1 to 6 (see Figure 6a in the Suppl. Material). This gain is a direct consequence of improved decoding for longer distances, as made evident by the gradual rightward shift in the drop-off of the classiﬁcation accuracy plotted against distance to goal (Figure 4c). This holds true even when we increase the number of iterations beyond the number of iterations used during the training of the planner. Altogether, this suggests that the planner is able to “look” further ahead when given more iterations. We can visualize this result by selecting a single evaluation environment (in this case, a larger neigh- borhood with 12 intersections) with a ﬁxed goal location, and measuring the likelihood resulting from all possible states in that environment. This manipulation reveals a spatial pattern in the ability to compute distance to goal, spreading radially from the goal location as we increase the number of self-attention iterations (see Figure 4d and Figure 6b in the Suppl. Material). Ablations – Besides providing a strong baseline, MERLIN/MRA also represent a key ablation for understanding the causes of EPNs’ performance: they have everything the EPN agent hasexcept the planning module. LSTMs represent the next logical ablation: they lack both episodic memory and 4The experiments described here were restricted to one-hot inputs for quicker turnaround. 5Note that while the decoders were trained in a supervised setting using 1 to 6 iterations, the weights of the planner were trained once in the RL setting using 4 iterations. 7Published as a conference paper at ICLR 2021 the planning module. The performance across these three types of architectures shows that episodic memory alone helps very slightly, but is not enough to produce planning behavior; for that, the self-attention module is necessary. Another important ablation study is the progressive removal of the later planning iterations (Figure 4a). From this experiment we learned that one step of self-attention isn’t enough: iterating the planning module is necessary for full performance. 6 R ELATED WORK Deriving repurposable environment knowledge to solve arbitrary tasks has been a long-standing goal in the ﬁeld of RL (Foster & Dayan, 2002; Dayan, 1993) toward which progress has recently been made (Sutton et al., 2011; Schaul et al., 2015; Borsa et al., 2018). While quick to accommodate new tasks, these approaches require large amounts of time and experience to learn about new envi- ronments. This can be attributed to their reliance on gradient descent for encoding knowledge and their inability to bring prior knowledge to new environments (Botvinick et al., 2019). Our approach overcomes these problems by recording information about new environments in activations instead of weights and learning prior knowledge in the form of exploration and planning policies. The possibility of learning to explore with recurrent neural networks was demonstrated by Wang et al. (2017) and Duan et al. (2016) in the context of bandit problems. Wang et al. (2017) further demonstrated that an LSTM could learn to display the behavioral hallmarks of model-based control – in other words, learn to plan– in a minimal task widely used for studying planning in humans Daw et al. (2011). The effectiveness of learned, implicit planning was demonstrated further in fully- observed spatial environments using grid-structured memory and convolutional update functions (Tamar et al., 2016; Lee et al., 2018b; Guez et al., 2019). Gupta et al. (2017) extended this approach to partially observed environments with ground-truth ego-motion by training agents to emulate an expert that computes shortest paths. Our present work can be seen as extending learned planning by (1) solving partially-observed environments by learning to gather the information needed for planning, and (2) providing an architecture appropriate for a broader range of (e.g. non-spatial) planning problems. Model-based reinforcement learning (MBRL) has long been concerned with building agents capable of constructing and planning over environment models (for review see Moerland et al., (2020)). However, we are not aware of any MBRL agents that learn a deployable model within a single episode of experience, as is required for the RTS setting. Learning to plan can be seen as a speciﬁc case of learning to solve combinatorial optimization problems, a notion that has been taken up by recent work (for review see Bengio et al., 2018). Es- pecially relevant to our work is Kool et al (2018), who show that transformers can learn to solve large combinatorial optimization problems, comparing favorably with more resource-intensive in- dustrial solvers. This result suggests that in higher-depth novel-environment planning problems than we consider in our current experiments, the transformer-based architecture may continue to be effective. Savinov et al. (2018) develop an alternative to end-to-end learned planning that learns a distance metric over observations for use with a classical planning algorithm. Instead of learning to explore, this system relies on expert trajectories and random walks. It’s worth noting that hand-written plan- ning algorithms do not provide the beneﬁts of domain-adapted planning demonstrated by Kool et al. (2018), and that design modiﬁcations would be needed to extend this approach to tasks requiring abstract planning- e.g. jumpy planning and planning over belief states - whereas the end-to-end learning approach can be applied to such problems out-of-the-box. Recent work has shown episodic memory to be effective in extending the capabilities of deep-RL agents to memory intensive tasks (Oh et al., 2016; Wayne et al., 2018; Ritter et al., 2018b; Fortu- nato et al., 2019). We chose episodic memory because of the following desirable properties. First, because the episodic store is non-parametric, it can grow arbitrarily with the complexity of the envi- ronment, and approximate k-nearest neighbors method make it possible to scale to massive episodic memories in practice, as in Pritzel et al. (2017). This means that memory ﬁdelity need not decay with time. Second, episodic memory imposes no particular assumptions about the environment’s stucture, making it a potentially appropriate choice for a variety of non-spatial applications such as 8Published as a conference paper at ICLR 2021 chemical synthesis Segler et al. (2018) and web navigation Gur et al. (2018), as well as abstract planning problems of interest in AI research, such as Gregor et al.’s (2019) V oxel environment tasks. Our approach can be seen as implementing episodic model-based control (EMBC), a concept re- cently developed in cognitive neuroscience (Vikbladh et al., 2017; Ritter, 2019). While episodic control (Gershman & Daw, 2017; Lengyel & Dayan, 2008; Blundell et al., 2016) produces value estimates using memories of individual experiences in a model-free manner, EMBC uses episodic memories to inform a model that predicts the outcomes of actions. Ritter et al. (2018a) showed that a deep-RL agent with episodic memory and a minimal learned planning module (an MLP) could learn to produce behavior consistent with EMBC (Vikbladh et al., 2017). Our current work can be seen as using iterated self-attention to scale EMBC to much larger implicit models than the MLPs of past work could support. 7 F UTURE WORK We showed that deep-RL agents with EPNs can meta-learn to explore, build models on-the-ﬂy, and plan over those models, enabling them to rapidly solve sequences of tasks in unfamiliar environ- ments. This demonstration paves the way for important future work. Temporally abstract planning(or, “jumpy” planning, Akilesh et al., 2019) may be essential for agents to succeed in temporally extended environments like the real world. our method makes no assumptions about the timescale of planning, unlike other prominent approaches to learned planning (Schrittwieser et al., 2019; Racani `ere et al., 2017). Training and evaluating EPNs on problems that beneﬁt from jumpy planning, such as One-Shot StreetLearn with all intermediate nodes, may be enough to obtain strong temporal abstraction performance. Planning over belief statesmay be essential for agents to succeed in dynamic partially-observed environments. Planning over belief states might be accomplished simply by storing belief states such as those developed by Gregor et al. (2019) and training on a problem distribution that requires planning over information stored in those states. Humans are able to solve a seemingly open-ended variety of tasks, as highlighted for example by the Frostbite Challenge (Lake et al., 2017). The EPN architecture is in principle suitable for any task class wherein the reward function can be represented by an input vector, so future work may test EPNs in RTS problems with broader task distributions than those addressed in this work, e.g., by using generative grammars over tasks or by taking human input (Fu et al., 2019). REFERENCES B Akilesh, Suriya Singh, Anirudh Goyal, Alexander Neitz, and Aaron Courville. Toward jumpy planning. In International Conference on Machine Learning, 2019. Adri`a Puigdom `enech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. arXiv preprint arXiv:2003.13350, 2020. Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimiza- tion: a methodological tour d’horizon. arXiv preprint arXiv:1811.06128, 2018. Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016. Diana Borsa, Andr ´e Barreto, John Quan, Daniel Mankowitz, R ´emi Munos, Hado van Hasselt, David Silver, and Tom Schaul. Universal successor features approximators. arXiv preprint arXiv:1812.07626, 2018. Mathew Botvinick, Sam Ritter, Jane X Wang, Zeb Kurth-Nelson, Charles Blundell, and Demis Hassabis. Reinforcement learning, fast and slow. Trends in cognitive sciences, 2019. 9Published as a conference paper at ICLR 2021 Nathaniel D Daw, Samuel J Gershman, Ben Seymour, Peter Dayan, and Raymond J Dolan. Model- based inﬂuences on humans’ choices and striatal prediction errors. Neuron, 69(6):1204–1215, 2011. Peter Dayan. Improving generalization for temporal difference learning: The successor representa- tion. Neural Computation, 5(4):613–624, 1993. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, V olodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018. Meire Fortunato, Melissa Tan, Ryan Faulkner, Steven Hansen, Adri `a Puigdom`enech Badia, Gavin Buttimore, Charles Deck, Joel Z Leibo, and Charles Blundell. Generalization of reinforcement learners with working and episodic memory. In Advances in Neural Information Processing Systems, pp. 12448–12457, 2019. David Foster and Peter Dayan. Structure in the space of value functions. Machine Learning, 49 (2-3):325–346, 2002. Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to goals: Inverse reinforcement learning for vision-based instruction following. arXiv preprint arXiv:1902.07742, 2019. Samuel J Gershman and Nathaniel D Daw. Reinforcement learning and episodic memory in humans and animals: an integrative framework. Annual review of psychology, 68:101–128, 2017. Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and Aaron van den Oord. Shaping belief states with generative environment models for rl. In Advances in Neural Information Processing Systems, pp. 13475–13487, 2019. Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, S´ebastien Racani`ere, Th´eophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, et al. An investigation of model-free planning. arXiv preprint arXiv:1901.03559, 2019. Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2616–2625, 2017. Izzeddin Gur, Ulrich R ¨uckert, Aleksandra Faust, and Dilek Hakkani-T ¨ur. Learning to navigate the web. CoRR, abs/1812.09195, 2018. URL http://arxiv.org/abs/1812.09195. Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems!arXiv preprint arXiv:1803.08475, 2018. Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. arXiv preprint arXiv:1810.00825, 2018a. Lisa Lee, Emilio Parisotto, Devendra Singh Chaplot, Eric Xing, and Ruslan Salakhutdinov. Gated path planning networks. arXiv preprint arXiv:1806.06408, 2018b. M´at´e Lengyel and Peter Dayan. Hippocampal contributions to control: the third way. In Advances in neural information processing systems, pp. 889–896, 2008. 10Published as a conference paper at ICLR 2021 Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Andrew Zisserman, Raia Hadsell, et al. Learning to navigate in cities without a map. In Advances in Neural Information Processing Systems, pp. 2419–2430, 2018. Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Denis Teplyashin, Karl Moritz Hermann, Mateusz Malinowski, Matthew Koichi Grimes, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, et al. The streetlearn environment and dataset. arXiv preprint arXiv:1903.01292, 2019. Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. Model-based reinforcement learn- ing: A survey. arXiv preprint arXiv:2006.16712, 2020. Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee. Control of memory, active perception, and action in minecraft. arXiv preprint arXiv:1605.09128, 2016. Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2827–2836. JMLR. org, 2017. S´ebastien Racani `ere, Th ´eophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning. In Advances in neural information processing systems, pp. 5690–5701, 2017. Samuel Ritter. Meta-Reinforcement Learning with Episodic Recall: An Integrative Theory of Reward-Driven Learning. PhD thesis, Princeton University, 2019. Samuel Ritter, Jane X Wang, Zeb Kurth-Nelson, and Matthew M Botvinick. Episodic control as meta-reinforcement learning. bioRxiv, pp. 360537, 2018a. Samuel Ritter, Jane X Wang, Zeb Kurth-Nelson, Siddhant M Jayakumar, Charles Blundell, Razvan Pascanu, and Matthew Botvinick. Been there, done that: Meta-learning with episodic recall. arXiv preprint arXiv:1805.09692, 2018b. Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018. Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima- tors. In International Conference on Machine Learning, pp. 1312–1320, 2015. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019. Marwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep neural networks and symbolic ai. Nature, 555(7698):604, 2018. Richard S Sutton and A Barto. Introduction to reinforcement learning, volume 2. MIT press Cam- bridge, 1998. Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsuper- vised sensorimotor interaction. In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2, pp. 761–768. International Foundation for Autonomous Agents and Multiagent Systems, 2011. Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Advances in Neural Information Processing Systems, pp. 2154–2162, 2016. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. 11Published as a conference paper at ICLR 2021 Oliver Vikbladh, Daphna Shohamy, and Nathaniel Daw. Episodic contributions to model-based reinforcement learning. In Annual conference on cognitive computational neuroscience, CCN, 2017. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. JX Wang, Z Kurth-Nelson, D Tirumala, H Soyer, JZ Leibo, R Munos, C Blundell, D Kumaran, and M Botivnick. Learning to reinforcement learn. arxiv 1611.05763, 2017. Greg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka Grabska- Barwinska, Jack Rae, Piotr Mirowski, Joel Z Leibo, Adam Santoro, et al. Unsupervised predictive memory in a goal-directed agent. arXiv preprint arXiv:1803.10760, 2018. 12Published as a conference paper at ICLR 2021 A A PPENDIX A.1 O NE-SHOT STREET LEARN Dataset preparation – The StreetLearn dataset Mirowski et al. (2019) provides a connectivity graph between nodes, and panoramic images (panos) for each node. As movement in the environment is limited to turning left, turning right, or moving forward to the next node, we only need from the panoramic images the frames corresponding to what an observer would see in a given location, when oriented towards a next possible location. This deﬁnes a mapping from oriented edge in the full StreetLearn graph to a single frame. We compute it ahead of time and store it in a data structure enabling efﬁcient random accesses. Observation associated to an edge – As our agent operates on a reduced graph containing only degree-2 nodes, we deﬁne the observation to an oriented edge A–B in the reduced graph to be the image associated to the ﬁrst edge in the shortest path from A to B on the full graph. Movement – When the agent moves forward, its new location becomes the neighbor it was oriented towards, and is reoriented towards the neighbor most compatible with its current orientation. Distributed learning setup – Each actor is assigned to a random city, out of 12 cities/regions in Europe: Amsterdam, Brussels, Dublin, Lisbon, Madrid, Moscow, Rome, Vienna, Warsaw, Paris North-East, Paris North-West and Paris South-West. A special evaluator actor, which does not send send trajectories to the learner, is assigned to a withheld region: Paris South-East. Sampling neighborhoods – In the beginning of a new episode, we start by sampling a random location in the region assigned to the actor – i.e. a random node of the region’s connectivity graph. This node will become the center-node of the new neighborhood and is added as the ﬁrst node of the sampled graph. We then proceed to traverse the connectivity graph, breadth ﬁrst, adding visited nodes to the sampled graph, until it contains the number of intersections required. An intersection is a node with degree greater than 2; the number of intersections is a parameter of environment that we can set for each experiment and it determines the difﬁculty (depth) of the planning problem. In our experiments we decided to remove all degree-2 nodes of the sampled graph. This manipulation allowed us to simplify the exploration problem substantially without reducing the planning difﬁculty. A.2 RL SETUP We used an actor-critic setup for all RL experiments reported in this paper, following the distributed training and V-Trace algorithm implementations described by Espeholt et al. (2018). The distributed agent consisted of 1000 actors that produced trajectories of experience on CPU, and a single learner running on a Tensor Processing Unit (TPU), which learned a policy πand a baseline V using mini- batches of actors’ experiences provided via a queue. The length of the actors’ trajectories is set to the unroll length of the learner. Training was done using the RMSprop optimization algorithm. Please see the table below for values of ﬁxed hyperparameters and intervals used for hyperparameter tuning. Hyperparameter Values Agent Mini-batch size [32, 128] Unroll length [10, 40] Entropy cost [ 1e−3, 1e−2] Discount γ [0.9, 0.95] RMSprop Learning rate [ 1e−5, 4e−4] Epsilon ϵ 1e−4 Momentum 0 Decay 0.99 Table 1: Hyperparameter values and tuning intervals used in RL experiments. 13Published as a conference paper at ICLR 2021 A.3 A RCHITECTURE DETAILS Vision – The visual system of our agents, which produced embeddings for state and goal from the raw pixel inputs, was identical to the one described in Espeholt et al. (2018), except ours was smaller. It comprised 3 residual-convolutional layers, each one with a single residual block, instead of two. The number of output channels in each layer was 16, 32 and 32. We used a smaller ﬁnal linear layer with 64 units. Planner – For the multi-head attention, queries, keys and values were produced with an embedding size of 64, using 1 to 4 attention heads. In our experiments, we did not observe a signiﬁcant beneﬁt from using more than a single attention head. The feedforward block of each attention step was a 2-layer MLP with 64 units per layer (shared row-wise). Both the self-attention block and the feedforward block were shared across iterations. After appending the state to the output of the ﬁnal attention iteration, we used another MLP (shared row-wise) consisting of 2-layers with 64 units. The output of this MLP applied to each row was then aggregated using a max pooling, feature-wise, operation. Policy network – The input to the policy network was the result of concatenating the output of the planner and the state-goal embedding pair (which can be seen as a skip connection). We used a 2-layer MLP with 64 units per layer followed by a ReLU and two separate linear layers to produced the policy logits (π) and the baseline (V). A.4 S CALABILITY We experimented with two variants of the update function described in Section 4. In the ﬁrst, all of the episodic memories attend to all of the others to produce a tensor with the same dimensions as the episodic memory. This variant, which we refer to as all-to-all (A2A), scales quadratically with the number of memories (and the number of steps per episode). The second version, which scales more favorably, takes thekmost recent memories from the episodic memory (M) as the initial belief state (B). On each self-attention iteration, each slot in B attends to each slot in M, producing a tensor with the same dimensions as B (i.e. k vectors), which then self-attend to one another to produce the next belief state. The idea behind this design is that the querying from the k vectors to the full memory can learn to select and summarize information to be composed via self-attention. The self-attention among the k vectors may then learn to compute relationships, e.g. state-state paths, using this condensed representation. The beneﬁt is that this architecture scales only linearly with the number of memories (N) in the episodic memory, because the quadratic time self-attention need only be applied over the k vectors. This approach is similar to the inducing points of Lee et al. (2018a). Because this scales with N ∗k rather than N2, we call this variant N-by-k (abbreviated, Nxk). Figure 5 compares the performance obtained on the Memory&Planning Game with the two archi- tecture variants, A2A and Nxk. With kset to one half of the original memory capacity (k=50), the Nxk agent recovers 92% of the performance of the A2A. This result indicates that we can indeed overcome the quadratic complexity and opens up the possibility of applying EPNs to problems with longer timescales. 14Published as a conference paper at ICLR 2021 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Training steps 1e9 0 5 10 15 20 25 30 Average reward 0.0 0.2 0.4 0.6 0.8 1.0 EPN (A2A)EPN (Nxk) Fraction of oracle Oracle with perfect information Optimal exploration within each task Figure 5: Comparison between architecture variants. The Nxk architecture variant, which scales linearly with the total number of memories, recovers 92% of the performance of the A2A variant, which scales quadratically. Likelihood 0.0 0.2 0.4 0.6 0.8 1.0 0 2 4 6 8 10 Optimization steps 0.0 0.5 1.0 1.5 2.0 2.5 Distance-to-goal loss 1e4 a) b) 4 iterations 5 iterations 6 iterations Figure 6: (a) Distance-to-goal decoding loss from the output of the planner (with frozen weights) after 1 to 6 iterations. Each additional iteration improves distance decoding. (b) The ability of EPNs to infer distance to goal expands with the number of self-attention iterations, even when that number goes beyond the number of iterations used during training of the EPN. 15",
      "references": [
        "Toward jumpy planning",
        "Agent57: Outperforming the atari human benchmark",
        "Machine learning for combinatorial optimization: a methodological tour d’horizon",
        "Model-free episodic control",
        "Universal successor features approximators",
        "Reinforcement learning, fast and slow",
        "Model-based influences on humans’ choices and striatal prediction errors",
        "Improving generalization for temporal difference learning: The successor representation",
        "Rl2: Fast reinforcement learning via slow reinforcement learning",
        "Go-explore: a new approach for hard-exploration problems",
        "Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures",
        "Generalization of reinforcement learners with working and episodic memory",
        "Structure in the space of value functions",
        "From language to goals: Inverse reinforcement learning for vision-based instruction following",
        "Reinforcement learning and episodic memory in humans and animals: an integrative framework",
        "Shaping belief states with generative environment models for rl",
        "An investigation of model-free planning",
        "Cognitive mapping and planning for visual navigation",
        "Learning to navigate the web",
        "Attention, learn to solve routing problems!",
        "Building machines that learn and think like people",
        "Set transformer: A framework for attention-based permutation-invariant neural networks",
        "Gated path planning networks",
        "Hippocampal contributions to control: the third way",
        "Learning to navigate in cities without a map",
        "The streetlearn environment and dataset",
        "Model-based reinforcement learning: A survey",
        "Control of memory, active perception, and action in minecraft",
        "Neural episodic control",
        "Imagination-augmented agents for deep reinforcement learning",
        "Meta-Reinforcement Learning with Episodic Recall: An Integrative Theory of Reward-Driven Learning",
        "Episodic control as meta-reinforcement learning",
        "Been there, done that: Meta-learning with episodic recall",
        "Semi-parametric topological memory for navigation",
        "Universal value function approximators",
        "Mastering atari, go, chess and shogi by planning with a learned model",
        "Planning chemical syntheses with deep neural networks and symbolic ai",
        "Introduction to reinforcement learning, volume 2",
        "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
        "Value iteration networks",
        "Attention is all you need",
        "Episodic contributions to model-based reinforcement learning",
        "Matching networks for one shot learning",
        "Learning to reinforcement learn",
        "Unsupervised predictive memory in a goal-directed agent"
      ],
      "meta_data": {
        "arxiv_id": "2006.03662v3",
        "authors": [
          "Sam Ritter",
          "Ryan Faulkner",
          "Laurent Sartran",
          "Adam Santoro",
          "Matt Botvinick",
          "David Raposo"
        ],
        "published_date": "2020-06-05T20:09:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces the challenge of rapid task-solving in novel environments (RTS) and presents two novel domains—the minimal Memory&Planning Game and One-Shot StreetLearn Navigation—to study this problem. It proposes the Episodic Planning Network (EPN), a new architecture that enables agents to learn exploration and planning policies through meta-reinforcement learning, outperforming prior deep RL agents.",
        "methodology": "The approach treats RTS as a meta-reinforcement learning problem where agents learn exploration and planning policies over a distribution of tasks. The key technique is the Episodic Planning Network (EPN), which combines an episodic memory module with an iterative self-attention-based planner and a policy network. This architecture enables agents to implicitly compute a value iteration-like planning algorithm over stored experiences.",
        "experimental_setup": "Experiments are conducted in two domains: a symbolic 4x4 grid Memory&Planning Game and pixel-based One-Shot StreetLearn navigation tasks derived from Google StreetView data. The setup benchmarks performance against baselines (e.g., LSTM, Merlin, MRA) by measuring metrics such as goals achieved per episode, path optimality relative to an oracle, and generalization to larger unseen maps. Distributed training with actor-critic methods using IMPALA further validates the results.",
        "limitations": "Potential limitations include the quadratic computational scaling of the full self-attention mechanism in EPNs, which may require architectural modifications (e.g., the Nxk variant) for longer episodes or larger memory. Additionally, while the architecture shows strong performance in the tested domains, it may still face challenges in more diverse or dynamic real-world scenarios and highly complex planning tasks.",
        "future_research_directions": "Future work could explore temporally abstract or 'jumpy' planning to handle longer time scales, extend planning over belief states for dynamic and partially-observed environments, and apply the method to broader task distributions. Further research may also investigate alternative scalable self-attention mechanisms and test the EPN framework in more varied real-world applications.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Soft Augmentation for Image Classification",
      "full_text": "Soft Augmentation for Image Classification Yang Liu, Shen Yan, Laura Leal-Taixé, James Hays, Deva Ramanan Argo AI youngleoel@gmail.com, shenyan@google.com, leal.taixe@tum.de, hays@gatech.edu, deva@cs.cmu.edu Abstract Modern neural networks are over-parameterized and thus rely on strong regularization such as data augmenta- tion and weight decay to reduce overfitting and improve generalization. The dominant form of data augmentation applies invariant transforms, where the learning target of a sample is invariant to the transform applied to that sam- ple. We draw inspiration from human visual classifica- tion studies and propose generalizing augmentation with invariant transforms to soft augmentation where the learn- ing target softens non-linearly as a function of the de- gree of the transformapplied to the sample: e.g., more ag- gressive image crop augmentations produce less confident learning targets. We demonstrate that soft targets allow for more aggressive data augmentation, offer more robust performance boosts, work with other augmentation poli- cies, and interestingly, produce better calibrated models (since they are trained to be less confident on aggressively cropped/occluded examples). Combined with existing ag- gressive augmentation strategies, soft targets 1) double the top-1 accuracy boost across Cifar-10, Cifar-100, ImageNet- 1K, and ImageNet-V2, 2) improve model occlusion perfor- mance by up to 4×, and 3) half the expected calibration error (ECE). Finally, we show that soft augmentation gen- eralizes to self-supervised classification tasks. Code avail- able at https://github.com/youngleox/soft_ augmentation 1. Introduction Deep neural networks have enjoyed great success in the past decade in domains such as visual understanding [42], natural language processing [5], and protein structure pre- diction [41]. However, modern deep learning models are often over-parameterized and prone to overfitting. In addi- tion to designing models with better inductive biases, strong regularization techniques such as weight decay and data augmentation are often necessary for neural networks to achieve ideal performance. Data augmentation is often a computationally cheap and effective way to regularize mod- els and mitigate overfitting. The dominant form of data aug- mentation modifies training samples with invariant trans- forms – transformations of the data where it is assumed that the identity of the sample is invariant to the transforms. Indeed, the notion of visual invariance is supported by evidence found from biological visual systems [54]. The robustness of human visual recognition has long been docu- mented and inspired many learning methods including data augmentation and architectural improvement [19, 47]. This paper focuses on the counterpart of human visual robust- ness, namely how our vision fails. Instead of maintaining perfect invariance, human visual confidence degrades non- linearly as a function of the degree of transforms such as occlusion, likely as a result of information loss [44]. We propose modeling the transform-induced information loss for learned image classifiers and summarize the contribu- tions as follows: • We propose Soft Augmentation as a generalization of data augmentation with invariant transforms. With Soft Aug- mentation, the learning target of a transformed training sample softens. We empirically compare several soften- ing strategies and prescribe a robust non-linear softening formula. • With a frozen softening strategy, we show that replac- ing standard crop augmentation with soft crop augmenta- tion allows for more aggressive augmentation, and dou- bles the top-1 accuracy boost of RandAugment [8] across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2. • Soft Augmentation improves model occlusion robustness by achieving up to more than 4× Top-1 accuracy boost on heavily occluded images. • Combined with TrivialAugment [37], Soft Augmentation further reduces top-1 error and improves model calibra- tion by reducing expected calibration error by more than half, outperforming 5-ensemble methods [25]. • In addition to supervised image classification models, Soft Augmentation also boosts the performance of self- supervised models, demonstrating its generalizability. arXiv:2211.04625v2  [cs.CV]  23 Jan 2024-32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty Top-1 Error: 20.80 Standard Hard Crop -32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty 22.99(+2.19) Aggressive Hard Crop -32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty 18.31(−2.49) Soft Augmentation 0 1 0 1 0 1 Target Confidence (p) original image 77% visible  38% visible  22% visible Figure 1. Traditional augmentation encourages invariance by requiring augmented samples to produce the same target label; we visualize the translational offset range (tx, ty) of Standard Hard Crop augmentations for 32 × 32 images from Cifar-100 on the left, reporting the top-1 error of a baseline ResNet-18. Naively increasing the augmentation range without reducing target confidence increases error (middle), but softening the target label by reducing the target confidence for extreme augmentations reduces the error ( right), allowing for training with even more aggressive augmentations that may even produce blank images. Our work also shows that soft augmentations produce models that are more robust to occlusions (since they encounter larger occlusions during training) and models that are better calibrated (since they are trained to be less-confident on such occluded examples). 2. Related Work 2.1. Neural Networks for Vision Since the seminal work from Krizhevskyet al. [24], neu- ral networks have been the dominant class of high per- forming visual classifiers. Convolutional Neural Networks (CNNs) are a popular family of high performing neural models which borrows a simple idea of spatially local com- putations from biological vision [12, 18, 26]. With the help of architectural improvements [15], auxiliary loss [42], and improved computational power [13], deeper, larger, and more efficient neural nets have been developed in the past decade. 2.2. Data Augmentation Data augmentation has been an essential regularizer for high performing neural networks in many domains includ- ing visual recognition. While many other regularization techniques such as weight decay [32] and batch normal- ization [4] are shown to be optional, we are aware of no competitive vision models that omit data augmentation. Accompanying the influential AlexNet model, Krizhevsky et al . [24] proposed horizontal flipping and random cropping transforms which became the back- bone of image data augmentation. Since the repertoire of invariant transformations has grown significantly in the past decade [42], choosing which subset to use and then finding the optimal hyperparameters for each transform has become computationally burdensome. This sparked a line of research [7, 28] which investigates optimal policies for data augmentation such as RandAugment [8] and TrivialAugment [37]. 2.3. Learning from Soft Targets While minimizing the cross entropy loss between model logits and hard one-hot targets remains the go-to recipe for supervised classification training, learning with soft targets has emerged in many lines of research. Label Smooth- ing [36, 43] is a straightforward method which applies a fixed smoothing (softening) factor α to the hard one-hot classification target. The motivation is that label smoothing prevents the model from becoming over-confident. Müller et al. [36] shows that label smoothing is related to knowl- edge distillation [17], where a student model learns the soft distribution of a (typically) larger teacher model. A related line of research [49,53] focuses on regularizing how a model interpolates between samples by linearly mix- ing two or more samples and linearly softening the result- ing learning targets. Mixing can be in the form of per-pixel blending [53] or patch-level recombination [49]. 2.4. Robustness of Human Vision Human visual classification is known to be robust against perturbations such as occlusion. In computer vision re- search, the robustness of human vision is often regarded as the gold standard for designing computer vision mod- els [34, 54]. These findings indeed inspire development of robust vision models, such as compositional, recurrent, and occlusion aware models [22,46,47]. In addition to specialty models, much of the idea of using invariant transforms toaugment training samples come from the intuition and ob- servation that human vision are robust against these trans- forms such as object translation, scaling, occlusion, photo- metric distortions, etc. Recent studies such as Tang et al. [44] indeed confirm the robustness of human visual recognition against mild to moderate perturbations. In a 5-class visual classifica- tion task, human subjects maintain high accuracy when up to approximately half of an object is occluded. However, the more interesting observation is that human performance starts to degenerate rapidly as occlusion increases and falls to chance level when the object is fully occluded (see Figure 2 right k = 2, 3, 4 for qualitative curves). 3. Soft Augmentation In a typical supervised image classification setting, each training image xi has a ground truth learning target yi asso- ciated to it thus forming tuples: (xi, yi), (1) where xi ∈ RC×W×H denotes the image and yi ∈ [0, 1]N denotes a N-dimensional one-hot vector representing the target label (Figure 2 left, “Hard Target”). As modern neural models have the capacity to memorize even large datasets [1], data augmentation mitigates the issue by hal- lucinating data points through transformations of existing training samples. (Hard) data augmentation relies on the key underlying assumption that the augmented variant of xi should main- tain the original target label yi: (xi, yi) ⇒ (tϕ∼S(xi), yi) , Hard Augmentation (2) where tϕ∼S(xi) denotes the image transform applied to sample xi, ϕ is a random sample from the fixed transform range S. Examples of image transforms include transla- tion, rotation, crop, noise injection, etc. As shown by Tang et al. [44], transforms of xi such as occlusion are approxi- mately perceptually invariant only whenϕ is mild. Hence S often has to be carefully tuned in practice, since naively in- creasing it can lead to degraded performance (Figure 1). In the extreme case of 100% occlusion, total information loss occurs, making it detrimental for learning. Label Smoothing applies a smoothing function g to the target label yi parameterized by a handcrafted, fixed smoothing factor α. Specifically, label smoothing replaces the indicator value ‘1’ (for the ground-truth class label) with p = 1 − α, distributing the remaining α probability mass across all other class labels (Figure 2 left, “Soft Target”). One can interpret label smoothing as accounting for the average loss of information resulting from averaging over transforms from the range S. From this perspective, the smoothing factor α can be written as a function of the fixed transform range S: (xi, yi) ⇒ \u0000 tϕ∼S(xi), gα(S)(yi) \u0001 , Label Smoothing (3) Soft Augmentation, our proposed approach, can now be described succinctly as follows: replace the fixed smoothing factor α(S) with an adaptive smoothing factor α(ϕ), that depends on the degree of thespecific sampled augmentation ϕ applied to xi: (xi, yi) ⇒ \u0000 tϕ∼S(xi), gα(ϕ)(yi) \u0001 , Soft Augmentation (Target) (4) Crucially, conditioning on the information loss from a par- ticular ϕ allows one to define far larger augmentation ranges S. We will show that such a strategy consistently produces robust performance improvements with little tun- ing across a variety of datasets, models, and augmentation strategies. Extensions to Soft Augmentation may be proposed by also considering loss reweighting [40, 48], which is an al- ternative approach for softening the impact of an augmented example by down-weighting its contribution to the loss. To formalize this, let us write the training samples of a super- vised dataset as triples including a weight factor wi (that is typically initialized to all ‘1’s). One can then re-purpose our smoothing function g to modify the weight instead of (or in addition to) the target label (Figure 2 left): (xi, yi, wi) ⇒ \u0000 tϕ∼S(xi), yi, gα(ϕ)(wi) \u0001 , Soft Augmentation (Weight) (5) (xi, yi, wi) ⇒ \u0000 tϕ∼S(xi), gα(ϕ)(yi), gα(ϕ)(wi) \u0001 . Soft Augmentation (Target & Weight) (6) Finally, one may wish to soften targets by exploit- ing class-specific confusions when applying α(ϕ); the smoothed target label of a highly-occluded truck example could place more probability mass on other vehicle classes, as opposed to distributing the remaining probability equally across all other classes. Such extensions are discussed in Section 5. 4. Experiments 4.1. Soft Augmentation with Crop As a concrete example of the proposed Soft Augmenta- tion, we consider the crop transform t(tx,ty,w,h)(x). In the case of 32×32 pixel Cifar images [23], the cropped images typically have a constant sizew = h = 32, and t(x) is fully parameterized by tx and ty, which are translational offsets1 0 0 1 0 0 0.1 0.1 0.6 0.1 0.1 0.1 0.1 0.6 0.1 0.1 0.6 x 0 0 1 0 0 0.6 x Hard Target Soft Weight Soft Target Soft Target & Weight 1.0 x 1.0 x Weight One-Hot Target  Variants of Soft Augmentation Variant 0.0 0.5 1.0 Proportion Visible (v) 0.0 pmin 1 - α 1.0 Target Confidence (p) Softening Curves chance k=1 k=2 k=3 k=4 LS Figure 2. Variants of Soft Augmentation as prescribed by Equations 4 (Soft Target), 5 (Soft Weight), 6 (Soft Target & Weight) with example target confidence p = 0.6 (left). Soft Augmentation applies non-linear (k = 2, 3, 4, ...) softening to learning targets based on the specific degree of occlusion of a cropped image (Equation 7), which qualitatively captures the degradation of human visual recognition under occlusion [44]. Label Smoothing applies a fixed softening factor α to the one-hot classification target. between the cropped and the original image. As shown in Figure 1 (left), the standard hard crop augmentation for the Cifar-10/100 classification tasks drawstx, tyindependently from a uniform distribution of a modest range U(−4, 4). Under this distribution, the minimal visibility of an image is 77% and ResNet-18 models trained on the Cifar-100 task achieve mean top-1 validation error of 20.80% across three independent runs (Figure 1 left). Naively applying aggres- sive hard crop augmentation drawn from a more aggressive range U(−16, 16) increases top-1 error by 2.19% (Figure 1 middle). We make two changes to the standard crop aug- mentation. We first propose drawing tx, tyindependently from a scaled normal distribution S∗ ∼N(0, σL) (with clipping such that |tx| < L,|ty| < L), where L is the length of the longer edge of the image ( L = 32 for Cifar). The distri- bution has zero mean and σ controls the relative spread of the distribution hence the mean occlusion level. Following the 3σ rule of normal distribution, an intuitive tuning-free choice is to set σ ≈ 0.3, where ∼ 99% of cropped samples have visibility ≥ 0. Figure 3 (left, α = 0) shows that chang- ing the distribution alone without target softening provides a moderate ∼ 0.4% performance boost across crop strength σ. Directly borrowing the findings from human vision re- seach [44], one can define an adaptive softeningα(tx, ty, k) that softens the ground truth learning target. Similar to La- bel Smoothing [43], a hard target can be softened to confi- dence p ∈ [0, 1]. Instead of a fixed α, consider a family of power functions that produces target hardness p given crop parameters tx, tyand curve shape k: p = 1 −α(tx, ty, k) = 1 −(1 −pmin)(1 −v(tx,ty))k, (7) where v(tx,ty) ∈ [0, 1] is the image visibility which is a function of tx and ty. The power function family is a simple one-parameter formulation that allows us to test both linear (k = 1) and non-linear (k ̸= 1) softening: higher k provides flatter plateaus in high visibility regime (see Figure 2 right). 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Top-1 Error Reduction (%) Label Smoothing baseline α=0 α=0.001 α=0.01 α=0.1 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Target baseline k=1 k=2 k=3 k=4 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Weight baseline k=1 k=2 k=3 k=4 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Target & Weight baseline k=1 k=2 k=3 k=4 Figure 3. Soft Augmentation reduces the top-1 validation error of ResNet-18 on Cifar-100 by up to 2.5% via combining both target and weight softening (Equation 6). Applying target softening alone (Equation 4) can boost performance by ∼ 2%. Crop parameters tx, ty are independently drawn from N(0, σL) (L = 32). Higher error reductions indicate better performance over baseline. All results are the means and standard errors across 3 independent runs.As seen in Label Smoothing, p can be interpreted as ground truth class probability of the one-hot learning target. pmin is the chance probability depending on the task prior. For example, for Cifar-100, pmin = 1 #classes = 0.01. Equation 7 has three assumptions: 1) the information loss is a function of image visibility and all information is lost only when the image is fully occluded, 2) the original label of a training image has a confidence of 100%, which suggests that there is no uncertainty in the class of the label, and 3) the information loss of all images can be approx- imated by a single confidence-visibility curve. While the first assumption is supported by observations of human vi- sual classification research [44], and empirical evidence in Sections 4.2 and 4.3 suggests that the second and the third assumptions approximately hold, the limitations to these as- sumptions will be discussed in Section 5. 4.2. How to Soften Targets As prescribed by Equations 4, 5, and 6, three versions of Soft Augmentation are compared with Label Smoothing across a range of crop strength σ. The popular ResNet-18 models [16] are trained on the 100-class classification Cifar- 100 training set. Top-1 error reductions on the validation set are reported (details in Appendix B). Consistent with prior studies, label smoothing can boost model performance by ∼ 1.3% when the smoothing factor α is properly tuned (Figure 3 left). Combining both target and weight softening (Equation 6) with k = 2 and σ ≈ 0.3 boosts model performance by 2.5% (Figure 3 right). Note that k = 2 qualitatively re- sembles the shape of the curve of human visual confidence degradation under occlusion reported by Tang et al. [44]. Interestingly, the optimal σ ≈ 0.3 fits the intuitive 3-σ rule. In the next section we freeze k = 2 and σ = 0 .3 and show robust improvements that generalize to Cifar-10 [23], ImageNet-1K [9], and ImageNet-V2 tasks [39]. 4.3. Supervised Classification 4.3.1 Comparison with Related Methods As mentioned in Section 2, many approaches similar to soft augmentation have demonstrated empirical perfor- mance gains, including additional data augmentation trans- forms [10], learning augmentation policies [8], softening learning targets [43], and modifying loss functions [29]. However, as training recipes continued to evolve over the past decade, baseline model performance has improved ac- cordingly. As seen in Table 1 (Baseline), our baseline ResNet-18 models with a 500-epoch schedule and cosine learning rate decay [33] not only outperform many recent baseline models of the same architecture, but also beat var- ious published results of Mixup and Cutout. To ensure fair comparisons, we reproduce 6 popular methods: Mixup, Table 1. Soft augmentation outperforms related methods. Optimal hyperparameters for Mixup [53], Cutout [10], and Online Label Smoothing [52] were applied. α of Focal Loss is tuned as [29] did not prescribe an optimal α for Cifar classification. It is worth not- ing that our baseline model (20.80%) not only outperforms other published baseline models by 1.5% to 4.8%, but also beat various implementations of Mixup and Cutout. Top-1 errors of ResNet-18 on Cifar-100 are reported. ResNet-18 Top-1 Error Baseline Zhanget al. [53] 25.6 DeVries and Taylor [10]22.46±0.31 Kimet al. [20] 23.59 Ours 20.80±0.11 Mixup Zhanget al. [53] 21.1 Kimet al. [20] 22.43 Ours 19.88±0.38 Cutout DeVries and Taylor [10]21.96±0.24 Ours 20.51±0.02 Label Smoothing Ours 19.47±0.18 Online Label Smoothing 20.12±0.05 Focal Loss (α= 1) 20.45±0.08 Focal Loss (α= 2) 20.38±0.08 Focal Loss (α= 5) 20.69±0.17 RandAugment 20.99±0.11 Soft Augmentation 18.31±0.17 Cutout, Label Smoothing, Online Label Smoothing, Focal Loss, and RandAugment, and report the Top-1 Error on Cifar-100 in Table 1. Additional comparisons with the self- reported results are available in Appendix Table 5. Table 1 shows that Soft Augmentation outperforms all other single methods. It is worth noting that although focal loss [29] proposed for detection tasks, it can be tuned to slightly improve classification model performance. 4.3.2 Soft Augmentation Compliments RandAugment This section investigates the robustness of Soft Augmen- tation across models and tasks, and how well it compares or complements more sophisticated augmentation policies such as RandAugment [8]. The ImageNet-1K dataset [9] has larger and variable-sized images compared to the Ci- far [23] datasets. In contrast with the fixed-sized crop augmentation for Cifar, a crop-and-resize augmentation t(tx,ty,w,h)(x) with random location tx, tyand random size w, his standard for ImageNet training recipes [7,8,42]. The resizing step is necessary to produce fixed-sized training images (e.g. 224 × 224). We follow the same σ = 0 .3 principle for drawing tx, tyand w, h(details in Appendix B). Comparing single methods, Soft Augmentation with crop only consistently outperforms the more sophisticated RandAugment with 14 transforms (Table 2). The small ResNet-18 models trained with SA on Cifar-10/100 even outperforms much larger baseline ResNet-50 [39] andTable 2. Soft Augmentation (SA) with a fixed softening curve of k = 2 doubles the top-1 error reduction of RandAugment (RA) across datasets and models. Note that the ResNet-18 models trained with SA on Cifar-10/100 even outperform larger baseline ResNet-50 and WideResNet-28 models. All results are mean ± standard error of top-1 validation error in percentage. Best results are shown in bold, runners-up are underlined, and results in parentheses indicate improvement over baseline. Statistics are computed from three runs. Dataset Model Baseline SA RA SA+RA Cifar100 EfficientNet-B0 49.70±1.55 42.13±0.45(−7.57) 46.68±1.52(−3.02) 38.72±0.71(−10.98) ResNet-18 20.80±0.11 18.31±0.17(−2.49) 20.99±0.11(+0.19) 18.10±0.20(−2.70) ResNet-50 20.18±0.30 18.06±0.24(−2.12) 18.57±0.09(−1.61) 16.72±0.06(−3.46) WideResNet-28 18.60±0.19 16.47±0.18(−2.13) 17.65±0.14(−0.95) 15.37±0.17(−3.23) PyramidNet + ShakeDrop15.77±0.17 14.03±0.05(−1.75) 14 .02±0.28(−1.76) 12.78±0.16(−2.99) Cifar10 EfficientNet-B0 17.73±0.69 12.21±0.22(−5.52) 14.54±0.47(−3.19) 11.67±0.26(−6.06) ResNet-18 4.38±0.05 3.51±0.08(−0.87) 3.89±0.06(−0.49) 3.27±0.08(−1.11) ResNet-50 4.34±0.14 3.67±0.08(−0.67) 3.91±0.14(−0.43) 3.01±0.02(−1.33) WideResNet-28 3.67±0.08 2.85±0.02(−0.82) 3.26±0.04(−0.41) 2.45±0.03(−1.20) PyramidNet + ShakeDrop 2.86±0.03 2.26±0.02(−0.60) 2.32±0.08(−0.54) 2.02±0.01(−0.84) ImageNet-1K ResNet-50 22.62±<0.01 21.66±0.02(−0.96) 22.02±0.02(−0.60) 21.27±0.05(−1.35) ResNet-101 20.91±0.04 20.63±0.03(−0.28) 20 .39±0.07(−0.52) 19.86±0.03(−1.05) ImageNet-V2 ResNet-50 34.97±0.03 33.32±0.10(−1.65) 34.16±0.21(−0.81) 32.38±0.16(−2.59) ResNet-101 32.68±0.04 31.81±0.16(−0.87) 32.08±0.19(−0.60) 31.26±0.12(−1.42) WideResNet-28 [50] models. Because RandAugment is a searched policy that is orig- inally prescribed to be applied in addition to the standard crop augmentation [8], one can easily replace the standard crop with soft crop and combine Soft Augmentation and RandAugment. As shown in Table 2, Soft Augmentation complements RandAugment by doubling its top-1 error re- duction across tasks and models. Note that for the small ResNet-18 model trained on Cifar-100, the fixed RandAugment method slightly de- grades its performance. Consistent with observations from Cubuk et al. [8], the optimal hyperparameters for RandAug- ment depend on the combination of model capacity and task complexity. Despite the loss of performance of applying RandAugment alone, adding Soft Augmentation reverses the effect and boosts performance by 2.7%. For the preceding experiments, a fixed k = 2 is used for Soft Augmentation and the official PyTorch RandomAug- ment [38] is implemented to ensure a fair comparison and to evaluate robustness. It is possible to fine-tune the hyperpa- rameters for each model and task to achieve better empirical performance. 4.3.3 Occlusion Robustness As discussed in Section 2, occlusion robustness in both hu- man vision [34,44,54] and computer vision [22,46,47] have been an important property for real world applications of vi- sion models as objects. To assess the effect of soft augmen- tation on occlusion robustness of computer vision models, ResNet-50 models are tested with occluded ImageNet vali- dation images (Figure 4 and Appendix Figure 7).224×224 validation images of ImageNet are occluded with randomly placed square patches that cover λ of the image area. λ is set to {0%, 20%, 40%, 60%, 80%} to create a range of oc- clusion levels. As shown in Figure 5, both RandAugment (RA) and Soft Augmentation (SA) improve occlusion robustness indepen- dently across occlusion levels. Combining RA with SA re- duces Top-1 error by up to 17%. At 80% occlusion level, SA+RA achieves more than 4× accuracy improvement over the baseline (18.98% vs 3.42%). 4.3.4 Confidence Calibration In addition to top-1 errors, reliability is yet another impor- tant aspect of model performance. It measures how close a model’s predicted probability (confidence) tracks the true correctness likelihood (accuracy). Expected Calibration Er- ror (ECE) is a popular metric [14, 25, 35] to measure con- fidence calibration by dividing model predictions into M confidence bins (Bm) and compute a weighted average er- ror between accuracy and confidence: ECE = MX m=1 |Bm| n |acc(Bm) − conf(Bm)|, (8) where n is the number of samples, acc(Bm) denotes the accuracy of bin m, and conf(Bm) denotes mean model confidence of bin m. Consistent with Guo et al. [14], we set M = 10 and compute ECE for Cifar-10 and Cifar-100 tasks. As shown in Table 3, many methods [25,30,35,45] have been proposed to improve confidence calibration, some- times at the cost of drastically increased computational overhead [25], or degraded raw performance [30]. We show in Table 3 (and Appendix Table 7) that it is possible to fur- ther reduce model top-1 error and expected calibration error simultaneously.Occlusion: 0% wreckBaselineSA+RA Class Prediction Probability wreck 1.00 wreck 0.98 Class Prediction Probability liner 0.00 liner 0.00 Class Prediction Probability dock 0.00 dock 0.00 Class Prediction Probability pirate 0.00 submarine 0.00 Class Prediction Probability submarine 0.00 pirate 0.00 Occlusion: 20% Class Prediction Probability wreck 0.95 wreck 0.95 Class Prediction Probability steel_arch_bridge 0.01 dock 0.01 Class Prediction Probability pier 0.01 submarine 0.01 Class Prediction Probability liner 0.01 liner 0.00 Class Prediction Probability crane 0.00 paddlewheel 0.00 Occlusion: 40% Class Prediction Probability crane 0.28 dock 0.20 Class Prediction Probability web_site 0.18 boathouse 0.10 Class Prediction Probability beacon 0.07 paddlewheel 0.08 Class Prediction Probability pier 0.06 pier 0.08 Class Prediction Probability envelope 0.04 wreck 0.07 Occlusion: 60% Class Prediction Probability web_site 0.78 dock 0.18 Class Prediction Probability crane 0.06 liner 0.16 Class Prediction Probability beacon 0.05 submarine 0.05 Class Prediction Probability seashore 0.01 crane 0.04 Class Prediction Probability envelope 0.01 container_ship 0.03 Occlusion: 80% Class Prediction Probability beacon 0.34 liner 0.07 Class Prediction Probability web_site 0.22 dock 0.03 Class Prediction Probability crane 0.07 aircraft_carrier 0.03 Class Prediction Probability seashore 0.06 container_ship 0.03 Class Prediction Probability liner 0.02 schooner 0.02 Figure 4. Examples of occluded ImageNet validation images and model predictions of ResNet-50. 224 × 224 validation images of ImageNet are occluded with randomly placed square patches that cover λ of the image area. λ is set to {0%, 20%, 40%, 60%, 80%} to create a range of occlusion levels. 0 20 40 60 80 Occlusion Level (%) 0 20 40 60 80Top-1 Accuracy (%) baseline RA SA RA+SA 0 20 40 60 80 Occlusion Level (%) 0 20 40 60 80Top-5 Accuracy (%) baseline RA SA RA+SA Figure 5. Soft Augmentation improves occlusion robustness of ResNet-50 on ImageNet. Both RandAugment (RA) and Soft Augmentation (SA) improve occlusion robustness independently. Combining RA with SA reduces Top-1 error by up to 17%. At 80% occlusion level, compared with baseline accuracy (3.42%), SA+RA achieves more than 4× accuracy (18.98%). Compared to previous single-model methods, our strong baseline WideResNet-28 models achieves lower top-1 er- ror at the cost of higher ECE. Combining Soft Augmen- tation with more recently developed augmentation policies such as TrivialAugment [37] (SA+TA) reduces top-1 error by 4.36% and reduces ECE by more than half on Cifar-100, outperforming the 4× more computationally expensive 5- ensemble model [25]. To the best of our knowledge, this is state of the art ECE performance for WideResNet-28 on Cifar without post-hoc calibration. 4.4. Soft Augmentation Boosts Self-Supervised Learning In contrast with supervised classification tasks where the learning target yi is usually a one-hot vector, many self-supervised methods such as SimSiam [6] and Barlow Twins [51] learn visual feature representations without class labels by encouraging augmentation invariant feature rep- resentations. This section investigates whether Soft Aug- Table 3. Soft Augmentation improves both accuracy and calibra- tion. We report mean and standard error of three WideResNet-28 runs per configuration (bottom two rows). On the more challeng- ing Cifar-100 benchmark, our Baseline already outperforms much of prior work in terms of Top-1 error, but has worse calibration er- ror (ECE). Applying Soft Augment + Trivial Augment (SA+TA) reduces Top-1 error by 4.36% and reduces ECE by more than half, outperforming even compute-heavy models such as the 5- Ensemble [25]. Similar trends hold for Cifar-10. Method Cifar-100 Cifar-10 Top-1 Error ECE Top-1 Error ECE Energy-based [31] 19.74 4.62 4.02 0.85 DUQ [45] – – 5.40 1.55 SNGP [30] 20.00 4.33 3.96 1.80 DDU [35] 19.02 4.10 4.03 0.85 5-Ensemble [25] 17.21 3.32 3.41 0.76 Our Baseline 18.60±0.16 4.86±0.10 3.67±0.07 2.22±0.03 SA+TA 14.24±0.11 1.76±0.15 2.23±0.06 0.61±0.10 mentation generalizes to learning settings where no one-hot style labeling is provided. In a typical setting, two random crops of the same image are fed into a pair of identical twin networks (e.g., ResNet- 18) with shared weights and architecture. The learning tar- get can be the maximization of similarity between the fea- ture representations of the two crops [6], or minimization of redundancy [51]. By default, all randomly cropped pairs have equal weights. We propose and test two alternative hypotheses for weight softening with SimSiam. To accom- modate self-supervised learning, Equation 7 is modified by replacing visibility vtx,ty with intersection over union IoU of two crops of an image: p = 1 −α(ϕ1, ϕ2, k) = 1 −(1 −pmin)(1 −IoUϕ1,ϕ2 )k, SA#1 (9) where ϕ1 = (tx1, ty1, w1, h1) and ϕ2 = (tx2, ty2, w2, h2) are crop parameters for the first and second sample in a pair.Table 4. Soft Augmentation (SA#1) improves self supervised learning with SimSiam (ResNet-18) on Cifar-100 by down- weighting sample pairs with small intersection over union (IoU), outperforming the opposite hypothesis (SA#2) of down-weighting pairs with large IoU. For each configuration, we report means and standard errors of 3 runs with best learning rates (LR) found for Cifar-100. The effect of SA#1 (with a fixed k = 4) generalizes to Cifar-10 without re-tuning. Task LR Baseline LR SA#1 LR SA#2 Cifar100 0.2 37.64±0.06 0.2 36.61±0.05 0.1 37.39±0.06 Cifar10 0.2 9.87±0.03 0.2 9.31±0.01 - - p is used to soften weights only as no one-hot classification vector is available in this learning setting. With this hypoth- esis (SA#1), “hard\" sample pairs with low IoUs are assigned low weights. Alternatively, one can assign lower weights to “easy\" sample pairs with higher IoUs (SA#2), as prescribed by Equation 10: p = 1 − α(ϕ1, ϕ2, k) = 1 − (1 − pmin)(IoUϕ1,ϕ2 )k. SA#2 (10) We first test all three hypotheses (baseline, SA#1, and SA#2) on Cifar-100 with the SimSiam-ResNet-18 models. Table 4 (top) shows that SA#1 outperform both baseline and SA#2 (details in Appendix B.4). Additional experiments show that models trained with the same SA#1 configuration also generalize to Cifar-10 (Table 4 bottom). 5. Discussion Other augmentations. While we focus on crop aug- mentations as an illustrative example, Soft Augmentation can be easily extended to a larger repertoire of transforms such as affine transforms and photometric distortions, as seen in the more sophisticated augmentation policies such as RandAugment. As the formulation of Equation 7 (and Figure 2 right) is directly inspired by the qualitative shape of human vision experiments from Tanget al. [44], optimal softening curves for other transforms may be discovered by similar human experiments. However, results with a sec- ond transform in Appendix Table 6 suggest that Equation 7 generalizes to additive noise augmentation as well. A po- tential challenge is determining the optimal softening strat- egy when a combination of several transforms are applied to an image since the cost of a naive grid search increases ex- ponentially with the number of hyperparameters. Perhaps reinforcement learning methods as seen in RandAugment can be used to speed up the search. Other tasks. While we limit the scope of Soft Augmen- tation to image classification as it is directly inspired by hu- man visual classification research, the idea can be general- ized to other types of tasks such as natural language mod- eling and object detection. Recent studies have shown that detection models benefit from soft learning targets in the fi- nal stages [3,27], Soft Augment has the potential to comple- ment these methods by modeling information loss of image transform in the models’ input stage. Class-dependant augmentations. As pointed out by Balestriero et al. [2], the effects of data augmentation are class-dependent. Thus assumption 3 of Equation 7 does not exactly hold. One can loosen it by adaptively determining the range of transform and softening curve on a per class or per sample basis. As shown in Equation 11, (xi, yi) ⇒ \u0000 tϕ∼S(xi,yi)(xi), gα(ϕ,xi,yi)(yi) \u0001 , (11) two adaptive improvements can be made: 1) the transforma- tion range S where ϕ is drawn from can be made a function of sample (xi, yi), 2) the softening factorα can also adapt to (xi, yi). Intuitively, the formulation recognizes the hetero- geneity of training samples of images at two levels. Firstly, the object of interest can occupy different proportions of an image. For instance, a high-resolution training image with a small object located at the center can allow more ag- gressive crop transforms without losing its class invariance. Secondly, texture and shape may contribute differently de- pending on the visual class. A heavily occluded tiger may be recognized solely by its distinctive stripes; in contrast, a minimally visible cloak can be mistaken as almost any clothing. 6. Conclusion In summary, we draw inspiration from human vision re- search, specifically how human visual classification perfor- mance degrades non-linearly as a function of image occlu- sion. We propose generalizing data augmentation with in- variant transforms to Soft Augmentation where the learning target (e.g. one-hot vector and/or sample weight) softens non-linearly as a function of the degree of the transform ap- plied to the sample. Using cropping transformations as an example, we em- pirically show that Soft Augmentation offers robust top-1 error reduction across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2. With a fixed softening curve, Soft Aug- mentation doubles the top-1 accuracy boost of the popular RandAugment method across models and datasets, and im- proves performance under occlusion by up to 4×. Combin- ing Soft Augment with the more recently developed Triv- ialAugment further improves model accuracy and calibra- tion simultaneously, outperforming even compute-heavy 5- ensemble models. Finally, self-supervised learning exper- iments demonstrate that Soft Augmentation also general- izes beyond the popular supervised one-hot classification setting.References [1] Devansh Arpit, Stanisław Jastrz˛ ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning , pages 233– 242. PMLR, 2017. 3 [2] Randall Balestriero, Leon Bottou, and Yann LeCun. The effects of regularization and data augmentation are class de- pendent. arXiv preprint arXiv:2204.03632, 2022. 8 [3] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S Davis. Soft-nms–improving object detection with one line of code. In Proceedings of the IEEE international conference on computer vision, pages 5561–5569, 2017. 8 [4] Andy Brock, Soham De, Samuel L Smith, and Karen Si- monyan. High-performance large-scale image recognition without normalization. In International Conference on Ma- chine Learning, pages 1059–1071. PMLR, 2021. 2 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural in- formation processing systems, 33:1877–1901, 2020. 1 [6] Xinlei Chen and Kaiming He. Exploring simple siamese rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 15750–15758, 2021. 7, 14 [7] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 113–123, 2019. 2, 5 [8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703, 2020. 1, 2, 5, 6, 12 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5 [10] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 5 [11] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 12 [12] Kunihiko Fukushima and Sei Miyake. Neocognitron: A self- organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neu- ral nets, pages 267–285. Springer, 1982. 2 [13] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 2 [14] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321–1330. PMLR, 2017. 6 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 2 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. 5, 11 [17] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. 2 [18] David H Hubel and Torsten N Wiesel. Receptive fields of single neurones in the cat’s striate cortex. The Journal of physiology, 148(3):574, 1959. 2 [19] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in neural informa- tion processing systems, 28, 2015. 1 [20] Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with super- modular diversity. arXiv preprint arXiv:2102.03065, 2021. 5, 12 [21] Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puz- zle mix: Exploiting saliency and local statistics for optimal mixup. In International Conference on Machine Learning , pages 5275–5285. PMLR, 2020. 12 [22] Adam Kortylewski, Ju He, Qing Liu, and Alan L Yuille. Compositional convolutional neural networks: A deep archi- tecture with innate robustness to partial occlusion. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8940–8949, 2020. 2, 6 [23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 3, 5 [24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. Advances in neural information processing systems , 25, 2012. 2 [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estima- tion using deep ensembles. Advances in neural information processing systems, 30, 2017. 1, 6, 7 [26] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015. 2 [27] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Advances in Neural Information Processing Systems, 33:21002–21012, 2020. 8 [28] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32, 2019. 2 [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017. 5, 12[30] Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Advances in Neural Infor- mation Processing Systems, 33:7498–7512, 2020. 6, 7 [31] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems , 33:21464–21475, 2020. 7 [32] Yang Liu, Jeremy Bernstein, Markus Meister, and Yisong Yue. Learning by turning: Neural architecture aware optimi- sation. In International Conference on Machine Learning , pages 6748–6758. PMLR, 2021. 2 [33] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas- tic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 5 [34] David G Lowe. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE international conference on computer vision, volume 2, pages 1150–1157. Ieee, 1999. 2, 6 [35] Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H. S. Torr, and Yarin Gal. Deep deterministic uncer- tainty: A simple baseline, 2021. 6, 7 [36] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in neural in- formation processing systems, 32, 2019. 2 [37] Samuel G Müller and Frank Hutter. Trivialaugment: Tuning- free yet state-of-the-art data augmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 774–782, 2021. 1, 2, 7 [38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im- perative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Informa- tion Processing Systems 32, pages 8024–8035. Curran Asso- ciates, Inc., 2019. 6, 12 [39] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to im- agenet? In International Conference on Machine Learning, pages 5389–5400. PMLR, 2019. 5, 12 [40] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urta- sun. Learning to reweight examples for robust deep learn- ing. In International conference on machine learning, pages 4334–4343. PMLR, 2018. 3 [41] Andrew W Senior, Richard Evans, John Jumper, James Kirk- patrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Im- proved protein structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020. 1 [42] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1–9, 2015. 1, 2, 5 [43] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception archi- tecture for computer vision. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 2818–2826, 2016. 2, 4, 5 [44] Hanlin Tang, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35):8835–8840, 2018. 1, 3, 4, 5, 6, 8 [45] Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deep de- terministic neural network. In International conference on machine learning, pages 9690–9700. PMLR, 2020. 6, 7 [46] Angtian Wang, Yihong Sun, Adam Kortylewski, and Alan L Yuille. Robust object detection under occlusion with context- aware compositionalnets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12645–12654, 2020. 2, 6 [47] Dean Wyatte, Tim Curran, and Randall O’Reilly. The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded. Journal of Cognitive Neuroscience, 24(11):2248–2261, 2012. 1, 2, 6 [48] Mingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Zhi-Ming Ma. Reweighting augmented samples by minimizing the maximal expected loss. arXiv preprint arXiv:2103.08933, 2021. 3 [49] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu- larization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international con- ference on computer vision, pages 6023–6032, 2019. 2 [50] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. arXiv preprint arXiv:1605.07146, 2016. 6, 11 [51] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Ma- chine Learning, pages 12310–12320. PMLR, 2021. 7 [52] Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei, Qi Han, Zhen Li, and Ming-Ming Cheng. Delving deep into label smoothing. IEEE Transactions on Image Process- ing, 30:5984–5996, 2021. 5, 12 [53] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 2, 5, 12 [54] Hongru Zhu, Peng Tang, Jeongho Park, Soojin Park, and Alan Yuille. Robustness of object recognition under ex- treme occlusion in humans and computational models.arXiv preprint arXiv:1905.04598, 2019. 1, 2, 6Appendix A. Implementation 1import torch 2 3class SoftCropAugmentation: 4def __init__(self, n_class, sigma=0.3, k=2): 5self.chance = 1/n_class 6self.sigma = sigma 7self.k = k 8 9def draw_offset(self, limit, sigma=0.3, n =100): 10# draw an integer from a (clipped) Gaussian 11for d in range(n): 12x = torch.randn((1))*sigma 13if abs(x) <= limit: 14return int(x) 15return int(0) 16 17def __call__(self, image, label): 18# typically, dim1 = dim2 = 32 for Cifar 19dim1, dim2 = image.size(1), image.size(2) 20# pad image 21image_padded = torch.zeros((3, dim1 * 3, dim2 * 3)) 22image_padded[:, dim1:2*dim1, dim2:2*dim2] = image 23# draw tx, ty 24tx = self.draw_offset(dim1, self. sigma_crop * dim1) 25ty = self.draw_offset(dim2, self. sigma_crop * dim2) 26# crop image 27left, right = tx + dim1, tx + dim1 * 2 28top, bottom = ty + dim2, ty + dim2 * 2 29new_image = image_padded[:, left: right, top: bottom] 30# compute transformed image visibility and confidence 31v = (dim1 - abs(tx)) * (dim2 - abs(ty)) / (dim1 * dim2) 32confidence = 1 - (1 - self.chance) * (1 - v) ** self.k 33return new_image, label, confidence Listing 1. Pytorch implementation of Soft Crop Augmentation for Cifar. 1import torch 2import torch.nn.functional as F 3 4def soft_target(pred, label, confidence): 5log_prob = F.log_softmax(pred, dim=1) 6n_class = pred.size(1) 7# make soft one-hot target 8one_hot = torch.ones_like(pred) * (1 - confidence) / (n_class - 1) 9one_hot.scatter_(dim=1, index=label, src= confidence) 10# compute weighted KL loss 11kl = confidence * F.kl_div(input=log_prob, 12target=one_hot, 13reduction=’none’). sum(-1) 14return kl.mean() Listing 2. Pytorch implementation of Soft Target loss function. Appendix B. Experiment Details Appendix B.1. Supervised Cifar-10/100 For Cifar-100 experiments, we train all ResNet-like models with a batch size 128 on a single Nvidia V100 16GB GPU on Amazon Web Services (AWS) and with an intial learning rate 0.1 with cosine learning rate decay over 500 epochs. EfficientNet-B0 is trained with an initial learning rate of 0.025, PyramidNet-272 is trained with 2 GPUs. We use the Conv-BatchNorm-ReLU configuration of ResNet models [16] and WideResNet-28 with a widening factor of 10 [50]. Horizontal flip is used in all experiments as it is considered a lossless transformation in the context of Ci- far images. We find decaying crop aggressiveness towards the end of training (e.g., last 20 epochs) by a large fac- tor (e.g., reducing σ by 1000×) marginally improve per- formance on Cifar-100, but slightly hurts performance on Cifar-10. Accordingly, we only apply σ decay for all Cifar- 100 experiments. A single run of ResNet-18, ResNet-50, and WideResNet-28 takes ∼ 2.5, ∼ 7, ∼ 9 GPU hours on Cifar-10/100, respectively. BL beaver 0.30 SA+TA seal 0.44 BL pear 0.43 SA+TA apple 0.79 BL television 0.12 SA+TA dinosaur 0.37 BL skyscraper 0.18 SA+TA castle 0.44 BL kangaroo 0.35 SA+TA rabbit 0.79 BL poppy 0.83 SA+TA beetle 0.27 BL poppy 0.74 SA+TA sunflower 0.89 BL clock 0.48 SA+TA ray 0.74 BL girl 0.51 SA+TA boy 0.83 BL crab 0.52 SA+TA crocodile 0.95 BL rabbit 0.56 SA+TA mouse 0.68 BL butterfly 0.28 SA+TA skunk 0.88 BL beetle 0.12 SA+TA flatfish 0.16 BL mouse 0.17 SA+TA lizard 0.98 BL forest 0.37 SA+TA pine_tree 0.56 BL skunk 0.68 SA+TA elephant 0.78 BL hamster 0.35 SA+TA raccoon 0.17 BL bee 0.84 SA+TA caterpillar 0.95 BL cattle 0.10 SA+TA kangaroo 0.31 BL cattle 0.24 SA+TA lion 0.89 Figure 6. Example images of the Cifar-100 validation set and pre- dictions of WideResNet-28. Predicted classes and confidence lev- els of models trained with Soft Augmentation + Trivial Augment (SA+TA) and baseline (BL) augmentation are reported. In many cases, SA+TA not only corrects the class prediction, but also im- proves the model confidence. For instance, BL mistakes “seal” for “beaver” (top-left, both classes belong to the same “aquatic mammal” superclass), and SA+TA makes a correct class predic- tion with higher confidence.Appendix B.2. Additional Results Table 5. Comparing SA with other methods. Recommended hyperparameters for Mixup [53], Cutout [11], and Online Label Smoothing [52]. α of Focal Loss is tuned as Lin et al. [29] did not prescribe an optimal α for Cifar classification. Top-1 errors of ResNet-18 on Cifar-100 are reported. ResNet-18 Top-1 Error Zhanget al. [53] Baseline 25.6 Mixup 21.1 Kimet al. [21] Baseline 23.67 Mixup 23.16 Manifold Mixup 20.98 Puzzle Mix 19.62 Kimet al. [20] Baseline 23.59 Mixup 22.43 Manifold Mixup 21.64 Puzzle Mix 20.62 Co-Mixup 19.87 Our Baseline 20.80±0.11 Label Smoothing 19.47±0.18 Online Label Smoothing20.12±0.05 Focal Loss (α= 1) 20.45±0.08 Focal Loss (α= 2) 20.38±0.08 Focal Loss (α= 5) 20.69±0.17 Mixup (α= 1.0) 19.88±0.38 Cutout (L= 8) 20.51±0.02 SA 18.31±0.17 RA 20.99±0.11 SA + RA 18.10±0.20 Table 6. Soft Augmentation with additive noise improves ResNet- 18 performance on Cifar-100. Given an image X and a random noise pattern Xnoise, and augmented image is given by Xaug = X + αXnoise, where α is drawn from N(0, 0.1) and pixel values of Xnoise are also independently drawn from N(0, 0.1). Apply- ing Soft Augmentation to additive noise boost performance over baseline as well as Soft Augmentation Crop + RandAugment. ResNet-18 Top-1 Error Baseline 20.80±0.11 RA 20.99±0.11 Hard Crop 20.26±0.12 SA-Crop (k=2) 18.31±0.17 Hard Noise 20.68±0.05 SA-Noise (k=1) 19.20±0.20 SA-Crop (k=2) + RA 18.10±0.20 SA-Noise (k=1) + SA-Crop (k=2) + RA17.87±0.17 Table 7. Soft Augmentation reduces expected calibration error (ECE) of ResNet-50 on ImageNet. Dataset Baseline RA SA RA+SA ImageNet-1K 5.11 4.09 3.17 2.78 ImageNet-V2 9.91 8.84 3.24 3.18 Appendix B.3. ImageNet All ImageNet-1k experiments are conducted with a batch size of 256 distributed across 4 Nvidia V100 16GB GPUs on AWS. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 dataset (BSD 3- Clause License) is downloaded from the official website (https://www.image-net.org/). Horizontal flip is used in all experiments as an additional lossless base augmentation. The base learning rate is set to 0.1 with a 5-epoch linear warmup and cosine decay over 270 epochs. A single run of ResNet-50 training takes ∼ 4 × 4 = 16 GPU days and ImageNet experiments take a total of 600 GPU days. We use the official PyTorch [38] implementation of Ran- dAugment and ResNet-50/101 (BSD-style license) and run all experiments with the standard square input Linput = W = H = 224. Note that the original RandAugment [8] uses a larger input size of H = 224 , W = 244 , but our re-implemention improved top-1 error (22.02 vs 22.4) of ResNet-50 despite using a smaller input size. ImageNet-V2 is a validation set proposed by He et al. [39]. For training, the standard crop transform has 4 hy- perparameters: (scalemin, scalemax) define the range of the relative size of a cropped image to the original one, (ratiomin, ratiomax) determine lower and upper bound of the aspect ratio of the cropped patch before the final resize step. In practice, a scale is drawn from a uni- form distribution U(scalemin, scalemax), then the loga- rithm of the aspect ratio is drawn from a uniform dis- tribution U(log(ratiomin), log(ratiomax)). Default val- ues are scalemin = 0 .08, scalemax = 1 .0, ratiomin = 3/4, ratiomax = 4/3. Similar to our Cifar crop augmentation, we propose a simplified ImageNet crop augmentation with only 2 hy- perparameters σ, Lmin. First, we draw ∆w, ∆h from a clipped rectified normal distribution NR(0, σ(L − Lmin)) and get w = W − ∆w, h= H − ∆h Lmin is the mini- mum resolution of a cropped image and set to half of input resolution 224. tx, tyare then independently drawn from N(0, σ(W +w)), N(0, σ(H +h)). Note that we use a fixed set of intuitive values σ = 0.3, Lmin = 1/2Linput = 112 for all the experiments. For model validation, standard augmentation practice first resizes an image so that its short edge has length Linput = 256 , then a center 224 × 224 crop is applied. Note that Linput is an additional hyperparameter introduced by the test augmentation. In contrast, we simplify this by setting Linput to the final input size 224 and use this con- figuration for all ImageNet model evaluation.Occlusion: 0% Boston_bullBaselineSA+RA Class Prediction Probability Boston_bull 0.82 Boston_bull 0.65 Class Prediction Probability French_bulldog 0.09 French_bulldog 0.23 Class Prediction Probability toy_terrier 0.03 toy_terrier 0.01 Class Prediction Probability tennis_ball 0.01 Chihuahua 0.00 Class Prediction Probability Chihuahua 0.00 pug 0.00 Occlusion: 20% Class Prediction Probability Boston_bull 0.95 Boston_bull 0.86 Class Prediction Probability French_bulldog 0.05 French_bulldog 0.13 Class Prediction Probability toy_terrier 0.00 toy_terrier 0.00 Class Prediction Probability Chihuahua 0.00 Chihuahua 0.00 Class Prediction Probability tennis_ball 0.00 pug 0.00 Occlusion: 40% Class Prediction Probability French_bulldog 0.83 Boston_bull 0.96 Class Prediction Probability Boston_bull 0.17 French_bulldog 0.04 Class Prediction Probability Staffordshire_bullterrier 0.00 toy_terrier 0.00 Class Prediction Probability American_Staffordshire_terrier0.00 Italian_greyhound 0.00 Class Prediction Probability Great_Dane 0.00 Chihuahua 0.00 Occlusion: 60% Class Prediction Probability Boston_bull 0.97 Boston_bull 0.80 Class Prediction Probability French_bulldog 0.01 French_bulldog 0.02 Class Prediction Probability Chihuahua 0.01 Italian_greyhound 0.02 Class Prediction Probability whippet 0.00 toy_terrier 0.02 Class Prediction Probability toy_terrier 0.00 Chihuahua 0.01 Occlusion: 80% Class Prediction Probability French_bulldog 0.88 Boston_bull 0.13 Class Prediction Probability Boston_bull 0.06 French_bulldog 0.06 Class Prediction Probability Chihuahua 0.01 Italian_greyhound 0.03 Class Prediction Probability pug 0.01 miniature_pinscher 0.02 Class Prediction Probability Staffordshire_bullterrier 0.01 Staffordshire_bullterrier 0.01 Occlusion: 0% papillonBaselineSA+RA Class Prediction Probability papillon 1.00 papillon 0.97 Class Prediction Probability Chihuahua 0.00 Chihuahua 0.00 Class Prediction Probability Japanese_spaniel 0.00 Japanese_spaniel 0.00 Class Prediction Probability toy_terrier 0.00 toy_terrier 0.00 Class Prediction Probability Pomeranian 0.00 Pomeranian 0.00 Occlusion: 20% Class Prediction Probability papillon 0.74 papillon 0.76 Class Prediction Probability Blenheim_spaniel 0.09 Blenheim_spaniel 0.03 Class Prediction Probability clumber 0.06 Japanese_spaniel 0.03 Class Prediction Probability Japanese_spaniel 0.05 Chihuahua 0.02 Class Prediction Probability Welsh_springer_spaniel 0.02 Pomeranian 0.01 Occlusion: 40% Class Prediction Probability papillon 0.29 Blenheim_spaniel 0.15 Class Prediction Probability Blenheim_spaniel 0.19 papillon 0.13 Class Prediction Probability Welsh_springer_spaniel 0.14 Welsh_springer_spaniel 0.13 Class Prediction Probability collie 0.11 Brittany_spaniel 0.05 Class Prediction Probability Brittany_spaniel 0.05 hare 0.02 Occlusion: 60% Class Prediction Probability envelope 0.09 bustard 0.04 Class Prediction Probability Indian_cobra 0.03 hare 0.02 Class Prediction Probability hognose_snake 0.02 golf_ball 0.02 Class Prediction Probability web_site 0.02 partridge 0.02 Class Prediction Probability dhole 0.01 kit_fox 0.02 Occlusion: 80% Class Prediction Probability envelope 0.15 kit_fox 0.01 Class Prediction Probability web_site 0.08 hare 0.01 Class Prediction Probability dhole 0.04 bustard 0.01 Class Prediction Probability red_fox 0.01 partridge 0.01 Class Prediction Probability honeycomb 0.01 bittern 0.01 Occlusion: 0% vending_machineBaselineSA+RA Class Prediction Probability vending_machine 0.65 vending_machine 0.98 Class Prediction Probability streetcar 0.04 streetcar 0.00 Class Prediction Probability refrigerator 0.02 refrigerator 0.00 Class Prediction Probability minibus 0.01 pop_bottle 0.00 Class Prediction Probability grocery_store 0.01 ambulance 0.00 Occlusion: 20% Class Prediction Probability vending_machine 0.39 vending_machine 0.83 Class Prediction Probability moving_van 0.12 streetcar 0.04 Class Prediction Probability refrigerator 0.12 moving_van 0.01 Class Prediction Probability web_site 0.05 refrigerator 0.01 Class Prediction Probability monitor 0.04 trolleybus 0.01 Occlusion: 40% Class Prediction Probability vending_machine 0.88 vending_machine 0.93 Class Prediction Probability refrigerator 0.03 refrigerator 0.01 Class Prediction Probability web_site 0.01 pay-phone 0.00 Class Prediction Probability desktop_computer 0.00 slot 0.00 Class Prediction Probability monitor 0.00 streetcar 0.00 Occlusion: 60% Class Prediction Probability screen 0.26 streetcar 0.03 Class Prediction Probability monitor 0.18 garbage_truck 0.02 Class Prediction Probability home_theater 0.14 parking_meter 0.01 Class Prediction Probability television 0.04 cab 0.01 Class Prediction Probability desktop_computer 0.04 trolleybus 0.01 Occlusion: 80% Class Prediction Probability home_theater 0.59 sliding_door 0.04 Class Prediction Probability monitor 0.13 refrigerator 0.02 Class Prediction Probability entertainment_center 0.05 vending_machine 0.02 Class Prediction Probability television 0.05 pay-phone 0.01 Class Prediction Probability screen 0.04 barbershop 0.01 Occlusion: 0% pirateBaselineSA+RA Class Prediction Probability pirate 1.00 pirate 0.93 Class Prediction Probability schooner 0.00 fireboat 0.01 Class Prediction Probability dock 0.00 dock 0.01 Class Prediction Probability fireboat 0.00 schooner 0.01 Class Prediction Probability crane 0.00 amphibian 0.00 Occlusion: 20% Class Prediction Probability pirate 0.98 pirate 0.76 Class Prediction Probability schooner 0.02 schooner 0.03 Class Prediction Probability dock 0.00 cannon 0.03 Class Prediction Probability stupa 0.00 dock 0.01 Class Prediction Probability crane 0.00 fireboat 0.01 Occlusion: 40% Class Prediction Probability crane 0.28 pirate 0.72 Class Prediction Probability pirate 0.22 schooner 0.04 Class Prediction Probability moving_van 0.12 fireboat 0.03 Class Prediction Probability schooner 0.05 dock 0.01 Class Prediction Probability scoreboard 0.03 drilling_platform 0.01 Occlusion: 60% Class Prediction Probability crane 0.43 pirate 0.26 Class Prediction Probability bookshop 0.08 schooner 0.03 Class Prediction Probability scoreboard 0.07 toyshop 0.03 Class Prediction Probability schooner 0.05 suspension_bridge 0.02 Class Prediction Probability drilling_platform 0.03 bookshop 0.02 Occlusion: 80% Class Prediction Probability pole 0.10 toyshop 0.03 Class Prediction Probability book_jacket 0.09 carousel 0.02 Class Prediction Probability comic_book 0.09 shoe_shop 0.02 Class Prediction Probability envelope 0.07 bookshop 0.01 Class Prediction Probability binder 0.06 totem_pole 0.01 Occlusion: 0% military_uniformBaselineSA+RA Class Prediction Probability military_uniform 0.74 military_uniform 0.70 Class Prediction Probability mortarboard 0.07 suit 0.06 Class Prediction Probability suit 0.05 bow_tie 0.02 Class Prediction Probability academic_gown 0.03 crutch 0.02 Class Prediction Probability cornet 0.03 groom 0.01 Occlusion: 20% Class Prediction Probability suit 0.19 crutch 0.23 Class Prediction Probability mortarboard 0.09 suit 0.14 Class Prediction Probability military_uniform 0.06 groom 0.09 Class Prediction Probability notebook 0.04 military_uniform 0.04 Class Prediction Probability lab_coat 0.04 turnstile 0.02 Occlusion: 40% Class Prediction Probability military_uniform 0.22 military_uniform 0.22 Class Prediction Probability lab_coat 0.07 projectile 0.04 Class Prediction Probability file 0.07 warplane 0.02 Class Prediction Probability bearskin 0.05 missile 0.02 Class Prediction Probability suit 0.05 grand_piano 0.01 Occlusion: 60% Class Prediction Probability military_uniform 0.11 military_uniform 0.14 Class Prediction Probability suit 0.10 lab_coat 0.02 Class Prediction Probability envelope 0.06 cowboy_hat 0.02 Class Prediction Probability kimono 0.06 rifle 0.02 Class Prediction Probability abaya 0.05 trombone 0.02 Occlusion: 80% Class Prediction Probability abaya 0.15 crutch 0.04 Class Prediction Probability space_heater 0.13 mortarboard 0.01 Class Prediction Probability web_site 0.13 academic_gown 0.01 Class Prediction Probability window_shade 0.12 trombone 0.01 Class Prediction Probability shower_curtain 0.04 shoe_shop 0.01 Figure 7. Examples of occluded ImageNet validation images and model predictions of ResNet-50.Appendix B.4. Self-Supervised Cifar-10/100 Self-supervised SimSiam experiments are run on a sin- gle Nvidia A6000 GPU. We follow the standard two-step training recipe [6]. 1) We first train the Siamese network in a self-supervised manner to learn visual features for 500 epochs with a cosine decay schedule and a batch size of 512. We apply Soft Augmentation only during this step. 2) The linear layer is then tuned with ground-truth labels for 100 epochs with an initial learning rate of 10 and 10× de- cay at epochs 60 and 80. Following [6], we set scalemin = 0.2, scalemax = 1 .0, ratiomin = 3 /4, ratiomax = 4 /3. Since down-weighting training samples in a batch effec- tively reduces learning rate and SimSiam is sensitive to it, we normalized the weight in a batch so that the mean re- mains 1 and re-tuned the learning rate (Table 8). Table 8. Soft Augmentation improve self supervised learning with SimSiam. Mean ± standard error of top-1 validation errors of three runs of ResNet-18 are reported. Task lr baseline SA#1 ∆#1 SA#2 ∆#2 Cifar100 0.1 39.50±0.13 40.21±0.03 +0.71 37 .39±0.06 −2.11 0.2 37.64±0.06 36.61±0.05 −1.03 39 .20±0.42 +1.56 0.4 40.28±2.49 37.68±0.06 −2.60 Diverged - 0.5 43.26±3.03 41.94±0.04 −1.32 Diverged - 0.8 78.88±9.05 55.44±4.15 −23.44 Diverged - Cifar10 0.2 9.87±0.03 9.31±0.01 −0.56 - - Table 9. SimSiam k tuning on Cifar-100 (single run) learning rate k Top-1 Error 0.2 1 37.78 2 37.27 3 36.34 4 36.31 Appendix C. Effects of Target Smoothing and Loss Reweighting on Loss Func- tions Consider the KL divergence loss of a single learning sample with a one-hot ground truth vector ytrue, and the softmax prediction vector of a model is denoted by ypred: L(ypred, ytrue) = w ∗ DKL(ytrue||ypred) =w ∗ NX n=1 ytrue n ∗ log(ytrue n ypred n ), (12) let n∗ be the ground truth class of an N-class classifica- tion task, Equation 12 can be re-written as: L(ypred, ytrue) = −w ∗ ytrue n∗ ∗ log(ypred n∗ ) + w ∗  ytrue n∗ ∗ log(ytrue n∗ ) + X n̸=n∗ ytrue n ∗ log(ytrue n ypred n )  . (13) In the case of hard one-hot ground truth target where ytrue n∗ = 1 and ytrue n = 0, n̸= n∗, with the default weight w = 1 it degenerates to cross entropy loss: L(ypred, ytrue) = −log(ypred n∗ ), (14) Now we apply label smoothing style softening to the one-hot target ytrue so that ytrue n∗ = p and ytrue n = (1 − p)/(N − 1) = q, n̸= n∗: L(ypred, ytrue) = −p ∗ log(ypred n∗ ) +  p ∗ log(p) + X n̸=n∗ q ∗ log( q ypred n )  . (15) If q is not distributed, and ytrue n = 0, n̸= n∗ (This con- figuration does not correspond to any of our experiments): L(ypred, ytrue) = −p ∗ log(ypred n∗ ) + p ∗ log(p), (16) When only weight w is softened to w = p: L(ypred, ytrue) = −p ∗ log(ypred n∗ ). (17) Note that p is not a function of model weights, so when we take the derivative w.r.t. model weights to compute gra- dient, Equations 16 and 17 yield the same gradient. When both the one-hot label and weight are softened with p: L(ypred, ytrue) = −p2 ∗ log(ypred n∗ ) + p ∗  p ∗ log(p) + X n̸=n∗ q ∗ log( q ypred n )  . (18) The three types of softening in Section 4 are unique as suggested by Equations 15, 17, and 18.",
      "references": [
        "A closer look at memorization in deep networks",
        "The effects of regularization and data augmentation are class de- pendent",
        "Soft-nms–improving object detection with one line of code",
        "High-performance large-scale image recognition without normalization",
        "Language models are few-shot learners",
        "Exploring simple siamese rep- resentation learning",
        "Autoaugment: Learning augmentation strategies from data",
        "Randaugment: Practical automated data augmentation with a reduced search space",
        "Imagenet: A large-scale hierarchical image database",
        "Improved regular- ization of convolutional neural networks with cutout",
        "Neocognitron: A self- organizing neural network model for a mechanism of visual pattern recognition",
        "Accurate, large mini- batch sgd: Training imagenet in 1 hour",
        "On calibration of modern neural networks",
        "Deep residual learning for image recognition",
        "Identity mappings in deep residual networks",
        "Distill- ing the knowledge in a neural network",
        "Receptive fields of single neurones in the cat’s striate cortex",
        "Spatial transformer networks",
        "Co-mixup: Saliency guided joint mixup with super- modular diversity",
        "Puzzle mix: Exploiting saliency and local statistics for optimal mixup",
        "Compositional convolutional neural networks: A deep architecture with innate robustness to partial occlusion",
        "Learning multiple layers of features from tiny images",
        "Imagenet classification with deep convolutional neural networks",
        "Simple and scalable predictive uncertainty estimation using deep ensembles",
        "Deep learning",
        "Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection",
        "Fast autoaugment",
        "Focal loss for dense object detection",
        "Simple and principled uncertainty estimation with deterministic deep learning via distance awareness",
        "Energy-based out-of-distribution detection",
        "Learning by turning: Neural architecture aware optimi- sation",
        "Sgdr: Stochas- tic gradient descent with warm restarts",
        "Object recognition from local scale-invariant features",
        "Deep deterministic uncer- tainty: A simple baseline",
        "When does label smoothing help?",
        "Trivialaugment: Tuning- free yet state-of-the-art data augmentation",
        "Pytorch: An im- perative style, high-performance deep learning library",
        "Do imagenet classifiers generalize to im- agenet?",
        "Learning to reweight examples for robust deep learning",
        "Improved protein structure prediction using potentials from deep learning",
        "Going deeper with convolutions",
        "Rethinking the inception archi- tecture for computer vision",
        "Recurrent computations for visual pattern completion",
        "Uncertainty estimation using a single deep deterministic neural network",
        "Robust object detection under occlusion with context- aware compositionalnets",
        "The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded",
        "Reweighting augmented samples by minimizing the maximal expected loss",
        "Cutmix: Regu- larization strategy to train strong classifiers with localizable features",
        "Wide residual networks",
        "Barlow twins: Self-supervised learning via redundancy reduction",
        "Delving deep into label smoothing",
        "mixup: Beyond empirical risk minimiza- tion",
        "Robustness of object recognition under ex- treme occlusion in humans and computational models"
      ],
      "meta_data": {
        "arxiv_id": "2211.04625v2",
        "authors": [
          "Yang Liu",
          "Shen Yan",
          "Laura Leal-Taixé",
          "James Hays",
          "Deva Ramanan"
        ],
        "published_date": "2022-11-09T01:04:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Soft Augmentation, a novel data augmentation strategy that adjusts the learning target (e.g., one‐hot label and/or sample weight) based on the degree of transformation applied to the image, such as aggressive cropping or occlusion. Inspired by human visual recognition degradation under occlusion, the method provides a non-linear softening function that leads to significant improvements in top-1 accuracy, robustness to occlusion, and better model calibration across several benchmarks. It also demonstrates that the approach generalizes to self-supervised learning scenarios.",
        "methodology": "The proposed approach replaces the traditional fixed, invariant target of data augmentation with an adaptive soft target that is a function of the transform’s severity. A power function parameterized by k is used to compute the target confidence based on the visibility of the transformed image. The method extends to soft augmentation of weights and can be applied in both supervised and self-supervised settings (e.g., by using IoU for crop pair comparison in self-supervised learning). Variants of the approach, including Soft Target, Soft Weight, and their combination, are experimentally evaluated.",
        "experimental_setup": "Experiments were conducted on popular image classification benchmarks such as CIFAR-10, CIFAR-100, ImageNet-1K, and ImageNet-V2, using models including ResNet-18, ResNet-50, WideResNet-28, and EfficientNet-B0. The training involved aggressive cropping (with parameters drawn from a scaled normal distribution) and comparisons with established augmentation methods like RandAugment and TrivialAugment over a schedule of up to 500 epochs. Additional tests measured occlusion robustness and expected calibration error (ECE), and experiments in self-supervised settings (e.g., SimSiam) were also conducted.",
        "limitations": "The method assumes that the information loss due to aggressive transforms (e.g., occlusion) can be adequately modeled by a single softening function, and it assumes a fixed initial confidence for the original one-hot labels. Extending the approach to a combination of multiple transformations may result in a significant increase in hyperparameter search complexity. Moreover, the assumption that a single softening curve works uniformly across all classes is a simplification that might not hold in practice, suggesting potential benefits for per-class or per-sample adaptive strategies.",
        "future_research_directions": "Future work could explore reinforcement learning or other automated methods to optimize the hyperparameters when using multiple complex transforms. Extending Soft Augmentation to other tasks such as natural language processing, object detection, and adversarial robustness is a promising direction. Additionally, developing class-dependent or sample-dependent softening strategies and evaluating alternative softening functions beyond the power function formulation could further improve the method's effectiveness.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Equivariance versus Augmentation for Spherical Images",
      "full_text": "Equivariance versus Augmentation for Spherical Images Jan E. Gerken 1 2 3 Oscar Carlsson 1 Hampus Linander 4 Fredrik Ohlsson 5 Christoffer Petersson 6 1 Daniel Persson 1 Abstract We analyze the role of rotational equivariance in convolutional neural networks (CNNs) applied to spherical images. We compare the perfor- mance of the group equivariant networks known as S2CNNs and standard non-equivariant CNNs trained with an increasing amount of data augmen- tation. The chosen architectures can be consid- ered baseline references for the respective design paradigms. Our models are trained and evaluated on single or multiple items from the MNIST or FashionMNIST dataset projected onto the sphere. For the task of image classiﬁcation, which is in- herently rotationally invariant, we ﬁnd that by considerably increasing the amount of data aug- mentation and the size of the networks, it is possi- ble for the standard CNNs to reach at least the same performance as the equivariant network. In contrast, for the inherently equivariant task of semantic segmentation, the non-equivariant networks are consistently outperformed by the equivariant networks with signiﬁcantly fewer pa- rameters. We also analyze and compare the in- ference latency and training times of the differ- ent networks, enabling detailed tradeoff consid- erations between equivariant architectures and data augmentation for practical problems. The equivariant spherical networks used in the ex- periments are available at https://github. com/JanEGerken/sem_seg_s2cnn. 1Department of Mathematical Sciences, Chalmers University of Technology, Gothenburg, Sweden 2Machine Learning Group at Berlin Institute of Technology, Berlin, Germany 3Berlin Insti- tute for the Foundations of Learning and Data (BIFOLD), Berlin, Germany 4Department of Physics, University of Gothenburg, Gothenburg, Sweden 5Department of Mathematics and Mathe- matical Statistics, Umeå University, Umeå, Sweden 6Zenseact, Gothenburg, Sweden. Correspondence to: Daniel Persson <daniel.persson@chalmers.se>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Figure 1: Sample from the spherical MNIST dataset used for semantic segmentation. Left: input data. Right: segmen- tation mask. 1. Introduction In virtually all computer vision tasks, convolutional neural networks (CNNs) consistently outperform fully connected architectures. This performance gain can be attributed to the weight sharing and translational equivariance of the CNN layers: The network does not have to learn to identify translated versions of an image, since the inductive bias of equivariance already implies this identiﬁcation. In contrast, fully connected layers require many more training samples to learn an effective form of equivariance. One way of ameliorating this problem is to supply the network with translated copies of the original training images, a form of data augmentation. However, training with this kind of data augmentation requires much longer training times and the performance of CNNs may not be reached in this way. A crucial point is that in the equivariant setting, we are viewing the input image not only as a vector, but as a signal deﬁned on a two-dimensional grid. The translational sym- metry then has a geometric origin: it is a symmetry of the grid. From this perspective it is natural to envision general- izations of CNNs where not only translational symmetries are implemented but also more general transformations, such as rotations. Examples of networks that realize this property are group equivariant CNNs which are equivariant with respect to rotations on the plane (Cohen & Welling, 2016), or spherical CNNs which are equivariant with respect to rotations of the sphere (Cohen et al., 2018). Similarly to the case of translations discussed above, these symmetry properties of the data can be learned approximately by a non-equivariant model using data augmentation. arXiv:2202.03990v2  [cs.LG]  12 Jul 2022Equivariance versus Augmentation for Spherical Images 0 50k 100k 150k 200k 250k 0 20 40 60 80 100 Number of training images mIoU (non-background) 204k S2CNN 218k CNN 1M CNN 5.5M CNN Figure 2: Semantic Segmentation on Spherical MNIST. The performance of equivariant (S2CNN) and non-equivariant (CNN) semantic segmentation models for various amounts of data augmented spherical MNIST single digit training images. Performance is measured in terms of mean intersec- tion over union (mIoU) for the non-background classes. The numbers in the model names refer to the number of train- able parameters. The non-equivariant models are trained on randomly rotated samples, whereas the equivariant models are trained on unrotated samples. In the present paper, we evaluate the performance of equiv- ariant classiﬁcation and semantic segmentation models (S2CNNs (Cohen et al., 2018)) on spherical image data subject to rotational symmetry (see Figure 1 for an exam- ple), and compare it to the performance obtained using rota- tional data augmentation in ordinary CNNs (see Figure 2). We conduct comparisons for several datasets to elucidate the inﬂuence of the type and complexity of the task on the performance. Our overall aim is to investigate whether equivariant architectures offer an inherent advantage over data augmentation in non-equivariant CNN models. Our choice of using an ordinary CNN, without additional structure to make it more compatible with spherical data, is motivated by the desire to isolate the limits of geometric data augmentation. While our present investigation is concerned with rotational equivariance in spherical data, group equivariant networks generalize to any homogeneous space with a global action of a symmetry transformation. In fact, the theoretical de- velopment in Section 2 applies to any homogeneous space, and the question of equivariance versus data augmentation is relevant also in the general setting. 1.1. Summary of contributions and outline Here we list the main contributions of the paper, both theo- retical and experimental. • We deﬁne a new group equivariant CNN layer (Equa- tion (2.5)), designed for the task of semantic segmen- tation for arbitrary Lie groups G. This generalizes previous results in (Linmans et al., 2018). • We extend the S2CNN architecture by adding a layer which allows for equivariant outputs on the sphere, as required for semantic segmentation tasks, and present a detailed proof of equivariance (Appendix A). • We demonstrate that non-equivariant classiﬁcation models require considerable data augmentation to reach the performance of smaller equivariant networks (Section 3.2). • We show that the performance of non-equivariant se- mantic segmentation models saturates well below that of equivariant models as the amount of data augmen- tation is increased (Section 3.3). We conﬁrm that this result still holds when the complexity of the original segmentation task is increased (Section 3.4). • We perform detailed inference time proﬁling of the GPU-based implementation of the equivariant models, and measure the throughput overhead compared to the non-equivariant models. Our results indicate that most inference time is spent in the last network layers pro- cessing the largest SO(3) tensors, suggesting possible avenues towards optimizing the S2CNN architecture (Section 3.6). • We show that the total training time for an equivariant model is shorter compared to a non-equivariant model at matched performance (Section 3.6). Appendix A contains mathematical details about our new ﬁnal layer used for semantic segmentation. Details of the model generation for the equivariant and non-equivariant networks can be found in Appendix B. Appendix C contains details of the datasets and augmentation. Finally, the latency proﬁling is summarized in Appendix D. 1.2. Related literature The theory of group equivariant neural networks was ﬁrst developed in (Kondor & Trivedi, 2018; Cohen et al., 2019; Esteves, 2020), generalizing the ordinary planar convolution of CNNs. An implementation for spherical data with a rotational SO(3) symmetry was introduced in (Cohen et al., 2018; Kondor et al., 2018; Cobb et al., 2021).Equivariance versus Augmentation for Spherical Images There are two main approaches to equivariance in semantic segmentation tasks. In the ﬁrst, one utilizes the methods available for ordinary ﬂat CNNs by modifying the shape of convolutional kernels to compensate for spherical distortion (Tateno et al., 2018), by cutting the sphere in smaller pieces or otherwise preprocessing the data to be able to apply an ordinary CNN without much distortion (Zhang et al., 2019; Lee et al., 2019; Haim et al., 2019; Eder & Frahm, 2019; Du et al., 2021; Shakerinava & Ravanbakhsh, 2021). In the second approach, one avoids spherical distortions by Fourier decomposing signals and ﬁlters on the sphere (Esteves et al., 2020). Semantic segmentation for group equivariant CNNs is analyzed in (Linmans et al., 2018). Data augmentation in the context of symmetric tasks was studied previously in (Gandikota et al., 2021), where a method to align input data is presented and compared to data augmentation. Closest to our work is a comparison between an equivariant model and a non-equivariant model for re- duced training data sizes for an MRI application in (Müller et al., 2021). In contrast, we systematically compare data augmentation with increased training data sizes for different tasks, datasets and several non-equivariant models with an equivariant architecture. 2. Theory 2.1. Group equivariant networks In this section we introduce the basic mathematical structure of GCNNs. Let Gbe a group and H ⊂Ga closed sub- group1. The input space of the network is the homogeneous space M= G/H, meaning that a feature map in the ﬁrst layer is a map f : G/H →RK, (2.1) where Kdenotes the number of channels. This feature map represents the input data, e.g. the pixel values of an image represented on the homogeneous space G/H. Features of subsequent layers are obtained using the convolution between the feature map f and a ﬁlter κ: G/H →RK′,K deﬁned by (κ⋆f )(g) = ∫ G/H κ(g−1x)f(x) dx, (2.2) where g ∈ G and dx is the invariant measure on G/H induced from the Haar measure on G. The action of Gextends from M= G/H to Gitself and the convolution (2.2) is equivariant with respect to this ac- tion, g(κ⋆f ) =κ⋆ (gf), g∈G, meaning that the result obtained by transforming the convoluted feature map κ⋆f by an element g ∈Gis identical to that obtained by con- 1In this paper, we will always consider the case where G is a Lie group and H is a compact subgroup. volving the transformed feature map gf. In other words, the transformation g∈Gcommutes with the convolution. The networks considered here all have scalar features throughout the architecture. In general, however, the group Gcan act through non-trivial representations on both the manifold G/H and the feature maps (see e.g. (Kondor & Trivedi, 2018; Cohen et al., 2019; Aronsson, 2021; Gerken et al., 2021)). Note that the output of the convolution (2.2) is a function on G, rather than on the homogeneous space G/H. Limiting feature maps to convolutions on the original manifold G/H is in general far too restrictive, as was ﬁrst noticed in the context of spherical signals by (Makadia et al., 2007). In order to keep the network as general as possible, and maxi- mize the expressiveness of each individual layer, subsequent convolutions will all be taken on the group G, (κ⋆f )(g) = ∫ G κ(g′−1g)f(g′) dg′, (2.3) g ∈G, except for the last one which will be discussed in more detail below. 2.2. Semantic segmentation and equivariance In the ﬁnal layers we must take into account the desired task that the network is designed to perform. If we are interested in a classiﬁcation task, we want the entire network to be invariant with respect to G. This can be achieved by integrating over Gfollowing the ﬁnal convolution fﬁnal = ∫ G f(g) dg . (2.4) For semantic segmentation, however, we are classifying individual pixels into a segmentation mask. The network output should therefore be equivariant with respect to trans- formations of the input. One way to achieve this aim is to deﬁne a ﬁnal convolution fﬁnal(x) = ∫ G κ(g′−1gx)f(g′) dg′, (2.5) where gx ∈Gis a representative in Gcorresponding to the point xin the coset spaceG/Hand κis required to be anH- invariant kernel on G. This generalizes the convolution de- ﬁned in (Linmans et al., 2018) for ﬁnite roto-translations on the plane. The output (2.5) is then a signal on G/H, rather than G, and equivariance ensures that a transformation by Gon the input image will result in the same transformation on the output. Alternatively, similarly to the classiﬁcation case, an output segmentation mask on G/H can be obtained by integrating along the orbits of the subgroup H fﬁnal(x) = ∫ H f(gxh) dh , (2.6)Equivariance versus Augmentation for Spherical Images ℓm n ℓ∑ n=−ℓ ℓ m L∑ ℓ=0 ℓ∑ m=−ℓ Yℓ m(x) Fourier transform SO(3) expansion coefﬁcients S2 expansion coefﬁcients (κ⋆f )ℓ mn S2 fﬁnal Figure 3: The ﬁnal layer (3.3) of our fully equivariant architecture takes Fourier coefﬁcients on SO(3) and sums them over nto yield Fourier coefﬁcients on the sphere, representing a segmentation mask. with gx deﬁned as above. This segmentation mask is equiv- ariant with respect to the action of Gby construction. 3. Data augmentation for classiﬁcation and semantic segmentation In this section, we investigate several aspects of the perfor- mance difference between equivariant and non-equivariant models, applied to both classiﬁcation and segmentation tasks on spherical images. Consequently, we consider input data in the form of grayscale images deﬁned on the sphere M= S2 with continuous rotational symmetry G= SO(3). Taking H = SO(2)to be the isotropy subgroup of a point x ∈S2, the sphere can be expressed as a homogeneous space S2 = SO(3)/SO(2). 3.1. Equivariant model As discussed in Section 2, the equivariant network architec- ture for spherical signals necessitates a redeﬁnition of the convolution compared to ordinary CNNs. For our imple- mentation we will use the S2CNN architecture of (Cohen et al., 2018), in the implementation available at (Köhler et al., 2021), adapted in the case of semantic segmentation.2 For S2CNNs the ﬁrst layer takes an input feature map f deﬁned on S2, convolves it with a kernelκdeﬁned on SO(3) and outputs a feature map deﬁned on SO(3) according to (κ⋆f )(R) = ∫ S2 κ(R−1x)f(x) dx , (3.1) where R∈SO(3). In subsequent layers, the input feature map f is also deﬁned on SO(3) and the convolution takes the form (κ⋆f )(R) = ∫ SO(3) κ(S−1R)f(S) dS. (3.2) In the S2CNN architecture, these convolutions are computed in the respective Fourier domain of S2 and SO(3). For S2, this amounts to an expansion in spherical harmonics Yℓ m; 2Our adaptation is available at https://github.com/ JanEGerken/sem_seg_s2cnn. for SO(3), to an expansion in Wigner matrices Dℓ mn with ℓ= 0,...,L and m,n = −ℓ...ℓ for some bandlimit L.3 The original S2CNN architecture introduced in (Cohen et al., 2018) was used for classiﬁcation tasks and hence in the last convolutional layer the feature map was integrated over SO(3) to render the output invariant under rotations of the input. We use the same setup for the classiﬁcation task discussed below. In contrast, for semantic segmentation, we need an equivari- ant network, as detailed in Section 2.2 above. To this end, instead of the Fourier-back-transform on SO(3), we use for the last convolution fﬁnal(x) = L∑ ℓ=0 ℓ∑ m=−ℓ ℓ∑ n=−ℓ (κ⋆f )ℓ mnYℓ m(x) , (3.3) where κ⋆f is as in (3.2) on SO(3). The sum over ncorre- sponds roughly to the Fourier space version of the integral over H presented in Equation (2.6) and ensures that the out- put fﬁnal lives on S2, as illustrated in Figure 3. We provide more mathematical details in Appendix A. After a softmax nonlinearity, we interpret fﬁnal as the pre- dicted segmentation mask and use a point-wise cross entropy loss for training. To facilitate the segmentation task, we use a sequence of downsampling SO(3) convolutions followed by a sequence of upsampling convolutions. These networks do not con- tain any fully connected layers. For the classiﬁcation task, we only use the downsampling layers, which are, after the integration over SO(3), followed by three fully connected layers, alternated with batch-normalization layers. The downsampling layers are realized in the S2CNN frame- work by choosing the bandlimit for the Fourier-back- transform to be lower than the bandlimit of the Fourier transform, i.e. we are dropping higher order modes in the result of the convolution. This is similar to strides used in 3Note that following the original S2CNN code (Cohen et al., 2018), we use Wigner matrices for the Fourier transform and com- plex conjugated Wigner matrices for the inverse Fourier transform, contrary to the usual convention.Equivariance versus Augmentation for Spherical Images downsampling layers of ordinary CNNs. Since the Fourier transforms of the feature maps are computed using the FFT algorithm, this also decreases the spatial resolution of the feature maps. For the upsampling layers, we select the Fourier-back- transform to have higher bandlimit than the Fourier trans- form. The missing Fourier modes are here ﬁlled with zeros, yielding a feature map of higher spatial resolution (but of course with still lower resolution in the Fourier domain). This is similar to upsampling by interpolation. To ﬁx the precise architectures and hyperparameters for the experiments without biasing for or against the equiv- ariant models, we generated architectures at random. For the equivariant models, we generated 20 models at random for each parameter range and selected the one performing best on a reference task, as detailed for the semantic seg- mentation models in Appendix B.2. This resulted in two equivariant models with 204k and 820k parameters, respec- tively for the semantic segmentation tasks and one model with 150k parameters for the classiﬁcation task. For the semantic segmentation tasks, we generated three non-equivariant models as a baseline with 218k, 1M and 5.5M parameters, respectively. Each of these performed best out of 20 randomly generated models with a ﬁxed parameter budget. The non-equivariant models have (ordi- nary) convolutional layers for downsampling and transpose convolutions for upsampling layers which mirror the im- age dimensions and channels of the downsampling layers exactly. We add skip connections over each (transpose) con- volution, resulting in a ResNet-like architecture. For details, cf. Appendix B.1. The precise architectures of the models used in our experiments are summarized in Tables 4 and 5 in the appendix. Similarly, for the classiﬁcation task, we generated three non-invariant models with 114k, 0.5M and 2.5M parame- ters, respectively. These models only have downsampling layers, followed by an average pooling layer and three fully connected layers alternated with batch-normalization layers. We did not use skip connections in this case. 3.2. Classiﬁcation The primary question we want to investigate in this work is whether data augmentation can make up for the beneﬁts of equivariant network architectures. We ﬁrst study this ques- tion in the context of classiﬁcation by training equivariant and non-equivariant models on training data with different amounts of data augmentation, as depicted in Figure 4. The input data consists of single digits sampled with replace- ment from the MNIST (Lecun et al., 1998) dataset which are projected onto the sphere and labeled with the classes of the digits. The spherical pictures are rotated by a random rota- 0 50k 100k 150k 200k 250k 80 85 90 95 100 Number of training images Test accuracy 150k S2CNN 114k CNN 0.5M CNN 2.5M CNN Figure 4: Image Classiﬁcation on Spherical MNIST . Ac- curacy of invariant and non-invariant classiﬁcation models for various amounts of data augmentation as in Figure 2. Training data for the equivariant model is sampled randomly with replacement from the available 60k training samples. tion matrix in SO(3). A sample is depicted in the left panel of Figure 1. For dataset larger than the 60k original MNIST dataset, digits are necessarily repeated but have different rotations, hence these correspond to data augmentation as compared to the original dataset. The equivariant networks were trained on unrotated spherical images with the dig- its being projected on the southern hemisphere, sampled randomly with replacement from the original 60k training samples of MNIST.4 Figure 4 shows that for the smaller data regimes the accu- racy of the equivariant model dominates the non-equivariant models, even though it uses fewer parameters than the non- equivariant models. Whereas the non-equivariant models continue to beneﬁt from the increased dataset size, the equiv- ariant model does not improve beyond the original 60k training samples of MNIST as expected. Looking at the trend of the non-equivariant models in Figure 4 it is unclear if they would eventually match the equivariant model for large enough augmented datasets. It turns out that for the task of spherical MNIST classiﬁcation, a large enough non- equivariant CNN trained on augmented data can achieve similar performance to an equivariant model, see Figure 10 left. A priori, it is not clear whether increasing model size and data augmentation is always sufﬁcient for non-equivariant models to match the performance of equivariant models. What is clear is that spherical MNIST classiﬁcation leaves much to be desired in terms of task and dataset complexity. 4Note that we did not take special precautions to treat ambigu- ous cases such as “6” vs. “9”.Equivariance versus Augmentation for Spherical Images 3.3. Semantic segmentation Semantic segmentation is an interesting example of a task where the output is equivariant, in contrast to the invariance of classiﬁcation output. This example lets us investigate if data augmentation on large enough non-equivariant models can match the equivariant models not only in classiﬁcation tasks, but also in more difﬁcult and truly equivariant tasks. For our experiments, we replaced the classiﬁcation labels from the spherical MNIST dataset described above by seg- mentation masks. A sample mask is depicted in the right panel of Figure 1, further details can be found in Ap- pendix C. In order to investigate whether data augmentation can push the performance of non-equivariant networks to match that of equivariant architectures, we trained the non-equivariant networks on rotated samples and compare to equivariant net- works trained only on non-rotated samples. For evaluation, we use the mean intersection over union (mIoU) where we drop the background class. More technical details can be found in Appendix C. The plot in Figure 2 shows the results of this experiment. Sample predictions for the same task with four digits are shown in Appendix C.4. As expected, more training data (and hence stronger data augmentation) increases the perfor- mance of the non-equivariant models. However, the equiv- ariant models outperform the non-equivariant models even for copious amounts of data augmentation. As is also shown in Figure 2, larger models outperform smaller models, as ex- pected. However, there seems to be a saturation of this effect as the 5.5M parameter model performs on par (within statis- tical ﬂuctuations) with the 1M parameter model. Notably though, even the largest model trained on data augmented with a factor of 20 cannot outperform the equivariant models. We also trained the non-equivariant models on even larger datasets with up to 1.2M data points, but their performance saturates at a level comparable to what is reached at 240k train data points, see Figure 10 right. We also trained larger spherical models to see if we could push performance even further. However, even the 820k parameter model speciﬁed in Table 5 in the appendix per- formed only at 76.71% non-background mIoU for 60k training data points. This is on par with the performance of 78.28% non-background mIoU that the 204k spherical model reached for this dataset and hence suggests that the smaller model already exhausts the chosen architecture for this problem. Since the larger model requires much more compute, we performed all the remaining experiments only with the 204k spherical model. In order to verify that our non-equivariant models are in principle expressive enough to learn the given datasets, we also trained them on unrotated data, as we did for the spher- ical models. From the plot in the left panel of Figure 5, it is clear that all the models perform well when evaluated on the same data on which they were trained. As shown in the right panel of Figure 5, performance deteriorates to almost random guessing (as expected) if the models are evaluated on rotated test data. The performance of the equivariant models is identical for both cases. For the task of train- ing and evaluation on unrotated data, which contains no symmetries, the considerably larger non-equivariant models slightly outperform the equivariant model. The performance of the smallest non-equivariant model is on par with that of the equivariant model. Note that the performance of the non-equivariant models fur- thermore crucially depends on where on the spherical grid the digits are projected. For the runs depicted in Figure 5, the digits were projected in the center of the Driscoll-Healy grid, so that the ordinary CNNs could beneﬁt maximally from their translation equivariance and distortions were min- imal. In Appendix C.3 we show that performance is greatly reduced if the digits are projected closer to the pole of the sphere, where distortions in the equirectangular projection of the Driscoll-Healy grid are maximal. 3.4. Dataset complexity The experiments described in the previous section used a very simple dataset, and since only one class needed to be predicted, all non-zero pixels belonged to the same fore- ground class. We have therefore investigated if the observed performance gain of the equivariant models persists for more complex datasets. The ﬁrst modiﬁcation we performed on the dataset depicted in Figure 1 is to project four MNIST digits onto the same sphere and construct a corresponding segmentation mask. A sample from this dataset is shown in Figure 6. The results of these experiments are summarized in Figure 7, sample pre- dictions for the best equivariant and non-equivariant models are shown in Appendix C.4. Note that the non-monotonic increase in performance in Figure 7 is due to sampling ef- fects during the data generation. We have explicitly veriﬁed that these features are within the range of the statistical ﬂuc- tuations of this sampling and since this affects all models equally, these features are irrelevant for the model compari- son. More details on this point are given in Appendix C.2. Moreover, for an increased number of digits, we observe a clear beneﬁt of the equivariant architectures. Again, increas- ing model sizes beyond a certain parameter count does not translate into an increase in performance. In comparison to Figure 2, we see that all models in Figure 7 beneﬁted from seeing more samples during training, with a steeper increase in performance with number of training samples and earlier saturation.Equivariance versus Augmentation for Spherical Images 0 50k 100k 150k 200k 250k 0 20 40 60 80 100 Number of unrotated training images mIoU on Unrotated Images 204k S2CNN 218k CNN 1M CNN 5.5M CNN 0 50k 100k 150k 200k 250k 0 20 40 60 80 100 Number of unrotated training images mIoU on Rotated Images 204k S2CNN 218k CNN 1M CNN 5.5M CNN Figure 5: Training on Unrotated Images. Performance of equivariant and non-equivariant models in semantic segmentation for various amounts of data augmentation for models trained on unrotated data with one digit. Performance is measured in terms of mIoU for the non-background classes. Left: evaluated on unrotated test data. Right: evaluated on rotated test data. Figure 6: Sample from the spherical MNIST dataset with four digits projected onto the sphere. Left: input data. Right: segmentation mask. In addition to increasing the number of MNIST digits in each datapoint, we have also made the object recognition as- pect more difﬁcult by swapping the MNIST digits for items of clothing from the FasionMNIST dataset (Xiao et al., 2017). A sample datapoint is depicted in Figure 8. The results shown in Figure 9 reﬂect the higher difﬁculty of this task: The performance of all models is lower compared to Figure 2. This is partly due to the difﬁculty of constructing segmentation masks for the items of clothing, as discussed in Appendix C. However, the equivariant models still outper- form the non-equivariant models for all training data sizes that we tried by a large margin. 3.5. Non-equivariant performance saturation To check whether the non-equivariant models could be pushed to the performance of the equivariant models by extending the training data even further, we trained our non-equivariant models on larger datasets. 0 50k 100k 150k 200k 250k 0 20 40 60 80 100 Number of training images mIoU (non-background) 204k S2CNN 218k CNN 1M CNN 5.5M CNN Figure 7: Segmentation on four digit Spherical MNIST . Performance of equivariant and non-equivariant models in semantic segmentation for various amounts of data augmen- tation as in Figure 2, but with four digits projected onto the sphere. For spherical MNIST classiﬁcation the non-equivariant mod- els can match the accuracy of the smaller equivariant model with enough augmented data, cf. Figure 10 left. On the other hand, for semantic segmentation, as shown in Fig- ure 10 right, even these much larger training datasets did not improve the test performance of the non-equivariant models. These experiments support the intuition that for equivariant tasks, equivariant models perform so much better than non- equivariant models that even very large amounts of dataEquivariance versus Augmentation for Spherical Images Figure 8: Sample from the spherical FashionMNIST dataset. Left: input data. Right: segmentation mask. 0 50k 100k 150k 200k 250k 0 20 40 60 80 100 Number of training images mIoU (non-background) 204k S2CNN 218k CNN 1M CNN 5.5M CNN Figure 9: Segmentation on Spherical FashionMNIST. Perfor- mance of equivariant and non-equivariant models in seman- tic segmentation for various amounts of data augmentation as in Figure 2, but with items of clothing from the Fashion- MNIST dataset projected onto the sphere. augmentation cannot compensate for this advantage. In contrast, although equivariant models (which were made invariant only in the last layer) still show higher performance for invariant tasks, non-equivariant models can ultimately reach the same performance if enough data augmentation is applied. 3.6. Inference latency and training times At similar parameter counts, the GPU based implementation (Cohen et al., 2018) of the equivariant layers have an order of magnitude higher inference latency as can be seen in Tables 1 and 2. A detailed proﬁling of the model and CUDA implementation shows that the bulk of the inference time is spent in the later upsampling layers which compute the largest SO(3) tensors in the network. In terms of operations, almost half of the time is spent in the custom implementation of the complex matrix multiplication of SO(3) tensors. There is also a Table 1: Runtime latency and throughput for the equivariant semantic segmentation model (204k S2CNN in Appendix Table 5) on an Nvidia T4 16GB GPU. Latency measures the time of a forward pass through the model on the GPU, throughput is the corresponding number of samples per second given the batch size. The larger batch size is chosen to maximize the throughput for the T4. Batch size Latency (ms) Throughput (N/s) 1 111 ±0.6 9 .0 ±0.04 7 479 ±2.2 14 .6 ±0.07 Table 2: Runtime latency and throughput for non- equivariant CNN model (200k CNN in Appendix Table 5) on an Nvidia T4 16GB GPU. Latency measures the time of a forward pass through the model on the GPU, throughput is the corresponding number of samples per second given the batch size. The larger batch size is chosen to maximize the throughput for the T4. Batch size Latency (ms) Throughput (N/s) 1 5.93 ±0.24 169 ±5.8 60 87.98 ±0.17 682 ±1.3 signiﬁcant overhead in the transformation back and forth to the Fourier domain. See Appendix D for the details on the proﬁling of layers and operations. The backpropagation latency of the equivariant models mir- rors the inference latency resulting in about an order of mag- nitude slower training as compared to the non-equivariant CNNs. Note that even though the training is slower, be- cause of the increase in data augmentation needed for the non-equivariant models, at a ﬁxed performance goal the equivariant model actually trains faster, cf. Table 3 for train- ing times for the classiﬁcation task at about 97.5% accuracy level. A similar comparison for semantic segmentation is less relevant as even large amount of data augmentation leaves a big performance gap compared to the equivariant model. 4. Conclusions Our results indicate that equivariant models possess an in- herent advantage over non-equivariant ones, which cannot be overcome by data augmentation when applied to tasks probing the full rotational equivariance of the spherical im- age data. In order to corroborate and generalize these ﬁnd- ings, several extensions of the current study are natural to pursue. In particular, it would be interesting to consider non-equivariant models speciﬁcally adapted to the sphere. Furthermore, even though our results indicate that the ad- vantage of equivariance is not explained by low data com-Equivariance versus Augmentation for Spherical Images 0 200k 400k 600k 800k 1.0M 1.2M 80 85 90 95 100 Number of training images Test accuracy 150k S2CNN 114k CNN 0.5M CNN 2.5M CNN 0 200k 400k 600k 800k 1.0M 1.2M 0 20 40 60 80 100 Number of training images Non-background IoU 204k S2CNN 218k CNN 1M CNN 5.5M CNN Figure 10: Non-equivariant performance saturation for segmentation. Left: For classiﬁcation of spherical MNIST as in Figure 4, the non-equivariant models reach the test accuracy of the equivariant models for very large amounts of data augmentation. Right: For semantic segmentation of one-digit spherical MNIST as in Figure 2, the non-background IoU of the non-equivariant models saturates well below the performance of the equivariant model even for moderately high amounts of data augmentation. Table 3: Training times for the S2CNN classiﬁcation model and non-equivariant CNN model at matched accuracy on rotated spherical images. The S2CNN model is trained on non-rotated images whereas the CNN is trained on an augmented dataset with rotated images. A single Nvidia T4 16GB was used for training. Model Accuracy Training time 150k S2CNN 97.64% 15h 5M CNN 97.49% 26h plexity, it would be interesting to investigate data sets which are both richer in complexity and native to the sphere rather than projected onto it. In terms of inference latency the CUDA implementation of the equivariant convolution in the Fourier domain, together with the large SO(3) tensors, is signiﬁcantly slower than a traditional spatial convolution and the proﬁling shows where future optimizations should be targeted. With wider adoption it is likely that this situation would improve on multiple fronts. It is appealing to think of symmetries as a fundamental design principle for network architectures. In this paper we use the symmetry of the sphere as a guiding principle for the network architecture. From this perspective it is natural to consider the question of how to train neural networks in the case of other “non-ﬂat” data manifolds, i.e. when the domain Mis a (possibly curved) manifold. This research ﬁeld is referred to as geometric deep learning, an umbrella term ﬁrst coined in (Bronstein et al., 2017) (see (Bronstein et al., 2021) and (Gerken et al., 2021) for recent reviews). The results of the present paper may thus be viewed as probing a small corner of the vast ﬁeld of geometric deep learning. Extending the exploration of inherent advantages of equiv- ariance to other tasks, data manifolds and, possibly local, symmetry groups offers exciting prospects for future re- search. Acknowledgments We are very grateful to Jimmy Aronsson for valuable dis- cussions and helpful feedback on the text. We also thank the anonymous referees for their comments. The work of D.P. and O.C. is supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. D.P. and J.G. are supported by the Swedish Research Council, J.G. is also supported by the Knut and Alice Wallenberg Foundation and by the German Ministry for Education and Research (BMBF) under Grants 01IS14013A-E, 01GQ1115, 1GQ0850, 01IS18025A and 01IS18037A. The computations were enabled by resources provided by the Swedish National Infrastructure for Computing (SNIC) at C3SE partially funded by the Swedish Research Council through grant agreement no. 2018-05973.Equivariance versus Augmentation for Spherical Images References Aronsson, J. Homogeneous vector bundles and G-equivariant Convolutional Neural Networks. arXiv:2105.05400, to appear in Sampling Theory, Signal Processing, and Data Analysis, May 2021. 3 Bronstein, M. M., Bruna, J., LeCun, Y ., Szlam, A., and Van- dergheynst, P. Geometric Deep Learning: Going beyond Eu- clidean data. IEEE Signal Processing Magazine, 34(4):18–42, July 2017. doi: 10.1109/MSP.2017.2693418. 9 Bronstein, M. M., Bruna, J., Cohen, T., and Veliˇckovi´c, P. Geo- metric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. arXiv:2104.13478 [cs, stat], April 2021. 9 Cobb, O. J., Wallis, C. G. R., Mavor-Parker, A. N., Marignier, A., Price, M., d’Avezac, M., and McEwen, J. D. Efﬁcient generalized spherical cnns. In International Conference on Learning Representations, Feb 2021. 2 Cohen, T., Geiger, M., and Weiler, M. A General Theory of Equivariant CNNs on Homogeneous Spaces. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., December 2019. 2, 3 Cohen, T. S. and Welling, M. Group Equivariant Convolutional Networks. In Balcan, M. F. and Weinberger, K. Q. (eds.),Pro- ceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Re- search, pp. 2990–2999, New York, New York, USA, 20–22 Jun 2016. PMLR. 1 Cohen, T. S., Geiger, M., Köhler, J., and Welling, M. Spherical CNNs. In 6th International Conference on Learning Represen- tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. 1, 2, 4, 8 Driscoll, J. and Healy, D. Computing Fourier Transforms and Convolutions on the 2-sphere. Advances in Applied Mathe- matics, 15(2):202–250, June 1994. ISSN 0196-8858. doi: https://doi.org/10.1006/aama.1994.1008. 14 Du, H., Cao, H., Cai, S., Yan, J., and Zhang, S. Spherical Trans- former: Adapting Spherical Signal to CNNs. arXiv:2101.03848 [cs], January 2021. 3 Eder, M. and Frahm, J.-M. Convolutions on Spherical Images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1–5, June 2019. 3 Esteves, C. Theoretical Aspects of Group Equivariant Neural Networks. arXiv:2004.05154 [cs, stat], April 2020. 2 Esteves, C., Makadia, A., and Daniilidis, K. Spin-Weighted Spher- ical CNNs. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Pro- cessing Systems, volume 33, pp. 8614–8625. Curran Associates, Inc., October 2020. 3 Gandikota, K. V ., Geiping, J., Lähner, Z., Czapli ´nski, A., and Moeller, M. Training or Architecture? How to Incorporate Invariance in Neural Networks. arXiv:2106.10044 [cs], June 2021. 3 Gerken, J. E., Aronsson, J., Carlsson, O., Linander, H., Ohlsson, F., Petersson, C., and Persson, D. Geometric Deep Learning and Equivariant Neural Networks. arXiv:2105.13926 [hep-th, cs], May 2021. 3, 9, 11 Haim, N., Segol, N., Ben-Hamu, H., Maron, H., and Lipman, Y . Surface Networks via General Covers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 632–641, October 2019. 3 Kondor, R. and Trivedi, S. On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research , pp. 2747–2755. PMLR, 10–15 Jul 2018. 2, 3 Kondor, R., Lin, Z., and Trivedi, S. Clebsch-Gordan Nets: A Fully Fourier Space Spherical Convolutional Neural Network. In Ben- gio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Pro- cessing Systems, volume 31. Curran Associates, Inc., November 2018. 2 Köhler, J., Cohen, T. S., Geiger, M., and Welling, M. Spheri- cal CNNs, October 2021. URL https://github.com/ jonkhler/s2cnn. 4 Lecun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998. ISSN 1558-2256. doi: 10.1109/5.726791. 5 Lee, Y ., Jeong, J., Yun, J., Cho, W., and Yoon, K.-J. SpherePHD: Applying CNNs on a Spherical PolyHeDron Representation of 360 degree Images. arXiv:1811.08196 [cs], April 2019. 3 Linmans, J., Winkens, J., Veeling, B. S., Cohen, T. S., and Welling, M. Sample Efﬁcient Semantic Segmentation using Rotation Equivariant Convolutional Networks. International Conference on Machine Learning Workshop “Towards learning with limited labels: Equivariance, Invariance, and Beyond”, July 2018. 2, 3 Makadia, A., Geyer, C., and Daniilidis, K. Correspondence-free Structure from Motion. International Journal of Computer Vision, 75(3):311–327, 2007. ISSN 1573-1405. doi: 10.1007/ s11263-007-0035-2. 3 Müller, P., Golkov, V ., Tomassini, V ., and Cremers, D. Rotation-Equivariant Deep Learning for Diffusion MRI. arXiv:2102.06942 [cs], February 2021. 3 Shakerinava, M. and Ravanbakhsh, S. Equivariant Networks for Pixelized Spheres. In Meila, M. and Zhang, T. (eds.), Proceed- ings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9477–9488. PMLR, 18–24 Jul 2021. 3 Tateno, K., Navab, N., and Tombari, F. Distortion-Aware Convo- lutional Filters for Dense Prediction in Panoramic Images. In Ferrari, V ., Hebert, M., Sminchisescu, C., and Weiss, Y . (eds.), Computer Vision – ECCV 2018, volume 11220 of Lecture Notes in Computer Science, pp. 732–750. Springer International Pub- lishing, 2018. ISBN 978-3-030-01269-4 978-3-030-01270-0. doi: 10.1007/978-3-030-01270-0_43. 3Equivariance versus Augmentation for Spherical Images Xiao, H., Rasul, K., and V ollgraf, R. Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv:1708.07747 [cs, stat], September 2017. 7 Zhang, C., Liwicki, S., Smith, W., and Cipolla, R. Orientation- Aware Semantic Segmentation on Icosahedron Spheres. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 3532–3540. IEEE, 2019. ISBN 978-1-72814-803-8. doi: 10.1109/ICCV .2019.00363. 3 A. Mathematical properties of the ﬁnal S2CNN layer In this appendix, we show the equivariance of the ﬁnal S2CNN layer used for semantic segmentation(3.3) and give an interpretation in terms of the projection (2.6) from Gto G/H. For convenience, we reproduce (3.3) here, fﬁnal(x) = L∑ ℓ=0 ℓ∑ m=−ℓ ℓ∑ n=−ℓ (κ⋆f )ℓ mnYℓ m(x) . (A.1) A.1. Proof of equivariance Using basic properties of Wigner matrices and spherical harmonics, we ﬁnd for R∈SO(3) fﬁnal(Rx) = L∑ ℓ=0 ℓ∑ m,n=−ℓ (κ⋆f )ℓ mnYℓ m(Rx) (A.2) = L∑ ℓ=0 ℓ∑ m,n,k=−ℓ (κ⋆f )ℓ mnDℓ mk(R)Yℓ k(x) (A.3) = L∑ ℓ=0 ℓ∑ m,n,k=−ℓ (κ⋆f )ℓ mnDℓ km(R−1)Yℓ k(x) , (A.4) where in (A.3) we used the transformation property of the spherical harmonics and in (A.4) that the Wigner matrices form a unitary representation. For a summary of the neces- sary formulae and the conventions5, see, e.g., (Gerken et al., 2021). Using the deﬁnition of the Fourier coefﬁcients on SO(3), 5We want to reiterate that we use the complex conjugate con- ventions for Wigner matrices in comparison to the reference. we obtain ℓ∑ m=−ℓ (κ⋆f )ℓ mnDℓ km(R−1) = ℓ∑ m=−ℓ 2ℓ+ 1 8π2 ∫ SO(3) (κ⋆f )(S)Dℓ mn(S)Dℓ km(R−1) dS =2ℓ+ 1 8π2 ∫ SO(3) (κ⋆f )(S)Dℓ kn(R−1S) dS =2ℓ+ 1 8π2 ∫ SO(3) (κ⋆f )(RS′)Dℓ kn(S′) dS′ =(κ⋆L Rf)ℓ kn, (A.5) where we have used the notation (LR(f))(Q) = f(RQ) and the equivariance of (3.2) in the last step. Plugging (A.5) into (A.4) completes the prove of equivari- ance of the last layer fﬁnal(Rx) = L∑ ℓ=0 ℓ∑ m,n=−ℓ (κ⋆L Rf)ℓ mnYℓ m(x) . (A.6) A.2. Projection from SO(3) to S2 To see the connection between (A.1) and the integration over H (in this case H = SO(2)) in (2.6), we rewrite (A.1) in position space fﬁnal(x) = L∑ ℓ=0 ℓ∑ m=−ℓ ℓ∑ n=−ℓ (κ⋆f )ℓ mnYℓ m(x) (A.7) = L∑ ℓ=0 ℓ∑ m,n=−ℓ ∫ SO(3) (κ⋆f )(S)Dℓ mn(S)Yℓ m(x) dS = L∑ ℓ=0 ℓ∑ n=−ℓ 2ℓ+ 1 8π2 ∫ SO(3) (κ⋆f )(S)Yℓ n(S−1x) dS . Now, we factorize S into an element α ∈SO(2) which stabilizes xand an element y ∈S2. This can always be done uniquely. With this, we obtain fﬁnal(x) = ∫ S2 ˜(κ⋆f )(y) L∑ ℓ=0 ℓ∑ n=−ℓ 2ℓ+ 1 8π2 Yℓ n(y) dy (A.8) where we have deﬁned ˜(κ⋆f )(y) = ∫ SO(2) (κ⋆f )(y,α) dα . (A.9) From (A.8), we can conclude that (A.1) can be understood as the projected version of κ⋆f , (A.9), integrated against the function ∑L ℓ=0 ∑ℓ n=−ℓ 2ℓ+1 8π2 Yℓ n.Equivariance versus Augmentation for Spherical Images B. Random model generation In this appendix, we give further details about the procedure used to randomly generate the model architectures we used in our experiments. We generated 20 random models for each desired parameter range, trained it to convergence, evaluated it on a holdout dataset and picked the best model according to the non- background mIoU. It should be noted that the goal of the procedure is not to come up with a selection of models that perform optimally on the given task for a certain parameter budget, but rather to select architectures to fairly compare equivariant and non-equivariant models. B.1. Non-equivariant models As mentioned in the main text, the non-equivariant models consist of skipped convolutional downsampling and skipped transposed convolutional upsampling layers. Speciﬁcally, the downsampling layers are deﬁned by down(ni,no,k,s ) = conv(ni,no,k,s ) (B.1) + conv(ni,no,1,1) ◦maxPool(k,s) , where conv(ni,no,k,s ) is a 2d convolution with ni input channels, no output channels, kernel size kand stride s, ◦ denotes function composition and maxPool(k,s) is a 2d max pooling operation with kernel size kand stride s. The upsampling layers are deﬁned by up(ni,no,k,s ) = tConv(ni,no,k,s ) (B.2) + upsample(d) ◦conv(ni,no,1,1) , where tConv(ni,no,k,s ) is a 2d transpose convolution with ni input channels, no output channels, kernel size k and stride sand upsample(d) is a 2d upsampling layer to a feature tensor with spatial resolution d×dand nearest neighbor interpolation. In (B.2), dis chosen such that the spatial dimensions of both summands match. After each down- and upsampling layer, we apply a ReLU nonlinearity. We randomly select the number of layers and their channels, the kernel sizes and the strides in the following way: First, we select the depth of the network between 1 and ⌊ˆp/(2 · 104)⌋, where ˆp is the upper limit of the parameter range. Then, in order to obtain an hourglass-shape for the network, we select the size of the image dimension at the bottleneck between 2 and 30 and linearly interpolate from that to the size of the input (and output) image. These interpolated image sizes are the target dimensions dtarget i which we then try to approximate by choosing the kernel sizes and strides of the convolutions appropriately. This is done only for the downsampling layers as the upsampling layers are set to exactly mirror the downsampling architecture. The kernel sizes ki are selected at random to be odd integers between 1 and 9. If the image in layer ihas smaller size than 9×9, we take instead the highest odd integer below the image dimension as the upper bound for the random choice. The strides si are then computed from the target dimension dtarget i , the input dimension din i of the layer and the kernel size ki according to si = round ( din i −ki dtarget i −1 ) . (B.3) If the number generated by (B.3) is 0, we set the stride to 1 instead. The actual output dimensions are then given by din i+1 = din i −ki si + 1. (B.4) In order to keep the total number of features roughly con- stant across layers, we set the number of channels Nc i to Nc i = ⌈ f (din i+1)2 ⌉ , (B.5) where f = 11·(2 ·L)2 is the number of features in the ﬁnal layer and Lis the bandwidth of the input (and output) data. During the model generation process, architectures are gen- erated according to the procedure summarized above and the total number of parameters for each architecture is com- puted. If the parameter count lies in the desired range, the model is accepted, otherwise it is rejected and a new model is generated. All models are trained with batch size 32 and learning rate 10−3 using Adam on a segmentation task with one MNIST digit on the sphere and 60k rotated training samples until convergence and then evaluated. In all experiments, we use early stopping on the non-background mIoU metric and a maximum of 100 epochs. In this way, for each desired parameter range, 20 models were trained and evaluated, we then picked the best per- forming models (according to the non-background mIoU) and used them for our experiments. The resulting architec- tures of the three non-equivariant models are summarized in Table 4. B.2. Equivariant models The equivariant models consist of the three layers described in Section 3.1. We will denote the operation (3.1) which takes input features on S2 and returns output features on SO(3) by S2SO3conv(ni,no,bi,bo,ˆβ) , (B.6) where ni is the number of input channels, no is the number of output channels, bi is the bandwidth of the input featureEquivariance versus Augmentation for Spherical Images Table 4: Non-equivariant model architectures used in our experiments. The up- and down-sampling layers are deﬁned in (B.1) and (B.2), respectively. The columns labeled as “output” contain the channel- and spatial dimensions of the output feature maps. Model 218k CNN 1M CNN 5.5M CNN block output block output block output input 1×1002 input 1×1002 input 1×1002 Down conv(1,13,5,1) 13 ×962 conv(1,12,3,1) 12 ×982 conv(1,12,3,1) 12 ×982 down(13,15,3,1) 15 ×942 down(12,13,3,1) 13 ×962 down(12,15,5,,1) 15 ×942 down(15,22,9,1) 22 ×862 down(13,16,5,1) 16 ×922 down(15,16,3,1) 16 ×922 down(22,31,7,1) 31 ×802 down(16,77,5,2) 77 ×922 down(16,85,7,2) 85 ×432 down(31,141,3,2) 141 ×392 down(77,96,3,1) 96 ×422 down(85,191,3,1) 191 ×392 down(96,163,5,1) 163 ×382 down(191,191,3,1) 191 ×372 down(191,1100,3,2) 1100 ×182 Up up(141,31,3,2) 31 ×802 up(163,96,5,1) 96 ×422 up(1100,191,3,2) 191 ×372 up(31,22,7,1) 22 ×862 up(96,77,3,1) 77 ×442 up(191,141,3,1) 141 ×392 up(22,15,9,1) 15 ×942 up(77,16,5,2) 16 ×922 up(141,85,5,1) 85 ×432 up(15,13,3,1) 13 ×962 up(16,13,5,1) 13 ×962 up(85,16,7,2) 16 ×922 tConv(13,11,5,1) 11 ×1002 up(13,12,3,1) 12 ×982 up(16,15,3,1) 15 ×942 tConv(12,11,3,1) 11 ×1002 up(15,12,5,1) 12 ×982 tConv(12,11,3,1) 11 ×1002 Params. 218,144 1 ,042,013 5 ,519,335 Table 5: Equivariant model architectures used in our experiments. The notation for the layers is introduced in (B.6)–(B.8). Note that the product of the input bandwidth and ˆβstays constant throughout the network. The columns labeled as “output” contain the channel- and spatial dimensions of the output feature maps. Model 204k S2CNN 820k S2CNN block output block output input 1×1002 input 1×1002 Down S2SO3conv(1,11,42,0.1238 ·π) 11 ×843 S2SO3conv(1,13,43,0.1881 ·π) 13 ×863 SO3conv(11,12,35,0.1474 ·π) 12 ×703 SO3conv(13,16,36,0.2187 ·π) 16 ×723 SO3conv(12,13,27,0.1768 ·π) 13 ×543 SO3conv(16,19,30,0.2613 ·π) 19 ×603 SO3conv(13,14,20,0.2292 ·π) 14 ×403 SO3conv(19,21,23,0.3135 ·π) 21 ×463 SO3conv(21,24,16,0.4089 ·π) 24 ×323 SO3conv(24,27,10,0.5878 ·π) 27 ×203 Up SO3conv(14,13,27,0.3095 ·π) 13 ×543 SO3conv(27,24,16,0.9405 ·π) 24 ×323 SO3conv(13,12,35,0.2292 ·π) 12 ×703 SO3conv(24,21,23,0.5878 ·π) 21 ×463 SO3conv(12,11,42,0.1768 ·π) 11 ×843 SO3conv(21,19,30,0.4089 ·π) 19 ×603 SO3S2conv(11,11,50,0.1474 ·π) 11 ×1002 SO3conv(19,16,36,0.3135 ·π) 16 ×723 SO3conv(16,13,43,0.2613 ·π) 13 ×863 SO3S2conv(13,11,50,0.2187 ·π) 11 ×1002 Params. 204,073 820 ,184Equivariance versus Augmentation for Spherical Images map, bo is the bandwidth of the output feature map and ˆβ is related to the kernel size as described further down. The operation (3.2) which takes and returns feature maps on SO(3) is similarly denoted by SO3conv(ni,no,bi,bo,ˆβ) (B.7) and the operation (3.3) with input feature maps on SO(3) and output feature maps on S2 is denoted by SO3S2conv(ni,no,bi,bo,ˆβ) . (B.8) After each layer, we apply a ReLU activation function. We also experimented with adding the equivariant skip con- nections provided by the original S2CNN implementation to the layers (B.7) but found no performance gain, so we did not use them in the experiments. For the spherical models, the procedure to generate archi- tectures is similar to the non-equivariant case, but now the bandlimit replaces the role of the image dimension in the construction. First, we select the depth and the bottleneck size in the same way as for the non-equivariant models. Then, we select the bandlimit at the bottleneck between 3 and 20 and compute a linear interpolation between the input bandlimit and the bandlimit at the bottleneck. Since the equivariant layers in the S2CNN architecture directly take the input- and output bandlimits as parameters, we only have to round those interpolated bandlimits to integers. On top of the bandlimits, the spherical convolutional layers also depend on the grid on which the kernel is sampled. For this grid, we take a Driscoll-Healy grid (Driscoll & Healy, 1994) around the unit element in SO(3). The three Euler angles α, β and γ of the grid points lie between 0 and 2π for αand γbut βtakes values between 0 and some upper bound ˆβ. This upper bound is responsible for the locality of the kernel and is analogous to the kernel size in standard CNN layers. However, since the bandlimit Li of the feature map determines how ﬁne the grid for the feature map is, the effective kernel size is characterized by the productˆβ·Li. In order to keep this product ﬁxed throughout the network, we set ˆβi = L/Li ·ˆβref where Lis the bandlimit of the input and ˆβref is a reference value that we pick at random between 0.02 and 0.25. Note that this construction implies that the number of trainable parameters in the spatial dimensions of the kernel is ﬁxed throughout the network. For the equivariant models, we randomly select the channel numbers by ﬁrst sampling a maximum channel number at the bottleneck between 11 and 30 and then linearly interpo- lating between 11 (the channel number at the output) and this maximum. Again, as in the non-equivariant case, we generate architec- tures according to this procedure and reject the ones whose parameter count lies outside the desired range. The models obtained in this way are trained which batch size 32 and learning rate 10−3 using Adam on a segmentation dataset of 60k unrotated training samples containing one MNIST digit on the sphere each until convergence and then evalu- ated. In all experiments, we use early stopping on the non- background mIoU metric and a maximum of 200 epochs. As in the non-equivariant case, we generated, trained and evaluated 20 models in the desired parameter ranges and picked the best performing ones according to the non- background mIoU. These models were then used for the experiments. The resulting architectures of the two equiv- ariant models are summarized in Table 5. C. Details on experiments In this appendix, we give more details about the experimen- tal setup for the spherical runs described in Section 3. C.1. Data generation As discussed in the main text, the input data to our models consist of MNIST digits and FashionMNIST items of cloth- ing which were projected onto the sphere, labeled by their corresponding segmentation masks. Here we give more details on the data generation process. For an input image with ndigits / items of clothing on the sphere, we ﬁrst sampled nimages from MNIST or Fashion- MNIST and pasted them at a random position on a 60 ×60 canvas. This canvas is then projected onto the sphere by projecting the spherical grid points onto the plane using a stereograhpic projection and performing a bilinear interpo- lation in the plane. To obtain a rotated input sample, we rotate the grid points with a random rotation matrix before projecting them. We generate the segmentation mask for a single digit / item of clothing by considering all pixels with a grayscale value above a certain threshold as belonging to the target class and to the background class otherwise. These segmentation masks are then assembled into the 60 ×60 canvas and pro- jected onto the sphere. Instead of a bilinear interpolation, we use nearest-neighbor interpolation for the segmentation masks. For MNIST, a threshold value of 150 yielded good results. For FashionMNIST however, we use a value of 10 to cap- ture ﬁner details of the cloths, as illustrated in Figure 11. Even lower values lead to a blurring along the edges in the segmentation mask. For validation, we generated datasets in the same way as for training, but sampling 10,000 data points from the test split of the MNIST dataset.Equivariance versus Augmentation for Spherical Images Figure 11: Examples of segmentation masks for Fashion- MNIST generated from the original (left) with threshold values of 150 (center) and 10 (right). 0 20k 40k 60k 80k 100k 120k 0 20 40 60 80 100 Number of training images mIoU (non-background) Reference run Weights reinitialized Datasets regenerated Figure 12: Variation in Performance. Performance for the non-equivariant 218k parameters model trained three times on rotated datasets of 2 MNIST digits on the sphere of various sizes. C.2. Performance variation due to data sampling As mentioned in the main text, increases or decreases in per- formance with varying dataset sizes that occur completely in parallel for all models are due to sampling effects dur- ing the data generation process. This point is illustrated by the plot depicted in Figure 12, where we trained the non-equivariant model with 218k parameters three times on rotated datasets of various sizes with 2 MNIST digits on the sphere. One run is for reference, one with randomly reinitialized weights before training and one was trained on a randomly regenerated dataset. We can see clearly that the weight reinitialization only had a minor inﬂuence on the model performance, but the peculiar irregularities of the performance increase with growing training dataset are within the variation of regenerating the dataset. C.3. Inﬂuence of projection point for non-rotated data The spherical data we use in our experiments is given in the Driscoll-Healy grid which is an equispaced grid in spherical Figure 13: Example of the input data in the Driscoll–Healy grid for a digit projected onto the grid center (left) and onto the pole of the sphere (right). coordinates, i.e. in the azimuthal angle φand polar angle θ. Therefore, what the input data looks like on this grid depends crucially on the projection point relative to the grid. Figure 13 illustrates this with a digit projected into the center of the φ,θ grid (i.e. on the equator, close to the φ= πline) and a digit projected onto the pole. For the experiments for non-rotated data depicted in Fig- ure 5 of the main text, we projected the input images onto the grid center, to help the non-equivariant networks. For this projection point, slight variations of the digit positions correspond almost to translations in the Driscoll-Healy grid with respect to which the ordinary CNNs are equivariant. For comparison, Figure 14 shows the results of training on data projected onto the pole of the sphere. Note that the performance on unrotated data is reduced considerably as compared to Figure 5, but slightly improved on rotated data. The reason is that slight positional variations across the pole lead to very non-linear deformations of the data represented in the Driscoll-Healy grid and the translational equivariance of the CNNs cannot help the training process. On the other hand, since the polar projections are most challenging, over- all performance on rotated test data is improved (although of course still very poor). Since the S2CNN models are equivariant with respect to all rotations on the sphere, their performance does not de- pend on the projection point of the digit, cf. Figure 14 vs. Figure 5. Therefore, we trained the S2CNN models for all other experiments on images projected onto the pole. C.4. Sample predictions In Figures 15 and 16, we show ten random sample predic- tions from the best equivariant and non-equivariant models in the four-digit task. D. Inference latency proﬁling Table 6 shows the inference latency per layer for the equiv- ariant semantic segmentation model. Here the bulk of the time is spent in the upsampling layers and in particular the last SO(3) to S2 convolution takes up almost half of the total inference time. Comparing with Table 5 this alsoEquivariance versus Augmentation for Spherical Images 0 50k 100k 150k 200k 250k 0 20 40 60 80 100 Number of unrotated training images mIoU on Unrotated Images 204k S2CNN 218k CNN 1M CNN 5.5M CNN 0 50k 100k 150k 200k 250k 0 20 40 60 80 100 Number of unrotated training images mIoU on Rotated Images 204k S2CNN 218k CNN 1M CNN 5.5M CNN Figure 14: Polar Training Images. Performance of equivariant and non-equivariant models for various amounts of data augmentation for models trained on unrotated data, projected on the pole of the sphere. Performance is measured in terms of mIoU for the non-background classes. Left: Evaluated on unrotated test data. Right: evaluated on rotated test data. Table 6: Latency per layer and fraction of total time for the equivariant semantic segmentation model (204k S2CNN in Appendix Table 5) on an Nvidia T4 16GB GPU as measured by NSight systems with NVTX for batch size 1. Latency measures the time of a forward pass through the layer on the GPU. Layer Latency (ms) Fraction (%) Down S2SO3conv 6.7 5.0 Down SO3conv 16.1 11.8 Down SO3conv 7.3 5.3 Down SO3conv 3.1 2.3 Up SO3conv 6.8 5.0 Up SO3conv 15.3 11.2 Up SO3conv 26.4 19.4 Up SO3S2conv 54.5 40.0 coincides with the largest SO(3) tensors being processed. In Table 7 the fraction of inference time is instead shown per operation. Here it is clear that a large portion is spent in the custom CUDA implementation for the complex ma- trix multiplication of SO3 tensors, a possible avenue for optimization reducing overall inference latency. Most of the remaining time is spent in the Fast Fourier Transform (SO3_fft_real) and the inverse FFT (SO3_ifft_real). Table 7: Fraction of time spent per operation (>1%) for the equivariant semantic segmentation model (204k S2CNN in Appendix Table 5) on an Nvidia T4 16GB GPU as measured by NSight systems for batch size 1. Module Op Fraction of time (%) s2cnn _cuda_SO3_mm 45.2 s2cnn SO3_ifft_real 15.6 s2cnn SO3_fft_real 14.4 pytorch einsum 12.2 pytorch contiguous 8.4 95.7Equivariance versus Augmentation for Spherical Images Input image  Ground truth mask  204k S2CNN  1M CNN background 0 1 2 3 4 5 6 7 8 9 Figure 15: Sample predictions on the test dataset for the best equivariant model (240k S2CNN, trained on 240k samples) and non-equivariant model (1M CNN, trained on 600k samples) on the four-digits task depicted in Figure 6. The ﬁve samples were selected at random from the dataset and we depict here the raw data on the Driscoll–Healy grid. Five more samples are depicted in Figure 16.Equivariance versus Augmentation for Spherical Images Input image  Ground truth mask  204k S2CNN  1M CNN background 0 1 2 3 4 5 6 7 8 9 Figure 16: Five more samples of model predictions, cf. Figure 15.",
      "references": [
        "Homogeneous vector bundles and G-equivariant Convolutional Neural Networks",
        "Geometric Deep Learning: Going beyond Euclidean data",
        "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges",
        "Efficient generalized spherical CNNs",
        "A General Theory of Equivariant CNNs on Homogeneous Spaces",
        "Group Equivariant Convolutional Networks",
        "Spherical CNNs",
        "Computing Fourier Transforms and Convolutions on the 2-sphere",
        "Spherical Transformer: Adapting Spherical Signal to CNNs",
        "Convolutions on Spherical Images",
        "Theoretical Aspects of Group Equivariant Neural Networks",
        "Spin-Weighted Spherical CNNs",
        "Training or Architecture? How to Incorporate Invariance in Neural Networks",
        "Geometric Deep Learning and Equivariant Neural Networks",
        "Surface Networks via General Covers",
        "On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups",
        "Clebsch-Gordan Nets: A Fully Fourier Space Spherical Convolutional Neural Network",
        "Gradient-Based Learning Applied to Document Recognition",
        "SpherePHD: Applying CNNs on a Spherical PolyHeDron Representation of 360 degree Images",
        "Sample Efficient Semantic Segmentation using Rotation Equivariant Convolutional Networks",
        "Correspondence-free Structure from Motion",
        "Rotation-Equivariant Deep Learning for Diffusion MRI",
        "Equivariant Networks for Pixelized Spheres",
        "Distortion-Aware Convolutional Filters for Dense Prediction in Panoramic Images",
        "Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms",
        "Orientation-Aware Semantic Segmentation on Icosahedron Spheres"
      ],
      "meta_data": {
        "arxiv_id": "2202.03990v2",
        "authors": [
          "Jan E. Gerken",
          "Oscar Carlsson",
          "Hampus Linander",
          "Fredrik Ohlsson",
          "Christoffer Petersson",
          "Daniel Persson"
        ],
        "published_date": "2022-02-08T16:49:30Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates the role of rotational equivariance in CNNs for spherical images by comparing group-equivariant architectures (S2CNNs) against standard CNNs enhanced with data augmentation. Key contributions include the introduction of a new group-equivariant CNN layer for semantic segmentation over arbitrary Lie groups, extension of the S2CNN architecture to produce equivariant outputs on the sphere, and comprehensive empirical evaluations that demonstrate the inherent advantage of equivariant models for segmentation tasks, along with detailed trade-offs in network performance, parameter counts, and inference latency.",
        "methodology": "The study employs theoretical analysis and empirical experiments. It builds upon group equivariant network theory, using Fourier domain techniques with spherical harmonics and Wigner matrices to define convolutions on the sphere and SO(3). The methodology involves designing both equivariant (S2CNN-based) and non-equivariant CNN models, with the latter relying on extensive rotational data augmentation. Different network architectures are generated randomly within specified parameter ranges, and the impact of data augmentation versus inherent equivariance is evaluated for classification and semantic segmentation tasks.",
        "experimental_setup": "Experiments are conducted on spherical projections of MNIST and FashionMNIST datasets. For classification, single digit images are projected on the sphere and evaluated on rotated and unrotated test samples. For semantic segmentation, segmentation masks of digits or clothing items on spherical images are used, with tasks varying in complexity (e.g., single digit, multiple digits, and FashionMNIST items). Both model types are compared in terms of accuracy and mean intersection over union (mIoU) with varying amounts of data augmentation and different parameter counts. Inference latency and training times are measured on GPU (Nvidia T4 16GB), and detailed profiling of layer-wise operations is provided.",
        "limitations": "The paper identifies that while data augmentation can bridge the gap in classification accuracy between equivariant and non-equivariant models, it falls short for segmentation tasks. Equivariant models, although more parameter efficient and effective for truly equivariant tasks, suffer from higher computational overhead, particularly due to complex Fourier transforms and large SO(3) tensor operations, leading to reduced inference throughput and increased training times. Additionally, performance for non-equivariant models can be sensitive to the projection point on the sphere.",
        "future_research_directions": "The study suggests exploring more efficient GPU and CUDA implementations to optimize the Fourier domain operations in equivariant networks. Further research could involve adapting non-equivariant architectures specifically to spherical data, extending experiments to richer and native spherical datasets, and generalizing the findings to other data manifolds and local symmetry groups within the broader field of geometric deep learning.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Current soft augmentation methods employ fixed softening functions, which do not adequately model the compound and context-dependent effects of multiple simultaneous data transformations. In particular, existing approaches neglect the nonlinear interactions between different augmentation components (e.g., occlusion, cropping, and noise), leading to miscalibrated confidence scores and reduced robustness. Moreover, there is a lack of adaptive strategies that can automatically tune augmentation parameters based on individual sample characteristics and transformation interactions.",
    "method": "We propose Adaptive Multi-Component Soft Regularization (AMSR+), an advanced framework that decomposes and dynamically models the compound effects of multiple data transformations. AMSR+ employs an auxiliary lightweight network to estimate per-sample severity coefficients for each transformation component. These coefficients are integrated into a novel hierarchical softening function that is both class-dependent and sample-adaptive. Furthermore, AMSR+ leverages self-supervised contrastive learning signals to fine-tune the soft-target distributions, thereby aligning the network's confidence with the actual information loss induced by complex augmentations.",
    "experimental_setup": "The method will be validated on standard image classification benchmarks, including CIFAR-10 and CIFAR-100, using network architectures like ResNet-18. Two experimental settings will be compared: (1) a baseline model using fixed soft augmentation, and (2) the proposed AMSR+ method that computes per-sample, multi-component adaptive soft targets via an auxiliary network and hierarchical softening function. Evaluation metrics will include overall accuracy and Expected Calibration Error (ECE). The experimental design will also include ablation studies to quantify the contribution of each component (e.g., multi-component decomposition, hierarchical softening, and contrastive fine-tuning).",
    "primary_metric": "accuracy",
    "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AMSRPlusLoss(nn.Module):\n    def __init__(self, base_loss_fn=nn.CrossEntropyLoss(), kl_weight=0.5):\n        super(AMSRPlusLoss, self).__init__()\n        self.base_loss_fn = base_loss_fn\n        self.kl_weight = kl_weight\n        # Define a small auxiliary network to extract severity coefficients\n        self.aux_net = nn.Sequential(\n            nn.Linear(2, 16),  # assuming two basic transformation parameters (e.g., cropping and occlusion severity)\n            nn.ReLU(),\n            nn.Linear(16, 2)  # output refined severity coefficients for each component\n        )\n\n    def forward(self, logits, targets, aug_params):\n        ce_loss = self.base_loss_fn(logits, targets)\n        batch_size = logits.size(0)\n        num_classes = logits.size(1)\n        one_hot = F.one_hot(targets, num_classes=num_classes).float()\n\n        # Predict refined severity coefficients using the auxiliary network\n        severity_coeffs = self.aux_net(aug_params)  # Shape: (batch_size, 2)\n        # Hierarchical softening: use different softening exponents for each transformation component\n        comp1, comp2 = severity_coeffs[:, 0], severity_coeffs[:, 1]\n        # Combine coefficients in a hierarchical manner: first component modulates base exponent, second refines it\n        base_exp = 1.0 - 0.5 * comp1\n        refined_exp = base_exp - 0.3 * comp2\n        softening_exponent = refined_exp.clamp(min=0.1, max=1.0).unsqueeze(1)\n\n        # Generate the adaptive soft target\n        soft_targets = one_hot.pow(softening_exponent)\n        soft_targets = soft_targets / soft_targets.sum(dim=1, keepdim=True)\n\n        # Compute KL divergence loss\n        log_probs = F.log_softmax(logits, dim=1)\n        kl_loss = F.kl_div(log_probs, soft_targets, reduction='batchmean')\n\n        loss = ce_loss + self.kl_weight * kl_loss\n        return loss\n\n# Example usage\nif __name__ == '__main__':\n    batch_size = 16\n    num_classes = 10\n    logits = torch.randn(batch_size, num_classes, requires_grad=True)\n    targets = torch.randint(low=0, high=num_classes, size=(batch_size,))\n    # Simulate augmentation parameters (e.g., cropping severity and occlusion severity) in the range [0,1]\n    aug_params = torch.rand(batch_size, 2)\n    \n    criterion = AMSRPlusLoss(kl_weight=0.5)\n    loss = criterion(logits, targets, aug_params)\n    loss.backward()\n    print('Loss:', loss.item())",
    "expected_result": "We expect AMSR+ to yield an increase in test accuracy of around 3-4% compared to the fixed soft augmentation baseline on datasets like CIFAR-10. Additionally, the Expected Calibration Error (ECE) is anticipated to decrease by 15-25%, indicating enhanced model calibration. Ablation studies should show that each component of the AMSR+ framework (multi-component decomposition, hierarchical softening, and contrastive fine-tuning) contributes to these improvements.",
    "expected_conclusion": "AMSR+ provides a novel, adaptive solution to regularizing models under complex and compound data transformations. By decomposing transformation effects and integrating self-supervised contrastive signals into a hierarchical softening strategy, the method offers superior calibration, robustness, and accuracy. Academically, this work extends the paradigm of soft augmentation by introducing per-sample, multi-component adaptation, while practically, it offers a computationally efficient approach that seamlessly integrates with existing training pipelines to yield significant performance gains in data-scarce scenarios."
  }
}